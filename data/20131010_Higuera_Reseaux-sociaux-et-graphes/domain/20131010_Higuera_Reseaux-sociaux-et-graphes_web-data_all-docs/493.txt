https://tel.archives-ouvertes.fr/tel-00494519/file/lip6-2009-001.pdf

Isomorphisme Inexact de Graphes par Optimisation

Évolutionnaire
Thomas Bärecke

To cite this version:
Thomas Bärecke. Isomorphisme Inexact de Graphes par Optimisation Évolutionnaire. Informatique
[cs]. Université Pierre et Marie Curie - Paris VI, 2009. Français. <tel-00494519>

HAL Id: tel-00494519

https://tel.archives-ouvertes.fr/tel-00494519

Submitted on 23 Jun 2010

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

École Doctorale d’Informatique, Télécommunication et Électronique de Paris

ISOMORPHISME INEXACT DE GRAPHES
PAR OPTIMISATION ÉVOLUTIONNAIRE

THÈSE DE DOCTORAT DE L’UNIVERSITÉ PARIS VI

présentée pour obtenir le grade de

DOCTEUR DE L’UNIVERSITÉ PIERRE ET MARIE CURIE - PARIS VI

(SPÉCIALITÉ INFORMATIQUE)

par

Thomas BÄRECKE

Soutenue publiquement le 22 octobre 2009

devant le jury composé de

Rapporteurs :

Evelyne LUTTON
Michèle SEBAG

Examinateurs :

Directeur de recherche, INRIA
Directeur de recherche, CNRS

Bernadette BOUCHON-MEUNIER Directeur de recherche, CNRS
Marcin DETYNIECKI
Patrick GALLINARI
El-Ghazali TALBI

Chargé de recherche, CNRS
Professeur, Université Paris VI
Professeur, Université Lille I

Bärecke, Thomas :
Isomorphisme inexact de graphes par
optimisation évolutionnaire
Thèse de doctorat, Université Pierre et
Marie Curie - Paris VI, 2009.

Résumé

L’isomorphisme inexact de graphes est un problème crucial pour la déﬁnition d’une distance
entre graphes, préalable nécessaire à une multitude d’applications allant de l’analyse d’images à
des applications biomédicales en passant par la reconnaissance optique de caractères. Ce problème
est encore plus complexe que celui de l’isomorphisme exact. Alors que ce dernier est un problème
de décision de complexité au moins de classe P et qui ne s’applique qu’à des graphes exactement
identiques, l’isomorphisme inexact est un problème combinatoire de complexité de classe NP qui
permet de prendre en compte des perturbations dues au bruit, qui apparaissent fréquemment dans
les applications réelles.

Dans ce cadre, nous choisissons d’étudier une solution basée sur les algorithmes génétiques pou-
vant être appliquée à l’isomorphisme exact et inexact. Nous proposons des opérateurs de croisement
généraux pour tout problème représenté par un codage de permutation, ainsi que des opérateurs
spéciﬁques à l’isomorphisme de graphes qui exploitent une heuristique gloutonne. Nous réalisons
une étude exhaustive pour comparer ces opérateurs avec les opérateurs existants, soulignant leurs
propriétés, avantages et inconvénients respectifs.

Nous étudions par ailleurs plusieurs pistes d’amélioration de l’algorithme, en théorie ou en pra-
tique, considérant successivement les objectifs d’accélération de l’exécution, d’augmentation de la
précision et de garantie de résultat optimal. Nous proposons pour cela de combiner l’approche pro-
posée avec d’autres techniques telles que des heuristiques générales comme la recherche locale, des
heuristiques dédiées comme l’algorithme A*, et des outils pratiques comme la parallélisation.

Ces travaux conduisent à la déﬁnition d’une méthode générique pour la résolution de tous les
problèmes d’isomorphismes de graphes, qu’il s’agisse d’isomorphismes exact ou inexact, d’isomor-
phismes de graphes de même taille ou d’isomorphismes de sous-graphes. Nous illustrons enﬁn la
validité de cette solution générale par trois applications concrètes issues de domaines différents, la
recherche d’images et la chimie, qui présentent chacune des caractéristiques spéciﬁques, utilisant
des graphes attribués ou non, soumis aux perturbations plutôt structurelles ou au niveau d’attributs.

Mots-clés: isomorphisme inexact de graphes, distance de graphes, algorithme évolutionnaire, re-
cherche locale, algorithme mémétique, optimisation combinatoire, opérateurs de croisement, ap-
proche hybride, parallélisation, méta-heuristique

EVOLUTIONARY OPTIMISATION FOR INEXACT GRAPH ISOMORPHISM

Abstract

The solution to the inexact graph matching problem is the key for deﬁning any type of
graph distance. It is even more complex than the exact graph isomorphism problem. On the one
hand, inexact graph matching is a combinatorial optimization problem in NP taking into account
perturbations inherent in noisy real world environments. Exact graph matching, on the other hand, is
a decision problem for which it has not yet been shown if its complexity class is P and which applies
only to exactly identical graphs.

In this thesis, we study an approach based on genetic algorithms addressing both exact and
inexact isomorphisms. We introduce several new crossover operators, some more general for use
with any kind of permutation encoding, some specialized which include a greedy heuristic speciﬁc
to graph matching. We conduct an exhaustive study in order to compare these operators with the
existing ones, underlining their respective characteristics, advantages and disadvantages.

Furthermore, we examine several aspects for enhancing the algorithm, both theoretical and prac-
tical ones, leading to faster execution, better precision or even the assurance of ﬁnding the global
optimum. We combine the genetic algorithm with generalized black-box heuristics, such as lo-
cal search, specialized heuristics such as the A* algorithm or practical tools like parallelization
techniques. Our ﬁnal aim is to present a method addressing all different types of graph matching
problems, i.e. exact and inexact, isomorphisms of graphs having the same size and sub-graph iso-
morphisms. We illustrate the generality of our approach with three applications with very distinct
properties which cover the different problem types.

Keywords: inexact graph matching, graph distance, evolutionary algorithm, local search, memetic
algorithm, combinatorial optimization, crossover operators, hybrid method, parallelization, meta-
heuristic

vii

Remerciements

Cette thèse n’aurait pas vu le jour sans l’aide d’un grand nombre de personnes que je tiens à

remercier ici.

J’aimerais exprimer toute ma gratitude à Marcin Detyniecki pour son indéfectible soutien tout
au long de ma thèse. Il a activement contribué à ma formation scientiﬁque en me permettant de
participer à des écoles d’été et des congrès, aussi lointains qu’ils fussent, et en menant tambour
battant de fructueuses discussions jusque sur les courts de tennis. Mais il a également su m’ouvrir
d’autres portes, en résolvant tous les problèmes que j’ai pu rencontrer et en m’offrant une autre
vision de la culture française qui m’a permis de m’adapter plus facilement. Je le remercie enﬁn tout
particulièrement pour sa grande disponibilité et la talentueuse pondération de son regard sur mon
travail entre critique exigeante et enthousiasme stimulant.

Je remercie très chaleureusement Bernadette Bouchon-Meunier pour m’avoir suivi tout au long
de mon parcours, pour m’avoir fait bénéﬁcier de ses conseils précieux et en particulier pour ses
encouragements pendant la « phase terminale » de rédaction.

J’adresse également mes remerciements les plus sincères à Evelyne Lutton pour avoir accepté
d’être rapporteur de ma thèse. Ses commentaires et notre discussion pendant la soutenance m’ont
permis d’améliorer mes connaissances des algorithmes co-évolutionnaires.

Je suis très honoré que Michèle Sebag ait accepté d’être rapporteur de ma thèse malgré ses
importantes contraintes, professionnelles et personnelles. Ses très nombreux commentaires m’ont
permis non seulement d’améliorer mon travail mais aussi de découvrir de nouvelles idées.

J’apprécie tout particulièrement qu’El-Ghazali Talbi et Patrick Gallinari aient accepté d’être
examinateurs de ma thèse et aient ainsi contribué à élargir mon horizon avec leurs commentaires
lors de ma soutenance.

Je tiens à remercier sincèrement l’équipe LOFTI pour son accueil chaleureux, les discussions
fructueuses lors et en dehors des réunions d’équipes, les pauses café, chocolat et plus récemment
fruits. J’aimerais remercier en particulier Marie-Jeanne et Christophe pour leur disponibilité et leurs
solutions à toutes mes questions que ce soit au sujet de LATEX, Matlab, ou n’importe quel autre
énigmatique outil.

J’ai eu la chance de travailler dans des conditions optimales et d’avoir eu beaucoup d’aide au
niveau administratif. J’aimerais exprimer ma gratitude au personnel administratif du LIP6 et de
l’Université ainsi qu’aux ingénieurs systèmes, qui sont d’ailleurs disponibles même le week-end.
Merci à Ghislaine, Jacqueline, Thierry, Christophe, Jean-Pierre, Vincent... J’aimerais également re-
mercier Ghislaine Hannot et Marilyn Galopin d’avoir fait tout ce qui était nécessaire pour que ma
soutenance ait lieu malgré un dossier très en retard.

Je voudrais également remercier les personnes extérieures au LIP6 avec lesquelles j’ai eu l’oc-
casion de collaborer pendant ma thèse. Grâce aux travaux de Stefano Berretti, je me suis lancé dans

viii

le domaine de l’appariement de graphes. Je remercie également Christine Solnon, que j’ai pu ren-
contrer à l’occasion d’un séminaire en 2008, pour notre discussion très bénéﬁque.

Bien qu’un lien avec le présent manuscrit soit inexistant, je tiens à remercier l’équipe du CEA,
François Werkoff, Christine Mansilla et Sophie Avril, avec qui j’ai eu l’occasion de travailler pendant
la première année de ma thèse sur un sujet très éloigné mais pas pour autant moins intéressant.

J’adresse un grand merci à tous ceux qui ont, à plusieurs reprises, relu ce manuscrit pour éliminer
les coquilles et améliorer le style : merci à Marie-Jeanne, Marie, Jean-Marc, et bien sûr, Bernadette
et Marcin.

Je tiens à remercier tous ceux qui m’ont accompagné sur le chemin vers la thèse et qui m’ont
donné envie de me lancer dans le domaine de l’intelligence artiﬁcielle, pendant mes études à Mag-
debourg : j’adresse ma gratitude à Christian Borgelt, Rudolf Kruse, Andreas Nürnberger qui m’a
d’ailleurs beaucoup aidé lors de ma venue en tant qu’étudiant Erasmus, Tobias Scheffer,... . Je re-
mercie également tous ceux qui m’ont enseigné l’outillage d’informaticien, à commencer par Ralf
Feuerstein - toujours avec l’autocollant « Des ordinateurs nous aident à résoudre des problèmes que
nous n’aurions jamais eus sans eux » - en passant par les équipes du Fraunhofer IFF et d’icubic - les
lundis sans Burger King, c’est dur au début.

J’adresse ma gratitude à tous ceux qui devraient ﬁgurer ici et que j’ai oubliés. J’espère qu’ils ne

m’en voudront pas trop.

Je remercie enﬁn mes proches et amis pour leur soutien sans faille, et bien sûr Noemí, pour être

toujours à mes côtés.

ix

A Noemí

x

Table des matières

Sigles

Symboles

Table des ﬁgures

1 Introduction

.

. . . . . .
. . . . . .

. . . . .
1.1 Motivation . . .
. . . . .
1.2 Graphes . . . . .
1.3 Appariements dans un graphe . . . .
1.4
. . . . .

. . . . . .
. . . . . .
. . . . . .
. . . . . .
Isomorphisme de graphes
Isomorphisme vs homomorphisme . . .
1.4.1
. . . . .
Isomorphisme de sous-graphes
1.4.2
. . . . . .
. . . . .
Problèmes inexacts
1.4.3
. . . .
. . . . . .
1.4.4 Distances de graphes
. . . . . .
1.4.5 Appariements multivoques
.
1.4.6 Appariements pour la recalage
. . . . .
. . . . . .
. . . . .
1.5 Algorithmes
. .
. . . . . .
1.6 Méta-heuristiques
. . . . .
1.7 Algorithmes évolutionnaires . . . . .
. . . . . .
. . . .
1.8 Approches génétiques dans l’état de l’art

. . . . . .
. . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .

2 Algorithmes génétiques pour l’isomorphisme de graphes

2.1.1 Analyse théorique .
2.1.2 Analyse empirique .

. . . . .
. . . . .
2.2 Cadre général d’application . . . . .
2.3 Codage et fonction d’évaluation . . .

2.1 Étude des algorithmes évolutionnaires . . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
2.3.1 Codage robuste pour l’isomorphisme de sous-graphes . . . .
. . . . . .
2.3.2 Appariements multivoques

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

. . . . .

.

. . . . . .

xv

xix

xxi

1
1
6
8
8
9
9
10
10
13
14
14
18
19
21

23
23
24
27
32
34
34
35

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

xi

xii

TABLEDESMATIÈRES

2.3.3

. . . . .
. . . . .

2.4 Moteurs d’évolution . . . .

Sélection .
2.5 Croisement . . . .

Fonction d’évaluation . . . . .
. . . . . .

. . . . .
. . . . .
2.4.1 Moteur générationnel et steady-state . .
. . . . .
2.4.2
. . . . .
. . . . .
. . . . .

. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
2.5.1 Caractéristiques générales . . .
2.5.2 Opérateurs génériques . . . . .
. . . . . .
2.5.3 Opérateurs spéciﬁques pour les permutations . . . .
. . . . . .
2.5.4 Opérateurs classiques . . . . .
2.5.5 Nouveaux opérateurs
. . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . .
. . . . . .
2.6.1 Mutation d’échange . . . . . .
2.6.2 Mutation par brassage . . . . .
. . . . . .
. . . . . .
. . . . . .
2.7
Initialisation gloutonne . . .
. . . . . .
. . . . . .
2.8 Opérateurs par couleur . . .
2.9 Réglage des paramètres
. .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
2.9.1 Racing . .
. . . . .
2.9.2 No free lunch . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

2.6 Mutation . . . . .

3 Choix des opérateurs et leurs paramètres

Protocole expérimental

3.3 Comparaison des opérateurs de croisement

. . . .
3.2.1
3.2.2 Moteur générationnel
. . . . .
3.2.3 Moteur steady-state . . . . . .
3.2.4 Étude de la sensibilité au bruit .

3.1 Constitution des données de test . . . .
. . . . .
3.2 Détermination des paramètres optimaux . . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . .
3.3.1 Comparaison des paramètres optimaux .
3.3.2 Comparaison des performances . . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

3.4 Étude des stratégies de sélection . . . .
3.5 Étude des stratégies de mutation . . . .
. . . . . .
. . . . . .
. . . . . .

3.5.1
. . . . .
3.5.2 Expérimentation . .
. . . . .

3.6 Bilan .

Principe .

. . . . . .

. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

4 Recherche locale

4.1 Complexité . . . .

. . . . .

. . . . . .

. . . . .

. . . . . .

. . . . .

. . . . .

35
37
37
38
40
40
41
42
42
44
52
52
52
52
54
54
56
57

59
59
60
60
62
81
90
94
94
95
101
102
102
103
107

109
110

4.1.1 Complexité théorique . . . .
4.1.2 Complexité en pratique . . .

. . . . . .
. . . . . .
4.2 Stratégies de sélection pour la recherche locale .
. . . . . .

4.2.1 Choix des individus . . . . .

. . . . .
. . . . .
. . . . .
. . . . .

. . . . .
4.3 Expérimentation . . . . . .
. . . . .
4.3.1 Croisement UPBX .
4.3.2 Croisement DPX . .
. . . . .
4.3.3 Comportement des autres opérateurs de croisement
. . . . .

. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .

4.4 Bilan . . . . . .

. . . . . .

. . . . .

. . . . . .

. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . . .
. . . . . .
. . . . . .
. . . . .
. . . . . .

. . . . .
. . . . .
. . . . .
. . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

5 Modèles parallèles

5.1 Classiﬁcation . .

. . . . . .

. . . . .

. . . . . .

. . . . .

. . . . . .

. . . . .

. . . . .
. . . . . .
5.2 Gain théorique .
5.3 Approche proposée . . . . .
. . . . .
5.4 Gain pratique - expérimentation . . .
. . . . .

5.4.1 Taille des graphes .

. . . . . .
. . . . . .
. . . . . .
. . . . . .

5.4.2 Taille de la population . . . .
. . . . . .
5.4.3 Communication entre processus . . . . .
. . . . . .
. . . . . .

5.5 Parallélisation au niveau applicatif . .
. . . . .
5.6 Bilan . . . . . .

. . . . . .

. . . . .
. . . . .
. . . . .
. . . . .

. . . . .
. . . . .
. . . . .
. . . . .

. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .
. . . . .

. . . . .
. . . . .
. . . . .
. . . . .

6 Garantie du résultat optimal

6.1

Inﬂuence du seuil d’acceptation sur l’algorithme A* . . .
6.1.1 Expérimentation . .
. . . . .

. . . . . .

. . . . .

. . . . . .
. . . . . .

. . . . .
. . . . .

6.2 Stratégie de doublement
6.3 Algorithme hybride
6.4 Bilan . . . . . .

. .
. . . .
. . . . . .

. . . . .
. . . . .
. . . . .

. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .

. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .

7 Applications

7.1

Images . . . . .
. . . . . .
7.1.1 Contexte général . .
Images 3D . . . . .
7.1.2

. . . . .
. . . . .
. . . . .

7.2 Molécules

7.1.3 COILDel . . . . . .
. . . . . .
7.2.1
Filtrage par signature
7.2.2 Approche par couleur

. . .

. . . . .
. . . . .
. . . .
. . . .

. . . . . .
. . . . . .
. . . . . .

. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .

. . . . .
. . . . .
. . . . .
. . . . .

. . . . . .
. . . . . .
. . . . . .

. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .

. . . . .
. . . . .
. . . . .
. . . . .

7.3 Bilan . . . . . .

. . . . . .

. . . . .

. . . . . .

. . . . .

. . . . . .

. . . . .

xiii

111
113
113
114

115
116
117
117
118

119
119

121
123
124
125

126
127
130
131

133
133
134

138
142
145

147
147
147
149

153
156
156
158

163

xiv

TABLEDESMATIÈRES

8 Conclusion et perspectives

Bibliographie

Annexes

A Sensibilité au bruit des paramètres

A.1 Moteur générationnel

. . .
A.1.1 Bruit uniforme . . .
A.1.2 Bruit gaussien . . .
. . . .
A.2.1 Bruit uniforme . . .
A.2.2 Bruit gaussien . . .

. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

A.2 Moteur steady-state

B Comparaison d’opérateurs

C Mutation par brassage

C.1 Moteur générationnel
C.2 Moteur steady-state

. . .
. . . .

. . . . . .
. . . . . .

. . . . .
. . . . .

. . . . . .
. . . . . .

. . . . .
. . . . .

. . . . .
. . . . .

D Analyse de la variance (Kruskal-Wallis)

D.1 Taille de population . . . .

D.2 Probabilité de croisement
D.3 Probabilité de mutation

.
. .

. . . . . .
. . .
D.1.1 Algorithme générationnel
D.1.2 Algorithme steady-state . . . .
. . . . . .
. . . . . .
D.3.1 Algorithme générationnel
. . .
D.3.2 Algorithme steady-state . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .

D.4 Autres tests . . . .
D.5 Tableaux associés

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

E Note sur la représentation binaire des nombres réels

165

171

187

189
189
189
199
209
209
218

227

231
231
234

239
240
240
240
241
241
241
241
242
242

257

Sigles

Nous utilisons des sigles qui proviennent des termes anglais (en italique). L’équivalent de ces

termes en français est donné en dessous.

AES

ARG

COIL
CX

DPX

EA

ES

GA

GGA

GP

+

− δE((v1, v2), (m(v1), m(v2))

δE((v1, v2), (m(v1), m0(v2))

(4.4)

En conséquence, l’évaluation d’un tel échange se fait en un temps linéaire contrairement à l’éva-
luation du chromosome entier qui nécessite un temps quadratique en fonction du nombre de som-
mets (cf. Sec. 4.1.1, p. 111). Aussi, l’algorithme two-opt est un opérateur de mutation intelligente
effectuant la meilleure transposition.

Adaptation à l’approche par couleur Dans le cas des opérateurs par couleur (cf. Sec. 2.8, p. 54),
un sommet peut être remplacé uniquement par un sommet de la même classe, réduisant la taille de
l’espace de recherche sur les solutions faisables. Ceci s’applique uniquement aux applications dans
le cadre desquelles une classiﬁcation des sommets est disponible. Nous avons également modiﬁé
notre algorithme de recherche locale aﬁn qu’il tienne compte de telles restrictions dans ce cas. Ceci
réduit la complexité de la recherche locale car le nombre d’échanges à comparer est réduit.

4.1.1 Complexité théorique

Soit n la taille de graphes (et donc aussi la longueur des chromosomes). Chaque appel à la fonc-
tion d’évaluation nécessite l’addition de toutes les distances de sommets (il y a donc n distances à
calculer) ainsi que toutes celles d’arêtes (C 2
n). Par conséquent, la complexité d’un appel à la fonction
. A chaque génération, on calcule la fonction d’évalua-
tion une fois par individu : le nombre d’appels à la fonction d’évaluation est égal à la taille de la

d’évaluation est O(cid:0)n + C 2

n(cid:1) = n + n(n−1)

2

112

CHAPITRE4. RECHERCHELOCALE

population (hors d’éventuels chromosomes élite que l’on garde en mémoire sans les soumettre aux
opérateurs génétiques). Soit sp la taille de la population hors individus élite, alors la complexité
totale des appels de la fonction d’évaluation pendant une génération est sp × (n + n(n−1)

).

2

2

n = n(n−1)

Pour la phase d’échanges two-opt, on a besoin de vériﬁer toutes les possibilités de choisir deux
sommets parmi tous les sommets du graphe, ce qui représente C 2
échanges possibles
par chromosome. Chacun de ces échanges nécessite le calcul de O (4 + 4(n − 2)) distances élé-
mentaires. On calcule en effet les distances des deux sommets échangés sous l’appariement avant
et après l’échange (4) puis on calcule les distances entre toutes les arêtes incidentes à chacun des
sommets également avant et après l’échange. En effet, l’arête entre les sommets reste constante 8, ce
qui mène à la valeur de 4(n − 2). Si l’on applique la recherche locale à tous les chromosomes, alors
la complexité est O(cid:16)sp × n(n−1)
Le quotient entre la complexité de la recherche locale et le coût des appels à la fonction d’évalua-
tion est de l’ordre de O(n), donc la recherche locale est de l’ordre O(n) fois plus coûteuse que les
appels à la fonction d’évaluation (quand la recherche locale est effectuée pour chaque chromosome),
comme démontré ci-dessous.

2 × (4 + 4(n − 2))(cid:17).

Démonstration. Soit sp la taille de la population et n la longueur des chromosomes.

O(cid:18)recherche locale

ﬁtness

(cid:19) =

=

=

sp × C 2

n×(n−1)

2

n × (4 + 4 × (n − 2))
sp × (n + C 2
n)
× (4 + 4 × (n − 2))
n + n×(n−1)

2

4 6 n(n − 1)(1 + n − 2)

2 6 n+ 6 n(n − 1)

4(n − 1)2
=
n + 1
≈ O(4n)

Toutefois, on peut encore réduire le complexité de deux façons : premièrement en précalculant
toutes les distances entre sommets et arêtes affectés. Ceci réduira la complexité de la recherche
locale pour un seul individu à

O(cid:18)C 2

n × (2 + 2 × (n − 2)) +

1
2

(n2 + n)(cid:19) .

(4.5)

En effet, à l’intérieur de la recherche locale, on calcule désormais uniquement les affectations
changées, tandis que les distances des affectations actuelles sont reprises du calcul précédent de la
fonction d’évaluation. On obtient la moitié de la complexité précédente :

8. Dans un graphe orienté ce qui n’est pas le cas ici, elle change l’orientation.

4.2. STRATÉGIESDESÉLECTIONPOURLARECHERCHELOCALE

113

O(cid:18)recherche locale

ﬁtness

(cid:19) =

sp × C 2

n × (2 + 2(n − 2)) + 1

2 (n2 + n)

sp × (n + C 2
n)

2 6 n(n − 1)(n + 1)+ 6 (n)(n + 1)

=

2 6 n+ 6 n(n − 1)
= 2(n − 1) + 1 = 2n − 1
≈ O(2n)

Deuxièmement, si l’on possède une classiﬁcation des nœuds, il est possible de l’utiliser aﬁn de
réduire le nombre des échanges à vériﬁer. Étant donné une classiﬁcation des nœuds en k classes dont
i=1 ni = n), on réduit le nombre d’échanges à évaluer : au lieu

. Ceci est beaucoup plus favorable pour la recherche locale.

chaque classe contient ni nœuds (Pk

i=1 C 2
ni

de C 2

n on obtientPk

4.1.2 Complexité en pratique

Le quotient théorique entre la complexité de la recherche locale et la complexité d’un appel à
la fonction d’évaluation et le quotient obtenu par expérimentations sont semblables, même s’il y
a un léger avantage pour la recherche locale (LS) en pratique. Comment peut-on expliquer que la
recherche locale soit plus rapide qu’elle ne devrait l’être en théorie ? Apparemment, la recherche
locale exploite plus efﬁcacement le processeur, la charge moyenne étant supérieure dans un environ-
nement parallèle. Nous constatons une utilisation des processeurs à 100% avec la recherche locale et
strictement inférieure à 100% sans. Il y a plusieurs pistes possibles, dont notamment une meilleure
utilisation du cache, car un seul individu est traité répétitivement au lieu de changer la solution à
chaque fois, ou encore le taux de remplissage de la pipeline, qui doit être supérieur grâce à des opé-
rations ultérieures qui sont plus faciles à prédire. De toute façon, les différences sont dues aux effets
techniques liés au matériel, ce qui n’est pas le sujet de cette thèse.

4.2 Stratégies de sélection pour la recherche locale

L’importance de l’équilibre entre la recherche locale et la fonction d’évaluation a déjà été souli-
gnée dans l’introduction de ce chapitre. Aﬁn d’obtenir une estimation du rapport du coût entre l’EA
et la recherche locale, nous avons mis en rapport la complexité des appels à la fonction d’évaluation
avec celle de la recherche locale. En fait, le calcul de la fonction d’évaluation est la partie la plus
coûteuse de l’EA et elle est exécutée une fois pour chaque individu. Quant à la recherche locale
(pour laquelle nous avons considéré jusqu’ici qu’elle est exécutée pour chaque individu), elle ﬁgure
uniquement dans les algorithmes mémétiques et est encore plus coûteuse. Par conséquent, si l’on
veut équilibrer le temps de calcul consacré aux deux tâches, il faut réduire le nombre d’appels à la

114

CHAPITRE4. RECHERCHELOCALE

procédure de LS. Nous étudierons par la suite les réductions possibles avec différentes stratégies de
sélection.

4.2.1 Choix des individus

Aﬁn d’aborder la question du choix des individus, nous expérimentons plusieurs stratégies de
sélection des individus pour la recherche locale. Toutes nécessitent un paramètre supplémentaire, la
probabilité de recherche locale, ainsi que certaines adaptations à l’algorithme génétique classique.
Certaines mènent à un besoin en mémoire plus élevé, d’autres ralentissent légèrement l’exécution
en raison d’étapes de calcul supplémentaires. Les stratégies sont les suivantes :

GGA Le GA générationnel (GA générationnel (GGA)) est la variante la plus simple : après avoir
été créé, un individu est soumis à la recherche locale avec une probabilité ﬁxe. La sélection des
individus est alors uniforme et indépendante de leur qualité. S’il existe des copies dans la population,
la procédure de recherche locale est appliquée séparément à chaque copie.

UGGA L’idée de l’GGA unique (UGGA) est d’éviter une exécution répétée de la recherche locale
sur des individus identiques. Nous utilisons alors une table de hachage permettant de vériﬁer efﬁca-
cement la présence d’individus identiques. Quand on identiﬁe un duplicata, il y a deux possibilités :
d’une part, on peut copier le résultat de la recherche locale déjà effectuée sur son jumeau. De cette
façon, on inﬂuence la sélection en donnant à la population un biais fort vers ce chromosome. La
pression sélective peut donc devenir énorme surtout en combinaison avec une probabilité élevée.
D’autre part, on peut seulement ignorer toute copie en appliquant la recherche locale uniquement
aux individus uniques et aux premiers jumeaux. De cette manière, la décision de savoir si l’amélio-
ration apportée au premier individu est maintenue ou non est laissée à la sélection naturelle. Nous
utilisons la deuxième stratégie, car la pression de sélection est déjà très forte. Les individus ignorés
induisent néanmoins un biais sur l’exactitude de la probabilité de recherche locale. Si ces individus
dupliqués sont nombreux, la probabilité observée peut devenir en effet plus petite que la probabilité
choisie (car des individus initialement choisis pour la LS sont potentiellement rejetés par la vériﬁca-
tion d’unicité). Nous adaptons alors cette stratégie en utilisant la probabilité comme une proportion
du nombre d’individus et en appliquant la recherche locale à exactement ce quota de la population.
Par conséquent, la probabilité est généralement respectée. Toutefois, dans le cas où le nombre de
copies dans la population dépasse le nombre d’individus sur lesquels la recherche locale n’est pas
appliquée, elle est réduite. La sélection des individus reste indépendante de leur ﬁtness.

SGGA Contrairement aux deux stratégies précédentes, le but du GGA trié (SGGA) est de concen-
trer la recherche locale sur les meilleurs individus uniquement. Comme les algorithmes génétiques
sont des méthodes bien adaptées pour l’exploration globale avec un léger déﬁcit dans la convergence

4.3. EXPÉRIMENTATION

115

vers l’optimum, cette stratégie pourrait donner un avantage supplémentaire. Les meilleurs individus
méritent une recherche plus approfondie autour d’eux tandis que les autres continuent à explorer
les autres régions de l’espace de recherche. Notre algorithme est basé sur une population triée se-
lon la ﬁtness, ce qui impose une légère augmentation du temps de calcul. Comme pour UGGA la
probabilité de recherche locale est alors déﬁnie comme une proportion sur la taille de la population.

USGGA L’GGA unique et trié (USGGA) combine les deux variantes précédentes. Nous utilisons
une population triée et appliquons la recherche locale aux meilleurs individus sans prendre compte
leurs copies. Contrairement au UGGA et grâce au triage de la population, nous n’avons pas besoin
de table de hachage. Pour l’identiﬁcation des copies, il sufﬁt de comparer l’individu actuel avec son
précédent dans la population triée.

4.3 Expérimentation

Nous comparons expérimentalement les quatre stratégies présentées précédemment. Pour cela,
il faut d’abord étudier l’inﬂuence des différentes valeurs de la probabilité de recherche locale sur
les différents opérateurs de croisement avec lesquels nous combinons la recherche locale. Pour cela,
nous choisissons une taille de graphe de 60 car pour des tailles plus petites, l’algorithme génétique
sans recherche locale a déjà une précision quasi parfaite. Comme la recherche locale est assez coû-
teuse, elle n’améliore pas le temps de calcul pour les graphes plus petits (en particulier de taille 40
comme auparavant). Cela ne serait donc pas avantageux. À la taille 60, un GGA avec l’opérateur de
croisement UPBX atteint une précision d’environ 67% contre 78% pour un GA steady-state (SSGA).
Ces valeurs laissent de la place pour des améliorations sans être trop faibles.

Nous examinons le SSGA séparément des autres versions. D’une part, sa base, un EA steady-
state au lieu d’un EA générationnel, est différente. D’autre part, comme on n’a qu’un seul individu
à la fois, avec une population triée et qu’elle ne contient pas de copies par déﬁnition, la recherche
locale s’intègre simplement après la création d’un nouvel individu avec la probabilité donnée.

Nous comparons les différentes stratégies avec des paramètres variant de 1% à 100% pour l’ap-
plication de recherche locale sur les individus. Nous limitons le temps de calcul à travers un nombre
maximal d’appels à la fonction d’évaluation. En effet, pour la version sans recherche locale nous
permettons un total d’un million d’appels à la fonction d’évaluation.

Pour une comparaison juste entre des algorithmes avec ou sans recherche locale (et entre algo-
rithmes avec LS, mais avec des probabilités différentes), il faut poser les mêmes limites sur la durée
d’exécution totale indépendamment de la répartition entre recherche génétique et recherche locale.
Nous utilisons les résultats sur la complexité théorique de la LS aﬁn de calculer une approximation
du nombre d’appels à la fonction d’évaluation sans recherche locale. Cela correspond au même ef-
fort de calcul qu’un algorithme intégrant la LS. Soit f le nombre d’appels à la fonction d’évaluation,

116

CHAPITRE4. RECHERCHELOCALE

UPBX - 60 sommets - Bruit unif. 6

1

0.8

0.6

0.4

0.2

s
è
c
c
u
s

e
d
x
u
a
T

0

 

Sans LS 1

5

10
Probabilité de recherche locale en pourcentage

20

40

50

30

60

70

 

GGA
UGGA
SGGA
USGGA
SSGA

80

90

100

FIGURE 4.1 – Inﬂuence de la probabilité de recherche locale - croisement UPBX - par type de
moteur d’évolution : GGA - générationnel ; UGGA - générationnel unique ; SGGA - générationnel
trié ; USGGA - générationnel trié unique ; SSGA - steady-state

pRL la probabilité de recherche locale et n la taille du graphe, le nombre d’équivalents d’appels
est alors f0 = f (1 + 2pRL × n). Le nombre maximal des générations diminue alors naturellement
avec la probabilité autant que le coût de la recherche locale l’impose. En bref, le nombre d’appels
à la fonction d’évaluation diminue avec des probabilités de LS croissantes autant que le coût de la
recherche locale augmente.

4.3.1 Croisement UPBX

La ﬁgure 4.1 illustre les résultats pour UPBX. Pour SSGA nous constatons que la précision
diminue avec des paramètres élevés. A partir de 20%, elle baisse à près de 0. L’utilisation de la
recherche locale n’est donc pas favorable. De même les versions UGGA et GGA n’en proﬁtent
pas, bien que l’on constate pour les probabilités très élevées (à partir de 40% - 50%) une légère
régénération sur un niveau très faible. Quant aux versions triées, SGGA et USGGA, elles proﬁtent
de la recherche locale et atteignent des taux de succès de respectivement plus de 88% et 85%.
L’amélioration est toutefois très dépendante de la probabilité choisie, avec un optimum autour de
20%. Il est intéressant de noter qu’une probabilité très faible, autour de 1%, fait perdre sensiblement
de précision à l’algorithme. Quand on passe à 5%, cette perte est plus que compensée.

Moteur steady-state Les moteurs steady-state ne bénéﬁcient quasiment pas de la recherche locale.
Seulement avec un très petit taux, de 1%, nous observons une légère amélioration de la précision

4.3. EXPÉRIMENTATION

117

(cf. Fig. 4.1, p. 116). Notre implémentation est basée sur une population sans copies dont le pire
individu est remplacé par le nouveau descendant. Les autres individus sont affectés uniquement si
leur ﬁtness est inférieure à la ﬁtness du nouvel individu. Dans ce cas, leur rang dans la population
baisse d’une position. L’intégration répétée des bons individus mène à l’exclusion des individus
moins bien placés, tandis que les meilleurs sont toujours maintenus.

La recherche locale a deux effets potentiels. D’une part, les descendants sont uniformisés : s’il
y a une correspondance entre sommets spéciﬁques dont la distance est très petite, celle-ci est favo-
risée par la recherche locale et sera présente dans une grande partie des descendants, même si un
optimum global se trouve avec une autre conﬁguration. D’autre part, la recherche locale introduit
dans la population des bonnes solutions avec très peu de différences aux individus déjà présents.
La distribution des valeurs de ﬁtness perd donc un peu de sa variance. De cette manière la pression
de sélection, qui est déjà assez élevée pour ce type d’algorithme, est encore augmentée. Il est donc
assez difﬁcile pour un individu assez bon venant d’une autre région prometteuse de s’intégrer dans
la population, même si près de lui se trouve une meilleure solution. La recherche devient alors plus
locale et perd l’avantage de la bonne exploration des EA.

4.3.2 Croisement DPX

Le croisement DPX obtient déjà un taux de succès de 1 sans recherche locale. Une amélioration
n’est plus possible. Nous nous intéressons donc à l’effet sur le temps de calcul. La ﬁgure 4.2 montre
le temps de calcul en fonction de la probabilité de recherche locale. Nous constatons une baisse
importante pour des probabilités assez faibles autour de 0,01 (pour les moteurs GGA et SSGA) ou
0,05 (pour les moteurs UGGA, SGGA, et USGGA). Pour des probabilités plus élevées, le temps de
calcul augmente. Le gain est maximal pour les moteurs SGGA et USGGA.

4.3.3 Comportement des autres opérateurs de croisement

Tous les résultats présentés jusqu’ici ne sont applicables qu’avec l’opérateur UPBX et DPX.
Les autres opérateurs montrent un comportement différent. UPMX dont la précision sans recherche
locale est quasiment nulle pour les graphes de taille 60 reste encore très près de UPBX. Une proba-
bilité de recherche locale faible, en combinaison avec les stratégies SGGA et USGGA, augmente le
taux de succès jusqu’à 0,6. Contrairement au cas précédent, nous constatons une différence entre la
version USGGA et SGGA. De meilleures performances sont obtenues avec la stratégie USGGA.

En ce qui concerne l’opérateur CX, l’ajout de la recherche n’est avantageux avec aucun des
paramètres choisis. Donc, en général, il semble qu’une faible probabilité de recherche locale soit
avantageuse, mais une vériﬁcation avec l’opérateur choisi peut être nécessaire.

118

CHAPITRE4. RECHERCHELOCALE

DPX - 60 sommets - Bruit unif. 6

 

GGA
UGGA
SGGA
USGGA
SSGA

1.5

1

0.5

s
e
d
n
o
c
e
s
n
e

l
u
c
l
a
c

e
d
s
p
m
e
T

0

 
0

10

20

30

40

50

Probabilité de recherche locale en pourcentage

60

70

80

90

100

FIGURE 4.2 – Inﬂuence de la probabilité de recherche locale sur le temps de calcul - croisement DPX
- par type de moteur d’évolution : GGA - générationnel ; UGGA - générationnel unique ; SGGA -
générationnel trié ; USGGA - générationnel trié unique ; SSGA - steady-state

4.4 Bilan

Dans ce chapitre, nous avons étudié les effets de la recherche locale sur l’algorithme pour l’appa-
riement évolutionnaire de graphes et constaté que les effets dépendent de l’opérateur de croisement.
Nous avons observé que la recherche locale peut améliorer la précision. Si le taux de succès est
déjà 1 alors une réduction du temps de calcul est possible. Toutefois, comme la complexité de la
recherche locale par rapport à une évaluation de la fonction d’évaluation est très élevée, il est né-
cessaire d’utiliser une probabilité assez faible aﬁn de garder un temps de calcul acceptable. Nous
constatons qu’en pratique le rapport entre les temps d’exécution de la recherche locale et de la fonc-
tion d’évaluation est sensiblement moins important qu’il ne devrait l’être en théorie : la recherche
locale est comparativement plus rapide qu’attendu. Il apparaît que les processeurs modernes utilisés
effectuent efﬁcacement des accès répétés à des zones proches en mémoire (tels que les accès né-
cessaires pour l’évaluation des petits changements dans la recherche locale), plutôt que des accès à
des zones éloignées (lors du changement d’individu). Nous constatons en particulier une charge de
processeur afﬁchée plus proche de 100%.

Le résultat clef pour le design d’un algorithme mémétique est d’une part, qu’il faut choisir
les meilleurs individus de la population pour la recherche locale. Parmi les différentes stratégies
d’implémentation, les stratégies basées sur le tri de la population apportent une amélioration.

Chapitre 5

Modèles parallèles

Il existe plusieurs modèles de parallélisation pour les algorithmes évolutionnaires (Konfršt,
2004). Comme ils opèrent sur un ensemble de solutions dont les individus sont mis en concurrence,
l’idée générale est de décomposer la population aﬁn d’appliquer les opérations sur les différentes
sous-populations en parallèle.

La seule opération qui ne permette pas de parallélisation sans changement des caractéristiques
de l’algorithme génétique, est la sélection. Par contre, l’opération la plus importante est l’appel à la
fonction d’évaluation qui a une complexité typiquement très élevée par rapport aux autres parties
de l’algorithme. Le parallélisme global émerge de l’idée de préserver toutes les caractéristiques du
GA et d’accélérer son exécution (Alba & Tomassini, 2002). Le schéma de sélection est centralisé
tandis que l’évaluation des individus est faite en parallèle. Le gain apporté par la parallélisation
des autres parties de l’algorithme, notamment les opérateurs génétiques, ne justiﬁe en général pas
l’effort comme l’afﬁrment les mêmes auteurs.

Nous présentons d’abord une classiﬁcation des modèles des algorithmes génétiques parallèles.
Nous introduisons par la suite les limites théoriques de l’accélération du modèle maître-esclave.
Dans la section 5.3, nous présentons les détails techniques de notre implémentation que nous utili-
sons par la suite aﬁn d’étudier le gain pratique sur différents ordinateurs. Nous étudions notamment
l’inﬂuence de la complexité du problème, à travers la taille de graphes et de population, ainsi que de
différentes stratégies de communication entre les processus. Avant de conclure, nous introduisons
deux possibilités de parallélisation au niveau applicatif.

5.1 Classiﬁcation

Alba & Tomassini (2002) proposent une classiﬁcation des différents modèles d’algorithmes gé-
nétiques parallèles. La parallélisation ne permet pas seulement l’accélération de l’algorithme mais,
selon l’implémentation choisie, une variété d’avantages qui sont :

• Trouver plusieurs solutions au même problème
119

120

CHAPITRE5. MODÈLESPARALLÈLES

FIGURE 5.1 – Parallèlisme global et modèle en îlots.

• Recherche parallèle avec plusieurs points de départ dans l’espace de recherche
• Simple parallélisation comme îlots ou voisinages
• Recherche plus efﬁcace même si l’on n’utilise pas de matériel parallèle
• Meilleure efﬁcacité par rapport aux algorithmes séquentiels dans de nombreux cas
• Simple coopération avec d’autres stratégies de recherche
• Accélération du temps d’exécution

La classiﬁcation des différentes approches se base d’une part sur la taille et le nombre des sous-
populations et d’autre part sur leurs interactions. Les EA sont habituellement panmictiques, c’est-à-
dire que les opérateurs sont appliqués à l’ensemble de la population, en particulier durant la sélection
et le remplacement des individus où chaque individu peut s’accoupler avec n’importe quel autre.

On distingue ainsi des approches de granularité ﬁne de celles d’une granularité élevée. L’inter-
action principale est la migration d’individus d’une sous-population vers une autre, basée sur des
modèles de voisinage entre sous-populations. De même l’utilisation d’une décomposition hiérar-
chique est possible. De plus, les sous-populations peuvent être isolées les unes des autres, pendant
la sélection et le croisement. En cas d’isolation, un individu ne peut être combiné qu’avec un autre
individu de la même sous-population. Ainsi, l’algorithme n’est pas panmictique.

Dans la littérature, on trouve parfois des noms différents pour certains algorithmes, ainsi le
modèle global est appelé maître-esclave dans Nowostawski & Poli (1999). Toutefois, le principe de
la classiﬁcation reste le même.

La ﬁgure 5.1 montre à gauche le principe du parallélisme global. Déjà l’initialisation peut être
faite en parallèle. Puis, la fonction d’évaluation est calculée indépendamment pour chaque individu.
La sélection globale nécessite le transfert des valeurs de ﬁtness (et des chromosomes) au processeur
maître. Une fois qu’il reçoit ces données, il détermine les parents pour la prochaine génération

5.2. GAINTHÉORIQUE

121

et les envoie aux processeurs esclaves. Ceux-ci appliquent les opérateurs génétiques et calculent
la fonction d’évaluation pour les nouvelles sous-populations. Finalement, les nouvelles valeurs de
ﬁtness sont envoyées au processeur maître et on répète la boucle jusqu’à atteindre le critère d’arrêt.
Une variante est de paralléliser uniquement l’application de la fonction d’évaluation car il s’agit de
l’opération la plus coûteuse.

Dans la même ﬁgure, à droite, l’idée générale du modèle en îlots (island model) est illustrée.
Un îlot consiste en une sous-population isolée des autres, par la mer, et est associé à un processeur.
Ainsi sur chaque îlot, un EA indépendant est exécuté. Périodiquement des individus traversent la
mer aﬁn d’arriver à l’îlot suivante. Le choix de ces individus peut se faire aléatoirement, ou selon
la ﬁtness ou même en fonction d’une notion d’âge. Le processus de changement de population est
nommé migration. Cette modèle se comporte différemment d’un EA séquentiel avec la même taille
de population globale (par exemple en ce qui concerne les caractéristiques suivantes : l’intensité de
sélection, et le risque d’optima locaux).

5.2 Gain théorique

Le gain de temps maximal que l’on peut obtenir avec la parallélisation d’un algorithme gé-
nétique a été étudié par Cantú-Paz (2007, cf. aussi Cantú-Paz & Goldberg, 1999). L’accélération
(speedup) parallèle est déﬁnie comme quotient entre le temps de calcul séquentiel et le temps paral-

lèle(cid:16) ts

tp(cid:17). Dans le cas idéal, la meilleure accélération possible est linéaire en nombre de processeurs,

ce qui correspond à la division du temps de calcul par le nombre de processeurs utilisées. Il faut tou-
tefois prendre en compte des temps de communication entre les processeurs. Si l’on considère le
modèle global, le processeur maître (celui qui gère la sélection) communique les différentes sous-
populations à traiter à chaque processeur esclave. Cette communication s’effectue en série car il ne
peut communiquer avec les autres processeurs qu’un après l’autre.

Cantú-Paz (2007) suppose une distribution uniforme des individus sur les processeurs, c’est-
à-dire que chaque processeur reçoit le même nombre d’individus. Cette hypothèse est en général
raisonnable et rend le calcul plus facile. Elle présuppose que le temps de communication soit négli-
geable par rapport au temps d’évaluation de la fonction d’évaluation. Le temps pour l’application
des opérateurs génétiques est ignoré, car considéré comme négligeable par rapport au calcul de la
fonction d’évaluation.

Il faudrait revoir cette hypothèse si le temps de communication devenait élevé, comme par
exemple si l’on souhaitait construire un algorithme distribué sur des ordinateurs distants ayant des
latences plus longues. Ceci serait également nécessaire dans le cas rare où la quantité d’informa-
tions à transmettre est très lourde (selon la taille de la population et la longueur des chromosomes
principalement) et le temps d’évaluation d’un individu est faible. On pourrait, dans ces cas encore,
minimiser le temps d’inactivité tel que les premiers processeurs contactés reçoivent plus d’individus

replacements

122

CHAPITRE5. MODÈLESPARALLÈLES

1
10
100
linéaire

103

n
o
i
t
a
r
é
l
é
c
c
A

102

101

100

 
100

 

101

102

Nombre de processeurs

103

FIGURE 5.2 – Accélération théorique pour des différents rapports entre temps de calcul et temps de
communication (Cantú-Paz & Goldberg, 1999; Cantú-Paz, 2007) en double échelle logarithmique.
L’accélération linéaire est un cas idéalisé, uniquement atteignable avec un temps de communication
de zéro.

que les derniers, dont notamment le processeur maître. Ce dernier ne peut commencer à travailler
sur sa sous-population qu’après avoir terminé la communication avec le dernier processeur esclave.
Le premier résultat important est qu’il n’est pas toujours intéressant d’utiliser plus de proces-
seurs. Le nombre de processeurs optimal peut être exprimé en fonction du rapport entre le temps de
calcul de la fonction d’évaluation et le temps de communication. Quand on ajoute des processeurs,
le temps de calcul diminue mais le temps de communication augmente. Soit sp la taille de la popu-
lation, tf le temps de calcul d’un appel à la fonction d’évaluation et tc le temps de communication
par sous-population, le nombre optimal de processeurs est :

P =r sptf

tc

(5.1)

La ﬁgure 5.2 montre l’accélération possible en fonction des nombres de processeurs avec des
paramètres différents pour le rapport tf
en double échelle logarithmique. L’accélération maximale
tc
s’exprime en fonction du nombre optimal des processeurs comme 1
2 P . Dans ce cas, la moitié du
temps est utilisée pour la communication et non pour le calcul. Ceci mène à deux conclusions.
D’une part, on pourrait encore s’interroger sur la question d’optimiser la distribution de charge entre
les processeurs en fonction du début de calcul (surtout au cas où tf
= 1). D’autre part, on va
tc
probablement préférer un nombre de processeurs moins élevé pour lequel l’accélération reste plus
près de l’accélération linéaire et les processeurs individuels seront donc mieux exploités dans la

5.3. APPROCHEPROPOSÉE

plupart des cas pratiques.

123

En ce qui concerne les autres modèles parallèles une estimation exacte de l’accélération de-
vient plus compliquée. D’une part, chaque processeur exécute son propre algorithme génétique, ce
qui élimine une partie du temps de communication 9 parce que les sous-populations ne sont pas
échangées complètement. D’autre part, les propriétés de l’algorithme changent selon la topologie
des sous-populations, ce qui rend nécessaire une adaptation des paramètres. En particulier, il n’y
a plus d’équivalence avec un algorithme séquentiel. La sélection n’est plus globale, aussi le temps
de convergence peut changer. La pression de la sélection change avec la migration. Si l’on choisit
pour la migration par exemple les individus en fonction de leur ﬁtness, ce qui est normalement le
cas, on augmente directement la pression menant à une convergence plus rapide. Une comparaison
des quatre stratégies de migration principales focalisée sur le changement de la pression sélective se
trouve dans Cantú-Paz (2001). Le nombre optimal de processeurs reste asymptotiquement le même
que dans le cas de la parallélisation globale (Cantú-Paz & Goldberg, 1999).

Dans certains cas, une accélération super-linéaire peut être observée mais pour les raisons précé-
dentes ces comparaisons ne sont en général pas honnêtes vis à vis de la version séquentielle (Cantú-
Paz, 2007). Une autre possibilité d’une meilleure accélération que celle théoriquement possible, peut
être due au fait que les sous-populations rentrent complètement dans le cache du processeur tandis
que la population globale nécessite l’utilisation de la mémoire vive qui est beaucoup plus lente.

5.3 Approche proposée

La parallélisation des EA et de notre algorithme pour le problème d’appariement de graphes en
particulier est relativement simple à implémenter. Nous nous intéressons surtout à la parallélisation à
l’intérieur de l’EA. La section 5.5 introduit d’autres possibilités au niveau applicatif. La population
peut être divisée en sous-populations. Cette approche est très générale car non-dépendante d’une
application précise.

Il n’existe qu’une seule opération qui nécessite l’intégralité de la population en même temps :
la sélection. Plus précisément, ce n’est pas la population qui est indispensable, mais les valeurs de
la ﬁtness de chaque individu. Toutes les autres opérations, y compris les plus coûteuses comme la
recherche locale et la fonction d’évaluation, sont déﬁnies sur les individus (à l’exception du croi-
sement nécessitant deux parents pour créer un enfant). L’idée fondamentale est de démarrer autant
de processus que l’on possède de processeurs après avoir sélectionné les individus qui participeront
à la reproduction. Cet ensemble de parents est ainsi distribué également sur tous les processeurs
avec approximativement le même nombre d’opérations de chaque type à exécuter. En conséquence,
le temps de calcul sur chaque processeur sera à priori égal, ce que permet une exploitation quasi-
optimale des ressources, car il n’y aura pas de temps d’attente après que les processus aient terminé.

9. Et potentiellement du temps d’inactivité sous conditions qu’il ne faille pas trop synchroniser.

124

CHAPITRE5. MODÈLESPARALLÈLES

En réalité, l’éventuel temps d’attente est une source majeure de perte de performance lors de la syn-
chronisation. En ce qui concerne la synchronisation des données, nécessaire aﬁn qu’aucun processus
ne puisse corrompre les données des autres, elle peut être assez limitée. En effet, si l’on s’assure que
le croisement, la mutation et la recherche locale créent des nouveaux individus au lieu d’altérer les
existants, on n’a pas besoin de synchroniser le tout. Chaque processus possède désormais sa propre
liste d’individus et sa liste des valeurs de ﬁtness de ceux-ci.

Le surcoût provient de la fusion des listes aﬁn de faire la sélection (étant faite par une liste
d’indices au lieu de copier les individus). Après la sélection, les nouveaux parents sont redistribués
parmi les différents processus. Le surcoût en mémoire est limité aux objets nécessaires pour dé-
marrer les nouveaux processus ainsi que les références redondantes (qui sont nécessaires aﬁn que
chaque processus puisse accéder aux parents dont il a besoin) sur les individus.

Quant au générateur de nombres aléatoires, certains opérateurs nécessitent un grand nombre
d’appels à lui. Comme il est nécessairement synchronisé, il risque d’être un goulot d’étranglement
s’il y a des appels concurrents venant des différents processus. Aﬁn d’éviter tout appel concurrent,
chaque processus possède son propre générateur.

5.4 Gain pratique - expérimentation

Pour étudier la performance à large échelle de notre EA, nous utilisons une grappe de calcul
hétérogène, constituée de 17 nœuds qui se basent sur six types de processeurs. Aﬁn d’obtenir des ré-
sultats homogènes, nous utilisons en particulier les nœuds équipés de processeurs Intel Xeon X7350
Tigerton (2.93 GHz, 4 noyaux, 4Mo cache L2 par noyau) et Intel Xeon Tulsa (3.4 GHz, 2 noyaux,
2Mo cache L2 par noyau, 16Mo cache L3). Les nœuds utilisés ont 16 noyaux chacun. Le système
d’exploitation est Linux. Il faut aussi noter que notre algorithme est écrit en Java. Par conséquent, un
accès direct aux ressources n’est pas possible, on n’a notamment pas de contrôle sur la distribution
des processus sur les processeurs. La performance est dépendante de l’implémentation de la machine
virtuelle. Nous utilisons la Java Hotspot VM dans la version 1.6. Dans les dernières comparaisons
avec d’autres langues de programmation, Java s’est montré extrêmement efﬁcace (Amedro et al.,
2008).

Nous avons choisi une base de graphes réels. Il s’agit des graphes extraits d’une base de données
de 42000 molécules 10 au total. Nous utilisons plusieurs sous-ensembles contenant tous des molé-
cules d’une taille spéciﬁque. L’algorithme est lancé trente fois sur chaque paire de graphes après un
ﬁltrage qui consiste à éliminer les graphes ayant une distribution différente d’atomes (c’est-à-dire un
nombre différent pour au moins un type d’atomes). En effet, il s’agit d’une mesure qui permet la li-
mitation de l’espace de recherche basé sur une signature (cf. Sec. 2.8, p. 54). De plus, si deux graphes
ont des signatures non-identiques, alors on n’a pas besoin de chercher un appariement, puisque l’on

10. NCI AIDS antiviral screen.

5.4. GAINPRATIQUE-EXPÉRIMENTATION

125

peut directement conclure qu’il s’agit de deux graphes différents. Dans l’autre cas uniquement, la
recherche de l’appariement pour la décision d’isomorphisme est nécessaire.

La diminution du temps de calcul dépend en pratique de plusieurs facteurs. Dans cette section,
nous étudions certains d’entre eux qui peuvent aider à décider si la parallélisation est avantageuse
dans une application précise. La question principale étant : le gain en vitesse justiﬁe-t-il le coût sup-
plémentaire (notamment l’utilisation d’ordinateurs parallèles très coûteux) ? De manière générale,
la parallélisation est avantageuse si le calcul à distribuer est assez lourd. En termes techniques, il
faut un minimum de travail dans chaque processus pour justiﬁer la distribution. Les processus longs
sont préférables aux processus plus courts.

Le calcul principal se passe au niveau de la fonction d’évaluation. Il devient plus coûteux quand
la taille des graphes augmente. Comme la parallélisation se fait par génération, la taille de la po-
pulation joue aussi un rôle, car elle détermine combien d’appels à la fonction d’évaluation sont
distribués.

5.4.1 Taille des graphes

Nous étudions d’abord l’inﬂuence de la taille de graphes. L’accélération pour des tailles de
graphes de 22, 30 et 50 sommets est illustrée sur les ﬁgures 5.3 et 5.4. Les expériences sont effectuées
en utilisant la taille de population 121, dont un individu élite sur lequel les opérateurs ne sont pas
appliqués. De fait, cette valeur assure que le nombre d’opérations est divisible par la plupart de
nombres de 1 à 8 (le nombre de processeurs) ce qui mène à des sous-populations de taille égale.

Nous constatons que l’accélération est effectivement plus forte pour des tailles plus élevées.
Entre les résultats obtenus avec les processeurs Tulsa et Tigerton, on peut noter que les processeurs
Tulsa donnent un gain supérieur. Les processeurs Tigerton, qui sont a priori environ deux fois plus
rapides, ne donnent quasiment pas de gain en vitesse lorsque l’on passe d’un seul à deux. L’accé-
lération est ensuite comme attendu. Toutefois, avec quatre processeurs, l’algorithme est seulement
deux fois plus rapide. Pour des graphes de taille 22, le nombre optimal de processeurs est quatre (les
gains que l’on observe en utilisant 5 et 6 processeurs étant négligeables). Quand la taille augmente,
alors l’algorithme peut proﬁter de plus de processeurs (5 pour la taille 30, 8 à taille 50).

Les processeurs Tulsa proﬁtent dès le début de l’ajout de processeurs. A part cela, nous consta-
tons le même comportement qu’avec les processeurs Tigerton. L’accélération est plus grande, mais
les processeurs sont a priori moins performants. En utilisant trois processeurs l’algorithme est déjà
entre 2.5 fois (un peu moins pour la taille 22, un peu plus pour la taille 30) et presque 3 fois (pour
la taille 50) plus rapide. Lorsque l’on utilise encore plus de processeurs, l’accélération s’éloigne de
son optimum théorique.

En résumé, la parallélisation est plus avantageuse pour de grands graphes que pour des petits.
Avec les tailles de graphes testées, nous observons un comportement presque idéal pour les petits

126

CHAPITRE5. MODÈLESPARALLÈLES

Population 121 - Intel Xeon X7350 (Tigerton) 2.93 GHz

 

Taille 22
Taille 30
Taille 50

4

3.5

3

2.5

2

1.5

n
o
i
t
a
r
e
l
é
c
c
A

1

 
1

2

6
3
Nombre de processeurs (de noyaux)

5

4

7

8

FIGURE 5.3 – Accélération par rapport à la taille des graphes (processeurs Tigerton).

nombres de processeurs. En effet, on s’approche plus du cas idéal avec des graphes plus grands.
Quand on traite des problèmes plus complexes, le nombre de processeurs dont on peut proﬁter
efﬁcacement augmente.

5.4.2 Taille de la population

Nous étudions ensuite l’inﬂuence de la taille de la population. L’étude théorique 11 sur l’accé-
lération maximale et sur le nombre optimal de processeurs suppose que la taille de la population
soit assez grande (1000 individus). Cependant, les tailles qui mènent à des bonnes performances en
termes de précision et temps de calcul (cf. Ch. 3, p. 59 et suivantes), ainsi que celles mentionnées
dans la plupart des travaux, sont souvent plus petites. En effet, la taille de la population est choisie en
fonction des performances de l’algorithme et non aﬁn de permettre une meilleure accélération. Les
différents applications et opérateurs nécessitent des tailles bien différentes (par exemple 400 pour un
GGA-CX contre 10 pour un GGA-UOX). Il est donc néanmoins intéressant d’observer la relation
entre la taille de la population et l’accélération. De même, les expériences sur la taille de population
donnent une indication sur les effets prévisibles de la taille du problème. Comme en effet, l’effort
de computation ne croît pas linéairement avec la taille de graphes, ceci est le cas pour la taille de la
population.

Nous observons que les différences entre les types de processeurs subsistent. C’est-à-dire l’uti-
lisation de deux processeurs Tigerton n’augmente que très peu la vitesse et l’accélération est de

11. cf. Sec. 5.2, p. 121.

5.4. GAINPRATIQUE-EXPÉRIMENTATION

Population 121 - Intel Xeon (Tulsa) 3.4GHz

4

3.5

3

2.5

2

1.5

n
o
i
t
a
r
e
l
é
c
c
A

1

 
1

2

6
3
Nombre de processeurs (de noyaux)

4

5

127

 

Taille 22
Taille 30
Taille 50

7

8

FIGURE 5.4 – Accélération par rapport à la taille des graphes (processeurs Tulsa).

manière générale plus faible que sur les processeurs Tulsa. Le comportement général reste néan-
moins comparable.

La ﬁgure 5.5 montre l’effet de la taille de la population sur la vitesse pour un graphe de taille 22.
Les tailles de population utilisées sont 100, 500, 1000 et 1500 individus. Nous constatons qu’une
petite taille mène à une accélération plus faible, tandis qu’un algorithme utilisant une grande taille
accélère plus. Pour la taille 1000 par exemple, l’algorithme est un peu moins de trois fois plus rapide
avec trois processeurs. Quand on utilise huit processeurs, le gain n’augmente que peu.

De manière générale, sur les graphes de taille 20-50, l’utilisation de trois processeurs semble
approprié. Nous pouvons conclure que l’application de la parallélisation vaut la peine quand les
graphes sont grands et/ou la taille de la population est grande. Par contre, augmenter explicitement
de la taille de la population aﬁn de faire une bonne parallélisation ne paraît pas avantageux.

5.4.3 Communication entre processus

Après ces expériences générales, nous étudions aussi des optimisations possibles concernant
la communication entre processeurs. De fait, quand on copie un grand nombre de chromosomes
du processeur principal aux processeurs travailleurs, il se peut qu’une approche étape par étape
accélère l’exécution car le temps d’attente diminue. L’idée que nous suivons consiste à envoyer
uniquement les identiﬁants des parents et non les parents eux mêmes. De la sorte les processeurs
peuvent commencer à travailler plus rapidement, y compris le processeur maître. On tente donc de
réduire le temps d’inactivité.

Soit l la longueur des chromosomes et sp la taille de la population. On a alors besoin d’envoyer

128

CHAPITRE5. MODÈLESPARALLÈLES

4

3.5

3

2.5

2

1.5

n
o
i
t
a
r
e
l
é
c
c
A

1

 
1

2

22 sommets - Intel Xeon (Tulsa) 3.4GHz

 

Population 100
Population 500
Population 1000
Population 1500

7

8

3
6
Nombre de processeurs (de noyaux)

5

4

FIGURE 5.5 – Accélération par rapport à la taille de la population (processeurs Tulsa).

un tableau de taille l × sp. L’idée d’envoyer des identiﬁants réduit la taille du tableau à envoyer ini-
tialement à une valeur entre sp et 2sp selon la quantité de croisements à faire. On obtient notamment
2sp quand chaque enfant a deux parents (quand le croisement s’applique à 100% de la population).
On obtient sp si aucun enfant n’est créé en utilisant le croisement.

Bien que l’idée générale soit de copier uniquement les identiﬁants, au moment d’appliquer les
opérateurs, le processeur esclave a besoin de connaître les chromosomes. Nous étudions l’algo-
rithme parallèle sur une seule machine dont la mémoire est partagée : le processeur esclave cherche
alors lui-même les parents nécessaires pour la création du descendant suivant. Pour cela une syn-
chronisation d’accès est inévitable car la population des parents n’existe qu’une seule fois dans la
mémoire.

Un schéma très souvent utilisé dans les algorithmes génétiques est d’appliquer les opérateurs
l’un après l’autre sur l’ensemble de la population. On commence avec le croisement. Une fois que
ceci est fait sur toute la population, on applique la mutation sur les résultats, et ainsi de suite. Du
point de vue de la synchronisation, le problème avec ce schéma, que nous appelons ordre classique
par la suite, est que, comme le croisement est un opérateur très peu coûteux notamment par rapport
à la recherche locale et l’appel à la fonction d’évaluation, les demandes des parents ne sont pas bien
distribuées. Tous les processeurs esclaves essaient d’accéder aux parents au début de l’exécution.
Ceci crée des blocages qui ralentissent l’algorithme. Le nombre de blocages (et ainsi le ralentisse-
ment dû à la synchronisation) augmente avec le nombre de processeurs. La taille de la population
joue aussi un rôle : quand elle est faible, il y a peu de données à communiquer, aussi l’envoi de toute
la matrice des parents n’est pas coûteux.

5.4. GAINPRATIQUE-EXPÉRIMENTATION

22 sommets - Population 1500 - Intel Xeon (Tulsa) 3.4 GHz

129

 

4

3.5

3

2.5

2

1.5

n
o
i
t
a
r
e
l
é
c
c
A

1

 
1

2

Ordre classique, pas de sync
Ordre adapté, sync
Ordre classique, sync
Ordre adapté, pas de sync

3
6
Nombre de processeurs (de noyaux)

5

4

7

8

FIGURE 5.6 – Accélération par rapport à l’ordre (processeurs Tulsa).

Aﬁn d’uniformiser la distribution des accès et donc limiter le nombre de blocages, nous propo-
sons de changer l’ordre d’exécution. De fait, rien n’empêche de suivre un individu de la création par
le croisement jusqu’au calcul de la fonction d’évaluation pour ensuite continuer avec le prochain.
Nous effectuons donc une comparaison de ce nouvel ordre et de l’ordre classique, dans les deux cas
sans ou avec synchronisation. L’absence de synchronisation implique que les parents sont copiés
pour chaque processeur esclave par le processeur principal. Les ﬁgures 5.6 et 5.7 montrent les résul-
tats obtenus. Nous constatons en premier lieu que la stratégie la moins bonne est l’ordre classique
avec synchronisation. Ensuite nous observons que le changement de l’ordre est avantageux dans tous
les cas (avec ou sans synchronisation).

Pour les processeurs Tulsa, les différences entre les trois versions (autres que l’ordre classique
avec synchronisation) sont négligeables jusqu’à 7 processeurs parallèles. Quand on en utilise 8, nous
constatons un léger avantage pour l’ordre adapté sans synchronisation, devant l’ordre adapté avec
synchronisation.

Pour les processeurs Tigerton, les relations sont plus compliquées. Bien que l’ordre classique
sans synchronisation montre le comportement le plus cohérent, l’ordre adapté montre une meilleure
accélération (avec synchronisation). Sans synchronisation, l’accélération est aussi meilleure quand
on utilise peu de processeurs. Toutefois, quand on passe à 7, l’accélération diminue. Pour l’ordre
classique avec synchronisation, nous observons une accélération jusqu’à 5 processeurs, puis, la vi-
tesse baisse beaucoup. La meilleure stratégie est l’ordre adapté avec synchronisation, mais les diffé-
rences ne sont pas très élevées.

130

CHAPITRE5. MODÈLESPARALLÈLES

22 sommets - Population 1500 - Intel Xeon (Tigerton) 2.93 GHz

4

3.5

3

2.5

2

1.5

n
o
i
t
a
r
e
l
é
c
c
A

1

 
1

2

Ordre classique, pas de sync
Ordre adapté, sync
Ordre classique, sync
Ordre adapté, pas de sync

6
3
Nombre de processeurs (de noyaux)

5

4

 

7

8

FIGURE 5.7 – Accélération par rapport à l’ordre (processeurs Tigerton).

5.5 Parallélisation au niveau applicatif

Recherche d’un graphe dans une base Dans certaines applications il est possible de paralléliser
à un niveau plus haut. Par exemple si l’on souhaite identiﬁer un graphe donné parmi un ensemble des
graphes existants pour trouver celui qui correspond le mieux, on a besoin de plusieurs appariements.
Dans le pire cas, il faut le comparer à tous les graphes de la base. Dans ce cas, au lieu de distribuer
un algorithme sur les processeurs disponibles, on peut paralléliser au niveau de l’identiﬁcation. En
d’autres termes, on utilise l’algorithme génétique séquentiel mais on en utilise plusieurs instances
sur différentes paires de graphes en parallèle. La parallélisation se situe alors au niveau applicatif.
Les avantages principaux sont d’une part que les ﬁls d’exécution sont plus longs et d’autre part que
la communication entre processeurs est minimisée. Aussi dans le cadre de l’identiﬁcation on peut
observer l’évolution des distances : si par exemple une exécution arrive à une distance proche de 0,
alors il n’est plus nécessaire de suivre les autres. On peut alors interrompre de façon anticipée tous les
algorithmes qui tournent en fonction des résultats d’un seul. Ceci est particulièrement intéressant car
nous observons que le nombre de générations est moins élevé quand il s’agit des graphes similaires
que si l’on compare des graphes différents. Bien que ceci soit aussi vrai pour d’autres méthodes
comme les approches basées sur la recherche de l’arbre A*, ils ne permettent pas d’obtenir une
solution courante facilement.

Augmentation de la précision avec des populations indépendantes
Il peut également être avan-
tageux d’exécuter le même algorithme génétique sur une seule paire de graphes plusieurs fois aﬁn

5.6. BILAN

131

d’augmenter la probabilité de trouver la bonne solution. Par exemple, si l’on suppose que l’algo-
rithme obtienne la solution optimale avec une probabilité p de 50%, alors la probabilité d’obtenir la
solution optimale avec (au moins) un algorithme parmi n algorithmes identiques est de 1− (1− p)n.
Pour n = 2 on obtient 75%. Comme 1 − (1 − p)n tend asymptotiquement vers 1, on peut, avec le
choix d’un nombre d’algorithmes parallèles sufﬁsant, assurer qu’un trouve la solution optimale avec
n’importe quelle probabilité p < 1 donnée.

5.6 Bilan

Bien que la parallélisation d’un algorithme génétique semble évidente, il existe de très nom-
breux détails techniques à prendre en compte aﬁn que l’on puisse vraiment accélérer l’exécution.
Par exemple, le confort d’un langage de programmation orienté objet comme JAVA cache certaines
choses de bas niveau qui sont cependant importantes (optimisation de l’ordre de boucles, synchro-
nisation, distribution des ﬁls d’exécution sur les processeurs). Il est donc important de bien choisir
les technologies utilisées et surtout de valider les résultats. Nos expériences montrent que la qualité
de l’accélération dépend de la taille du problème et de la taille de la population. Avec les graphes de
taille moyenne que nous testons, l’utilisation de 3-5 processeurs semble appropriée. Si l’on traite des
graphes plus grands ou si l’on a besoin de tailles de population plus importantes, alors ce nombre
peut augmenter et on peut effectivement proﬁter de plus de processeurs. Finalement, selon les appli-
cations, il est possible de paralléliser non au niveau des générations mais au niveau des algorithmes.
Globalement, la parallélisation améliore la vitesse dans tous les cas. Il existe cependant une limite,
théorique et pratique, du nombre maximal de processeurs dont l’algorithme peut proﬁter. En ce
qui concerne la précision, et les autres propriétés de l’algorithme, elles restent inchangées car nous
avons choisi une parallélisation globale. En effet, d’autres types de parallélisation sont possibles
mais nécessitent une étude beaucoup plus approfondie car les propriétés de l’EA changent.

132

CHAPITRE5. MODÈLESPARALLÈLES

Chapitre 6

Garantie du résultat optimal

Malgré toutes les astuces que l’on peut utiliser aﬁn d’améliorer la convergence des EA, ces der-
niers restent des méthodes stochastiques dont le résultat dépend des nombreuses variables aléatoires.
Bien que l’on puisse augmenter la probabilité de trouver la solution optimale, on n’a pas de garantie
d’optimalité en général 12. Dans les chapitres précédents nous avons amélioré l’algorithme avec plu-
sieurs autres heuristiques qui ne garantissent pas l’obtention de la solution optimale. Des méthodes
dites optimales existent mais elles sont très lentes.

Notre proposition consiste à combiner l’algorithme génétique, qui obtient une bonne solution
assez rapidement, avec une méthode exacte, qui est plus lente mais garantit la solution optimale
(Bärecke & Detyniecki, 2007a). L’idée principale est d’utiliser une méthode de recherche d’arbre
qui permet de limiter les chemins explorés suivant une estimation de la distance maximale accep-
table entre les graphes. Il s’agit d’une distance pour laquelle on sait qu’il existe une solution cor-
respondante : l’algorithme génétique donne une estimation de départ et accélère ainsi la recherche
d’arbre.

Nous étudions d’abord la méthode optimale avec laquelle nous proposons de combiner, exami-
nant en particulier son paramètre, la distance maximale tolérée. Nous détaillons ensuite l’algorithme
hybride proposé.

6.1 Inﬂuence du seuil d’acceptation sur l’algorithme A*

Notre intérêt pour l’algorithme de Berretti et al. (2001) (cf. Sec. 1.5, p. 14) est basé sur le fait
qu’il garantit la solution optimale. Son seul paramètre applicatif est le seuil maximal d’accepta-
tion modélisant la différence maximale tolérée entre deux graphes, ce qui correspond dans le pro-
blème d’identiﬁcation au seuil sur lequel deux graphes peuvent être considérés comme différents.
On constate une corrélation directe entre ce seuil et le temps de calcul de l’algorithme. L’idée est

12. Dans certains cas des EA simples, on peut toutefois prouver l’optimalité mais sous l’hypothèse d’un temps de

calcul inﬁni (Rudolph, 1994).

133

134

CHAPITRE6. GARANTIEDURÉSULTATOPTIMAL

que la déﬁnition d’un seuil permet d’élaguer l’ensemble des appariements candidats. Il serait alors
souhaitable de réduire le seuil d’acceptation le plus possible aﬁn d’obtenir les meilleures perfor-
mances. Si le seuil est inférieur à la distance correspondante à la meilleure solution existante et donc
à la distance réelle entre les graphes en question, alors l’algorithme ne parvient pas à trouver de
solution. Dans ce cas, il valide simplement la non-existence d’une solution inférieure au seuil choisi
et le temps de calcul est considérablement réduit.

Nous cherchons à établir une analyse plus profonde de la corrélation entre le seuil et le temps de
calcul. Il est évident que le seuil est fortement dépendant de l’application concrète et en particulier
de l’amplitude des distorsions possibles. Nous utilisons une base de graphes aléatoires car une telle
base permet de ﬁxer le niveau de bruit et ainsi de calculer une espérance de la distance réelle. En
effet, nous cherchons à établir empiriquement un lien entre le temps de calcul et le rapport entre la
distance réelle et le seuil. Cela nécessite une base contenant des paires de graphes avec des distances
proches.

L’espérance mathématique de la distance entre les graphes se calcule en fonction de leur taille
et du niveau de bruit utilisé lors de leur création. Soit σ l’écart-type de la variable aléatoire qui
détermine le bruit ajouté, alors l’espérance de distance entre un graphe et le graphe correspondant
√3
perturbé est E = E(d(g1, g2)) =
4 σn(n + 1). Dans le cas du bruit uniforme le niveau de bruit
est déterminé par la longueur de l’intervalle [−ε; +ε]. Aﬁn d’assurer une espérance égale à celle du
bruit gaussien, alors on calcule ε en fonction de σ : ε = √3σ.

Nous examinons en particulier l’intervalle centré sur cette espérance. Nous déﬁnissons vingt
seuils équidistants dans l’intervalle [0, 1E + τ, 2, 0E + τ ]. Nous excluons le cas d’une espérance
de 0, qui correspond à σ = ε = 0, car il n’est pas intéressant en pratique. De fait, une espérance
de 0 transforme l’isomorphisme inexact en un problème d’isomorphisme exact pour lequel des mé-
thodes bien plus performantes existent (cf. Sec. 1.5, p. 14). Quand on lance l’algorithme de Berretti
et al. avec un seuil qui correspond exactement à la distance entre les graphes, alors l’algorithme ne
trouve pas de solution pour des raisons techniques (cf. Ann. E, p. 257). Nous introduisons donc une
tolérance numérique. Une valeur sufﬁsante pour le fonctionnement de tous les algorithmes dont en
particulier l’A* est τ = 10−10.

6.1.1 Expérimentation

D’abord nous visons à établir un lien entre le temps de calcul et le seuil d’acceptation. Nous
exprimons le seuil d’acceptation comme un multiple de l’espérance de la distance. En effet, ceci
permet de voir l’inﬂuence d’un seuil plus ou moins proche de la distance réelle (que l’on ne connaît
pas dans les cas réels). Deux cas sont à distinguer : d’une part, le seuil peut être supérieur ou égal à la
distance réelle. Une solution sera alors trouvée. D’autre part, le seuil peut être inférieur à la distance
réelle, et dans ce cas, l’algorithme ne peut pas trouver de solution, vériﬁant ainsi la non-existence

6.1. INFLUENCEDUSEUILD’ACCEPTATIONSURL’ALGORITHMEA*

135

d’une solution.

Nous constatons qu’en général, le temps de calcul augmente avec le seuil d’acceptation. Les
ﬁgures 6.1 (bruit uniforme) et 6.2 (bruit gaussien) montrent les temps de calcul en fonction du seuil
choisi. Nous utilisons une échelle logarithmique aﬁn de pouvoir visualiser un domaine très large. Les
lignes pointillées horizontales correspondent au temps de calcul sans seuil d’acceptation. Le temps
est alors le temps nécessaire pour vériﬁer la non-existence d’une solution. Les lignes en trait plein
correspondent au premier cas. Les lignes semi-pointillées indiquent les exécutions sans solution
inférieure au seuil choisi.

Nous constatons que pour des seuils très petits (inférieures à 0, 4E) par rapport à la distance
espérée les temps de calcul sont négligeables. Entre 40% et 60% de l’espérance, le temps de calcul
augmente considérablement. Nous observons également que le niveau de bruit inﬂuence le début de
la croissance du temps de calcul : un niveau plus élevé précipite la croissance. La différence entre
des exécutions avec ou sans solution est négligeable : les courbes s’enchainent sans saut. Quand le
seuil dépasse la distance, alors le temps de calcul progresse toujours avec un gradient dépendant du
bruit : plus le bruit est fort plus la hausse est importante. Nous observons que le temps de calcul
n’atteint pas celui de référence (sans seuil), même quand on met le seuil à deux fois la distance.

s
e
d
n
o
c
e
s
n
e

l
u
c
l
a
c

e
d
s
p
m
e
T

103

102

101

100

10−1

10−2

10−3

 
0

A* - bruit uniforme - taille 15

 

σ = 2%
σ = 4%
σ = 6%
σ = 8%
σ = 10%

0.2

0.4

0.6

0.8

1

Seuil x σ

1.2

1.4

1.6

1.8

2

FIGURE 6.1 – Inﬂuence du seuil d’acceptation sur le temps de calcul - bruit uniforme.

136

CHAPITRE6. GARANTIEDURÉSULTATOPTIMAL

s
e
d
n
o
c
e
s
n
e

l
u
c
l
a
c

e
d
s
p
m
e
T

103

102

101

100

10−1

10−2

10−3

 
0

A* - bruit gaussien - taille 15

 

Sans seuil initial

σ = 2%
σ = 4%
σ = 6%
σ = 8%
σ = 10%

Aucune solution

Solution trouvée

Seuil = espérance

0.2

0.4

0.6

0.8

1

Seuil x σ

1.2

1.4

1.6

1.8

2

FIGURE 6.2 – Inﬂuence du seuil d’acceptation sur le temps de calcul - bruit gaussien.

Les tableaux 6.1 et 6.2 montrent la relation entre l’algorithme A* sans seuil et avec un seuil égal
au double de l’espérance, dans un environnement parallèle et séquentiel respectivement. L’exécution
en parallèle permet d’obtenir des résultats plus rapidement mais on ne peut pas exclure des effets
de la parallélisation sur le mesure du temps. L’exécution séquentielle donne ainsi des résultats plus
précis mais nous ne pouvons pas aller aussi loin en termes de taille de graphes et de niveau de
bruit. Nous constatons que les temps de calcul avec seuil sont largement inférieurs à ceux sans seuil.
Dans tous les cas, le temps de calcul augmente de manière très importante avec le niveau de bruit
et la taille des graphes, ce qui rend l’utilisation de l’algorithme pratiquement impossible pour des
graphes de taille 20. Pour la version séquentielle nous avons utilisé un type de processeur qui est
environ deux fois plus rapide que le type utilisé dans la version parallèle, par contre on n’en utilise
qu’un seul à la fois au lieu de 16. Par conséquent la version parallèle fournit encore des résultats
pour des problèmes environ huit fois plus complexes (en termes du temps de calcul nécessaire).
Nous observons également une différence entre les deux types de bruit, le temps de calcul avec du
bruit gaussien est inférieur au temps avec du bruit uniforme. En effet, il s’agit du temps moyen sur
l’ensemble de 50 paires de graphes. Les distances entre les paires de graphes sont plus concentrées
autour de l’espérance quand on utilise du bruit gaussien que quand on utilise du bruit uniforme. Pour

6.1. INFLUENCEDUSEUILD’ACCEPTATIONSURL’ALGORITHMEA*

137

A* sans seuil

Double de l’espérance

2%

4%

6%

8%

10% 2% 4% 6%

8% 10%

3,8
10,6
36,0
230,9

4,4
5,1
17,8
89,9

7,1
34,7
69,9
253,7

5,5
12,0
107,4
148,7

15,4
58,6
82,9
954,9

6,2
44,8
125,6
702,6

Bruit uniforme

17,2
98,4
302,7
749,4

39,1
127,5
495,5
3085,0

0,1
0,1
0,2
0,2

Bruit gaussien

11,9
36,9
240,2
615,0

32,2
139,3
354,8
3457,2

0,1
0,1
0,1
0,3

0,1
0,2
0,2
0,6

0,1
0,2
0,3
0,7

0,4
1,4
3,2
8,5

0,3
1,2
3,3
8,0

3,4
9,3
31,7
102,4

2,4
6,0
31,9
70,8

19,7
54,6
191,0
789,7

12,3
39,7
120,6
925,3

Bruit

Taille

13
14
15
16

13
14
15
16

Tableau 6.1 – Temps de calcul en secondes de l’algorithme A* sans seuil - version parallèle - sur
Intel Xeon Tulsa.

ces dernières notamment, les valeurs extrêmes sont plus éloignées de l’espérance. Comme le temps
de calcul est exponentiel avec la distance, alors ces valeurs extrêmes ont une forte inﬂuence sur la
moyenne.

Dans cette section, nous avons effectué des expérimentations exhaustives avec des seuils diffé-
rents aﬁn de quantiﬁer la correspondance entre le temps de calcul et le seuil choisi. Nous constatons
d’une part que la convergence de l’algorithme A* est accélérée énormément si l’on fournit une bonne
estimation du seuil. Ceci est aussi vrai si l’on surestime la distance (jusqu’à un facteur de deux, ce
qui est toujours une bonne estimation dans des applications concrètes ayant une variance forte et a
priori inconnue des données). D’autre part, le temps de calcul pour les seuils inférieurs à la distance,
ne fournissant pas de solutions, tend à être très faible. Par contre, un problème majeur est que l’on
ne connaît pas en pratique l’espérance de la distance en avance, ce qui est nécessaire aﬁn de ﬁxer les
seuils.

138

CHAPITRE6. GARANTIEDURÉSULTATOPTIMAL

A* sans seuil

Double de l’espérance

Bruit

2% 4% 6%

8% 10% 2% 4% 6% 8% 10%

Taille

13
14
15

13
14
15

1,6
3,8
15,1

1,6
2,2
9,2

2,6
12,0
30,2

2,5
4,2
37,1

5,7
20,0
28,8

3,0
14,8
50,8

Bruit uniforme
0,03
15,6
0,05
52,4
217,0
0,07
Bruit gaussien
0,03
0,05
0,07

14,2
54,4
138,8

7,0
42,0
106,7

5,0
18,5
95,3

0,04
0,07
0,11

0,04
0,06
0,11

0,14
0,45
1,12

0,17
0,39
1,15

1,3
3,6
11,4

1,0
2,9
12,9

7,5
22,1
81,0

5,7
15,9
43,2

Tableau 6.2 – Temps de calcul en secondes de l’algorithme A* sans seuil - version sequentielle - sur
Intel Xeon X7350.

6.2 Stratégie de doublement

Dans la pratique, la déﬁnition d’une espérance de la distance et ainsi d’un seuil approprié re-
vient presque à la résolution du problème lui-même et est donc souvent impossible. Nous essayons
de contourner ce problème avec une version itérative qui commence par un seuil très petit, sous-
estimant la distance réelle, puis exécute l’algorithme à plusieurs reprises en augmentant le seuil
jusqu’à ce qu’une solution soit trouvée.

L’idée de la version itérative de l’algorithme A* vient des observations faites dans la section
précédente. D’une part, le temps de calcul pour de petits seuils est négligeable, celui d’un seuil égal
à deux fois la distance est inférieur au temps sans seuil. D’autre part, le temps de calcul augmente
plus que linéairement avec le seuil. Pour cette raison, nous espérons que la somme de plusieurs
exécutions avec des seuils assez petits montre des avantages par rapport à l’exécution sans seuil (ou
avec un seuil conservateur, c’est-à-dire très lointain aﬁn de n’exclure aucune solution possible).

Nous devons alors déﬁnir une stratégie d’augmentation du seuil. L’augmentation ne doit pas
être trop petite, sinon de nombreuses exécutions seraient nécessaires jusqu’à ce que l’on trouve la
solution, ce qui augmenterait le temps de calcul de façon signiﬁcative. Elle ne doit pas non plus être
trop grande, sinon le pas pourrait mener à un seuil conservateur que l’on pourrait peut-être déjà ﬁxer
auparavant et les exécutions précédentes n’auraient aucun effet positif.

Nous proposons de doubler la valeur du seuil à chaque étape. D’une part, cette stratégie est
exponentielle : même si la sous-estimation est importante, relativement peu d’étapes seront néces-
saires aﬁn de passer au-delà de la distance. D’autre part, la surestimation maximale que l’on peut
avoir avec cette approche est inférieure à deux fois la distance : en particulier, si l’avant-dernier seuil
atteint presque la distance, le prochain et dernier sera un peu inférieur à deux fois celle-ci. Comme
nous avons vu dans la section précédente, un tel seuil apporte déjà un gain important.

6.2. STRATÉGIEDEDOUBLEMENT

139

Les ﬁgures 6.3 et 6.4 illustrent les résultats obtenus par cette méthode par rapport à la version
initiale. La sous-estimation initiale est d’un facteur de 50 ce que nous jugeons assez important pour la
pratique. Par conséquent, sept itérations seront nécessaires en moyenne (pour la première itération
on utilise le seuil initial, puis on l’augmente 6 fois, 26 = 64 > 50). Les courbes à trait plein
correspondent à la stratégie de doublement tandis que celles à trait pointillé correspondent à l’A*
sans seuil initial.

103

102

101

100

10−1

10−2

10−3

s
e
d
n
o
c
e
s
n
e

l
u
c
l
a
c

e
d
s
p
m
e
T

A* sans seuil vs A* itératif - doublement de seuil - bruit uniforme

 

2% bruit
4% bruit
6% bruit
8% bruit
10% bruit

10−4

 
4

6

8

10

Taille de graphes

12

14

16

FIGURE 6.3 – Stratégie de doublement, bruit uniforme.

Nous constatons un gain considérable par rapport à l’approche sans seuil puisque le temps de
calcul reste inférieur à 10 secondes alors qu’il atteignait plus de 100 secondes auparavant. Une telle
stratégie est alors envisageable si l’on ne possède pas d’estimation d’un seuil d’acceptation, ce qui
est souvent le cas pour les applications réelles. Toutefois, nous constatons que la différence de temps
de calcul augmente exponentiellement avec la taille des graphes. En pratique, les applications sont
confrontées à une limitation de la taille maximale de graphes que l’on peut traiter. Nous pouvons
alors pousser la taille maximale vers des valeurs un peu plus importantes, mais sans effectivement
enlever la limitation sur la taille.

On peut s’interroger sur la valeur du paramètre qui détermine la sous-estimation initiale choisi.
Ici, nous pouvons utiliser un paramètre qui dépend de l’espérance, ce qui n’est pas possible en pra-

140

CHAPITRE6. GARANTIEDURÉSULTATOPTIMAL

103

102

101

100

10−1

10−2

10−3

s
e
d
n
o
c
e
s
n
e

l
u
c
l
a
c

e
d
s
p
m
e
T

A* sans seuil vs A* itératif - doublement de seuil - bruit gaussien

 

2% bruit
4% bruit
6% bruit
8% bruit
10% bruit

10−4

 
4

6

8

10

Taille de graphes

12

14

16

FIGURE 6.4 – Stratégie de doublement, bruit gaussien.

tique. La valeur exacte est cependant secondaire sous condition qu’elle soit inférieure à la plus petite
distance, puisqu’avec la stratégie de doublement le seuil accroît exponentiellement et atteint donc la
région de recherche très rapidement même si la distance réelle est plusieurs ordres de grandeurs plus
1
loin. Par exemple, si l’on a choisi un facteur de 1000, qui correspond à un seuil initial
1000 fois la
distance, alors on dépasse la distance et on trouve la solution après seulement 11 itérations. Pour un
facteur d’un million on calcule 21 itérations. Ce qui est beaucoup plus important dans ce contexte est
le montant du dépassement dans la dernière itération. La ﬁgure 6.5 montre le temps de calcul de la
version itérative avec des différents seuils initiaux. Les boîtes à moustaches montrent la distribution
des temps de calcul sur les paires de graphes de taille 20 avec un niveau de bruit moyenne (σ = 6).
A des ﬁns de comparaison nous avons ajouté, à droite, la distribution du temps de calcul pour la
stratégie F 2 qui utilise un seuil ﬁxe de deux fois l’espérance. Quant à l’algorithme A* sans seuil, il
n’est plus applicable pour cette taille de graphes.

Nous constatons que la courbe montre des formes répétitives : le comportement entre 8 (23)
et 16 (24) est équivalent à celui entre 16 et 32 (25). Les médianes sont les plus basses pour des
puissances de deux. En revanche la dispersion vers des valeurs plus grandes est maximale. De fait,
pour la moitié de l’échantillon, le seuil dans la dernière itération est très proche de la distance réelle

6.2. STRATÉGIEDEDOUBLEMENT

141

s
e
d
n
o
c
e
s
n
e

l
u
c
l
a
c

e
d
s
p
m
e
T

s
e
d
n
o
c
e
s
n
e

l
u
c
l
a
c

e
d
s
p
m
e
T

450

400

350

300

250

200

150

100

50

0

30

25

20

15

10

5

0

6

8

10 12 14 16 18 20 22 24 26 28 30 32 50 10001100 F2

Facteur du seuil initial pour l’algorithme itératif

6

8

10 12 14 16 18 20 22 24 26 28 30 32 50 10001100 F2

Facteur du seuil initial pour l’algorithme itératif

FIGURE 6.5 – Seuil initial pour la stratégie de doublement, taille de graphes 20, bruit uniforme
σ = 6%. En haut : domaine complet. En bas : restriction du temps de calcul sur l’intervalle [0, 30]

142

CHAPITRE6. GARANTIEDURÉSULTATOPTIMAL

tandis que pour le reste elle est égale à son double. De fait, le pire cas pour l’algorithme serait de
prendre une valeur initiale légèrement supérieure (et donc un facteur légèrement plus petit qu’une
puissance de deux).

Notre étude montre que si l’on applique la stratégie de doublement, alors le temps de calcul
dépend très fortement de la proximité du facteur à une puissance de 2 plutôt que du niveau de sous-
estimation (par exemple le temps de calcul avec un facteur 6 est quasiment identique à celui du
facteur 12, 24, et très proche du 50). En général, on ne connaît pas l’espérance exacte de la distance.
En effet, chaque paire de graphes a une autre distance. Si l’on ﬁxe un seuil pour une base entière,
alors le facteur associé à ce seuil est différent pour chaque graphe. Pour la base utilisée, on peut noter
que la variance des distances de graphes est très limitée (la distance moyenne sur toutes les paires
de graphes est : d(Gi,1, Gi,2) = 10, 85 ; pour la variance on obtient : σ = 0, 42(3, 85%) ; ainsi pour
le minimum et maximum min(d(Gi,1, Gi,2)) = 9, 69 ; max(d(Gi,1, Gi,2)) = 11, 69 ; E = 10, 91) :
les distances se concentrent autour de l’espérance. Par conséquent le facteur de sous-estimation de
la distance est similaire pour chaque paire de graphes.

Pour des bases réelles, on obtient une distribution sur un plage de facteurs, pour chaque paire
de graphes, le niveau de la sous-estimation est différent. La distribution des temps de calcul sur
une grande base est donc quasiment indépendante du seuil choisi. Par analogie avec notre expéri-
mentation, elle se compose de tous les groupes des diagrammes. Comme le temps est légèrement
supérieur au temps de calcul avec un seuil ﬁxe de deux fois la distance uniquement dans le pire cas
et dans beaucoup d’autres cas inférieur à celui-ci, alors le temps de calcul moyen de la stratégie de
doublement est souvent nettement inférieur à celui du seuil ﬁxe (nous effectuons une comparaison
après l’introduction de l’algorithme hybride dans la Fig. 6.6, p. 143).

6.3 Algorithme hybride

La combinaison des algorithmes approximatifs avec des algorithmes exhaustifs est possible
lorsque l’algorithme exhaustif proﬁte de la connaissance d’une solution candidate assez bonne.
L’idée principale consiste à exécuter les deux algorithmes l’un après l’autre en transmettant la
meilleure solution candidate de l’algorithme approximatif comme point de départ à l’algorithme
exhaustif. Celui-ci la vériﬁe ensuite. Nous avons choisi l’algorithme de Berretti et al. (2001) comme
méthode exhaustive et notre approche génétique comme approche approximative. D’abord, on ap-
plique un algorithme génétique. Nous utilisons l’algorithme générationnel avec le croisement UPBX
et les paramètres optimaux (cf. Tab. 3.1, p. 95). La distance entre les graphes obtenue par le meilleur
individu est évidemment une borne supérieure de la distance optimale. Ensuite on exécute l’algo-
rithme de Berretti en utilisant cette valeur comme limite d’acceptation 13. À la ﬁn, on garantit donc

13. Des erreurs d’arrondi venant de la représentation des nombres à virgule ﬂottante (cf. Ann. E, p. 257), pouvant être
considérées comme bruit numérique, imposent l’introduction d’une tolérance numérique. Nous utilisons 10−15 en double

6.3. ALGORITHMEHYBRIDE

143

Comparaison des stratégies - 15 sommets - Bruit gauss. 10

s
e
d
n
o
c
e
s
n
e

l
u
c
l
a
c

e
d
s
p
m
e
T

300

250

200

150

100

50

0

H5

H10

H15

F2

F3

Stratégies : cascade, seuil ﬁxe, doublement itératif

F5

D10

D20

D50

FIGURE 6.6 – Comparaison des différents stratégies, taille 15, bruit gaussien, niveau 10.

le résultat optimal. L’avantage en comparaison avec la méthode de Berretti simple, qui est basée sur
la stratégie de recherche d’arbre A* d’Ullmann (1976), est que plusieurs sous-arbres peuvent être
exclus plus tôt. Ainsi, le temps de calcul est fortement réduit, ce qui permet le traitement de graphes
plus grands.

La ﬁgure 6.6 montre le temps de calcul des différentes méthodes sur des graphes de taille 15
soumis à un bruit gaussien avec un écart-type σ = 0, 1. L’approche hybride qui combine l’algorithme
génétique et la méthode A* est dénommée H. Le numéro correspond au nombre de générations sans
changement que l’on utilise comme critère d’arrêt. La médiane du temps de calcul se situe dans les
trois cas autour de 0,95 secondes : on obtient précisément 0,9523 pour H5, 0,9526 pour H10, et
0,9413 pour H15. Nous constatons néanmoins que le nombre des valeurs déviantes est visiblement
réduit quand on choisit un critère d’arrêt plus relâché. Il est donc préférable d’attendre quelques
générations de plus aﬁn de permettre à l’algorithme génétique de converger.

Les trois colonnes notées F au centre correspondent à l’approche à seuil ﬁxe. Nous avons choisi
des seuils de deux, trois et cinq fois l’espérance mathématique. Ces deux dernières distributions
sont quasi-identiques tandis que le temps de calcul est signiﬁcativement plus petit quand on utilise
le double de l’espérance.

Quant à la stratégie de doublement itérative (lettre D), nous commençons avec des seuils entre
10 et 50 fois plus petits que l’espérance. Nous constatons des temps de calcul très similaires avec
les paramètres 10 et 20. En revanche le temps de calcul devient beaucoup plus petit quand on utilise
le paramètre 50. Ceci peut paraître étrange au premier regard. En effet, quand on sous-estime le

précision.

144

CHAPITRE6. GARANTIEDURÉSULTATOPTIMAL

Bruit uniforme

Bruit gaussien

Stratégie

σ = 0, 02 σ = 0, 06 σ = 0, 1 σ = 0, 02 σ = 0, 06 σ = 0, 1

H15
F2
F3
F5
D10
D20
D50

0,080
0,071
0,080
0,078
0,077
0,078
0,076

0,13
0,72
3,0
5,3
0,27
0,27
0,16

2,0
67
130
130
18
18
3,6

0,078
0,072
0,075
0,074
0,10
0,10
0,076

0,10
1,0
2,9
5,2
0,35
0,35
0,16

0,94
22
43
43
7,8
7,8
2,4

Tableau 6.3 – Médianes du temps de calcul en secondes, taille de graphes 15.

seuil d’un facteur de 10, alors on effectue en moyenne 6 itérations, pour les facteurs 20 et 50, on
effectue respectivement 7 et 8 itérations. Le point commun entre les facteurs 10 et 20 est le coût
de surestimation pendant la dernière itération. Soit a le facteur de sous-estimation et t le numéro
de l’itération, alors le seuil se détermine par 2t−1 × E
a . Pour la première itération dont le seuil est
supérieur à l’espérance, il est 60% plus grand pour les facteurs 10 et 20, et seulement 28% pour le
facteur 50. L’ajout d’itérations est beaucoup moins important.

Globalement, quand on compare les trois approches, on constate que l’algorithme hybride est

toujours le plus rapide. Le deuxième est la stratégie de doublement suivie par A* avec seuil ﬁxe.

La ﬁgure 6.7 montre le comportement des stratégies sur des graphes avec bruit uniforme. On
constate que les temps de calcul sont plus élevés qu’en utilisant un bruit gaussien. Les médianes
sont 1,75 au lieu de 0,94 secondes pour H15, 67,42 au lieu de 22,13 secondes pour F2, et 2,44 au
lieu de 3,63 secondes pour D50. L’avantage de l’algorithme hybride s’accentue donc encore plus.

On peut noter également que les différences augmentent particulièrement avec le niveau de bruit
(cf. Tab. 6.3, p. 144). Nous constatons que pour des niveaux de bruit très faibles, la médiane du
temps de calcul est inférieure à un dixième d’une seconde.

Si l’on considère des graphes plus grands, par exemple de taille 25, on constate une médiane
de 1,74 secondes pour l’algorithme hybride (H15) contre 1,83 secondes pour l’approche ﬁxe (F2).
La stratégie de doublement (D50) nécessite 1,73 secondes. Malgré la similitude des médianes, les
variances sont plus élevées pour l’approche à seuil ﬁxe. Par conséquent, l’utilisation des autres
stratégies permet de réduire le nombre de valeurs déviantes.

6.4. BILAN

145

Comparaison des stratégies - 15 sommets - Bruit unif. 10

s
e
d
n
o
c
e
s
n
e

l
u
c
l
a
c

e
d
s
p
m
e
T

300

250

200

150

100

50

0

H5

H10

H15

F2

F3

Stratégies : cascade, seuil ﬁxe, doublement itératif

F5

D10

D20

D50

FIGURE 6.7 – Comparaison des différents stratégies, taille 15, bruit uniforme, niveau 10.

6.4 Bilan

Si l’on souhaite appliquer des algorithmes de recherche d’arbre (basés notamment sur A*) sur
un problème inexact en pratique, il faut absolument limiter l’espace de recherche (d’une manière ou
d’une autre). En général, ceci est fait en se limitant à des graphes très petits. Des algorithmes du type
A* permettent aussi une limitation basée sur une distance maximale acceptable. Ceci mène à deux
problèmes principaux : d’une part, la détermination du seuil qui n’est pas triviale. D’autre part, si
la distance réelle est supérieure au seuil, on ne trouve aucun résultat. Nous avons comparé ici trois
stratégies différentes. Premièrement, nous avons utilisé un seuil ﬁxe établi sur la base de l’espérance
mathématique de la distance. La deuxième stratégie se base sur un seuil adaptatif, initialisé à une
valeur faible puis doublé jusqu’à ce que l’algorithme trouve une solution. On n’aura donc jamais un
seuil dépassant deux fois la distance. Bien que cette approche ne soit pas toujours plus performante
que celle du seuil ﬁxe, notamment si le seuil utilisé par cette dernière est très strict, l’avantage prin-
cipal réside dans la non-nécessité de connaître l’espérance mathématique de la distance. Ceci évite
à l’utilisateur une étude détaillée des variances dans son domaine d’application. Aussi, une solu-
tion est toujours identiﬁée même si la distance est supérieure à l’estimation préalable. La troisième
stratégie consiste à utiliser l’algorithme génétique aﬁn de déterminer le seuil. Cette approche est
encore plus générale, dans le sens que l’on peut l’appliquer sur de très nombreux problèmes réels,
et on obtient un seuil par instance et non général. Elle est particulièrement avantageuse dans des
environnements très bruités.

En ce qui concerne les algorithmes génétiques, bien que leur précision soit très élevée, il n’est

146

CHAPITRE6. GARANTIEDURÉSULTATOPTIMAL

pas possible de garantir l’obtention de la solution optimale. Pourtant, une vériﬁcation de la solution
par un algorithme A* est possible. Nous pouvons observer qu’en ajoutant cette étape, une garantie
peut être formulée et le temps de calcul reste très bas par rapport à l’application de la recherche
d’arbre simple. En effet, en fournissant une solution sous-optimale mais pas trop éloignée de l’op-
timum comme seuil à l’algorithme de Berretti-Ullmann, on peut accélérer celui-ci sensiblement.
De cette façon, nous constatons qu’en moyenne l’exécution de l’algorithme génétique puis de l’al-
gorithme de Berretti prend beaucoup moins de temps de calcul que l’exécution de l’algorithme de
Berretti simple. Toutefois, le temps de calcul dans le pire cas reste exponentiel, notamment quand
l’algorithme génétique ne trouve pas de solution sous-optimale assez bonne pour accélérer la re-
cherche.

Chapitre 7

Applications

Dans ce chapitre, nous illustrons les résultats de notre algorithme sur trois applications concrètes.
Nous introduisons d’abord le contexte général des applications aux images. Ensuite, nous présen-
tons les résultats pour deux applications dans ce domaine. La première s’inscrit dans le cadre de
l’identiﬁcation de personnes. Nous apparions des graphes extraits des photos en deux dimensions
avec ceux obtenus à partir des modèles en trois dimensions de leurs visages. La deuxième applica-
tion vise l’identiﬁcation d’objets basée sur des graphes a priori sans attributs. Enﬁn, nous illustrons
le fait que notre algorithme fonctionne également pour le problème d’isomorphisme exact, dans le
cas de molécules.

7.1 Images

Le recherche d’images est souvent associée à des sémantiques de haut niveau. Les méthodes
actuelles cherchant à réduire le fossé sémantique incluent l’utilisation d’ontologies, l’apprentissage
automatique des relations entre les caractéristiques de bas niveau avec des concepts sémantiques, les
boucles de pertinence, des templates sémantiques et la fusion de données, notamment le texte associé
à une image (un résumé de toutes ces méthodes est donné dans Liu et al., 2007). Nous proposons
d’exploiter une information de relations représentées par des graphes.

Nous introduisons d’abord des pistes générales de l’utilisation de graphes dans le domaine

d’images avant d’illustrer nos applications à deux problèmes précis.

7.1.1 Contexte général

Suivant l’idée qu’il y a plusieurs « objets » sur une image qui sont en relation les uns avec
les autres, on essaie de découper les images et de construire un graphe à partir de ses parties. Il
existe deux méthodes pour identiﬁer les entités (sommets) dans l’image. D’une part, la segmenta-
tion identiﬁe des régions (Shi & Malik, 2000; Boykov et al., 2001; Boykov & Kolmogorov, 2004;

147

148

CHAPITRE7. APPLICATIONS

FIGURE 7.1 – Isomorphisme de graphe à partir des régions, images de Omhover & Detyniecki
(2006).

Felzenszwalb & Huttenlocher, 2004). D’autre part, des approches d’extraction des points d’intérêts
identiﬁent un ensemble de points pertinents (Mikolajczyk & Schmid, 2002, 2005). Dans le cas de la
segmentation, nous sommes confrontés à trois déﬁs : la déﬁnition des régions et donc la segmenta-
tion automatique de l’image, le choix des caractéristiques à extraire pour chaque région et ﬁnalement
la question de la description des relations entre régions. La ﬁgure 7.1 montre schématiquement le
problème de l’isomorphisme de sous-graphes entre deux images. Pour nos expérimentations, nous
considérons la description de l’image comme graphe comme étant donnée, aﬁn que nous puissions
nous concentrer pleinement sur le problème d’optimisation de l’appariement.

En ce qui concerne la modélisation par points d’intérêts, la déﬁnition des relations devient plus
délicate. On obtient en premier lieu, un ensemble de points, non connectés, et sans notion explicite
de voisinage. Il faut donc connecter les points aﬁn d’obtenir un graphe. Une méthode courante est
l’utilisation d’un algorithme de triangulation, par exemple une triangulation de Delaunay (Luo &
Hancock, 2001). La construction d’un graphe de voisinage est également possible. La question qui
se pose par la suite est : est-ce que l’introduction des relations apporte plus d’informations ? A notre
avis ceci n’est pas le cas. Par ailleurs, si l’on considère une triangulation de Delaunay, la structure
du graphe peut être sensiblement modiﬁée par un petit déplacement d’un seul point. Ceci limite la
robustesse de l’approche qui est très importante pour l’application aux images.

Dans le domaine de la reconnaissance de formes, on parle aussi d’appariement des graphes pour
un problème spéciﬁque : on pourrait le dénommer l’appariement de dessins de traits, une application
principale étant la reconnaissance optique de caractères (Auwatanamongkol, 2007). Le terme graphe

7.1. IMAGES

149

est utilisé dans son sens géométrique. Il est toujours planaire (l’intersection de deux traits est iden-
tiﬁée comme un sommet durant l’étape d’extraction de graphe). De plus, les angles entre les traits
sont très souvent utilisés (pour des graphes généraux la notion d’angles n’existe pas). Le modèle est
donc très sensible aux déformations et beaucoup plus spéciﬁque que le problème d’isomorphisme
de graphes.

Un autre problème similaire est le problème de l’appariement d’ensembles de points (point pat-
tern matching, Gold et al., 1998; Caetano et al., 2006). Il consiste à trouver une correspondance
entre deux ensembles de points dans un espace euclidien à deux ou trois dimensions. Par rapport au
problème d’appariement de graphes il y a deux différences clefs. D’une part, il n’y a pas d’arêtes.
D’autre part, la correspondance recherchée est une isométrie (approximative dans le cas inexact).
L’application la plus courante est l’appariement des points d’intérêts, comme par exemple des SIFT
(Lowe, 2004). En pratique le problème est souvent inexact et requiert des méthodes d’optimisation.
Les méthodes utilisées incluent des approches basées sur les plus proches voisins (Beis & Lowe,
1997, 1999), des approches qui estiment la position et l’orientation des objets dans l’image et en
dérivent les correspondances par exemple en utilisant softassign (Gold et al., 1998), ou encore des
méthodes spectrales (Carcassoni & Hancock, 2003). Demirci et al. (2006) abordent le problème
des appariements many-to-many avec l’algorithme EMD (Earth Mover’s Distance). Caetano et al.
(2006a ; 2006b) proposent un changement de point de vue intéressant : au lieu d’utiliser un modèle
exact et des algorithmes inexacts, ils suggèrent l’utilisation d’un modèle inexact et d’un algorithme
exact qui a une complexité polynomiale grâce aux contraintes imposées.

En ce qui concerne les autres applications existantes, on peut citer la combinaison des graphes
avec la logique ﬂoue qui peut intervenir non seulement dans la description d’images par le graphe
mais aussi dans l’algorithme d’appariement (Krishnapuram et al., 2004). Les modèles de graphes
sont également utilisés pour la régularisation des images de couleur, notamment aﬁn de réduire le
bruit et d’améliorer la qualité (Lezoray et al., 2007).

On peut également passer au traitement des vidéos. Chevalier et al. (2007) proposent l’utilisation
des graphes d’adjacence de régions qui changent avec le temps pour la reconnaissance d’objets dans
des vidéos de basse résolution. Enﬁn, on peut modéliser l’aspect temporel de la vidéo avec un graphe,
par exemple pour le résumer (Ngo et al., 2005).

7.1.2 Images 3D

Notre première application dans le domaine des images se place dans le cadre de l’identiﬁcation
de personnes : nous utilisons une base ﬁxée de visages à reconnaître. Nous disposons concrètement
des modèles 3D des visages et tentons d’identiﬁer la bonne personne avec une photo en deux dimen-
sions uniquement. Il s’agit d’un problème d’isomorphisme de sous-graphes parce que des occlusions
sont omniprésentes. De plus, le niveau de bruit est très élevé car les photos et les modèles sont issus

150

CHAPITRE7. APPLICATIONS

de deux processus différents. Ceci rend l’application des méthodes exactes très délicate et favorise
plutôt notre approche génétique.

La base contient les modèles 3D de 61 visages ainsi que plusieurs photographies de chaque per-
sonne. Berretti et al. (2008) nous fournissent avec la description de chaque image sous forme d’un
graphe. Les attributs des nœuds et des arêtes sont des vecteurs de taille 27. De fait, ceci correspond à
un coefﬁcient par relation spatiale pour chaque direction. Les graphes ont été réduits aﬁn de pouvoir
être traités avec les méthodes existantes et contiennent en général 12 sommets. Ils sont donc assez
petits, ce qui défavorise l’application des algorithmes génétiques à cause de leur coût constant d’ini-
tialisation. Par ailleurs, nous sont données les mesures de distances pour les sommets et les arêtes,
correspondantes à cette description.

Pour chaque modèle 3D, deux vues frontales sont disponibles, la première avec une expres-
sion neutre et la deuxième avec un sourire. Les correspondances sont connues. Avant de se poser
la question de la qualité d’un algorithme en particulier, il faut d’abord étudier la pertinence de la
description. En effet, si l’on fait un classement des plus proches modèles par rapport à une image
donnée, il est fortement souhaitable que le meilleur modèle décrive la même personne. Nous consta-
tons que si l’on utilise la totalité des sommets de chaque graphe, alors ceci n’est pas toujours le cas.
Par conséquent, on peut se tromper de la personne, même si l’on a le résultat optimal numérique.
Toutefois, si l’on considère uniquement les sous-graphes composés des premiers neuf nœuds dans
l’ordre donné, alors la personne peut être identiﬁée correctement dans tous les cas (pour cette base)
si son expression est neutre. Si la personne sourit, cela devient moins pertinent.

Nous effectuons deux tests séparés. La différenciation se fait selon l’existence d’une correspon-
dance réelle entre les images associées aux graphes. Les cas sont nommés par conséquent positifs,
si les graphes correspondent à la même personne et négatifs, sinon.

Notre référence est l’algorithme A* (Berretti et al., 2001) qui garantit l’optimalité de résultats.
Il est très rapide dans le cas positif, où les distances sont très petites. Dans le cas négatif, il converge
beaucoup plus lentement du fait de la distance qui conduit à des seuils internes plus élevés. Dans
le cas positif, 90% des exécutions se terminent dans un délai très court. Cependant, le temps de
convergence des 10% restants est très variable. Dans le cas négatif, la variance du temps est beaucoup
plus élevée.

La précision des approches génétiques est de 100%, avec 30 tests lancés par paire (2D/3D) :
parmi environ 2000 exécutions indépendantes faites par opérateur, nous n’avons pas constaté une
seule erreur. Les ﬁgures suivantes (7.2 à 7.6, p. 151 à 153) illustrent les résultats avec une sélection
des différents opérateurs. Nous constatons que les temps de calcul sont très faibles, de l’ordre de 50
millisecondes 14.

14. Avec cet ordre de grandeur, il est important de vériﬁer la précision des mesures de temps utilisées. La précision
de l’horloge dépend du système d’exploitation, Windows XP, par exemple, a une résolution de 15 millisecondes. Nous
avons effectué les tests sous des noyaux Linux fournissant une précision d’une milliseconde. Aﬁn d’éviter toute inﬂuence

7.1. IMAGES

1

0.8

0.6

0.4

0.2

s
é
n
i
m
r
e
t

s
n
u
r

e
d
x
u
a
t

-
n
o
i
s
i
c
é
r
P

0

 
0

0.05

0.1

UOX - neutre

151

 

A* - positif
AG - positif
A* - négatif
AG - négatif

0.4

0.45

0.5

0.15

0.2

0.25

0.3

0.35

Temps de calcul en secondes

FIGURE 7.2 – Images 3D - expression neutre - UOX.

1

0.8

0.6

0.4

0.2

s
é
n
i
m
r
e
t

s
n
u
r

e
d
x
u
a
t

-
n
o
i
s
i
c
é
r
P

0

 
0

0.05

0.1

UPBX - neutre

 

A* - positif
AG - positif
A* - négatif
AG - négatif

0.4

0.45

0.5

0.15

0.2

0.25

0.3

0.35

Temps de calcul en secondes

FIGURE 7.3 – Images 3D - expression neutre - UPBX.

152

CHAPITRE7. APPLICATIONS

1

0.8

0.6

0.4

0.2

s
é
n
i
m
r
e
t

s
n
u
r

e
d
x
u
a
t

-
n
o
i
s
i
c
é
r
P

0

 
0

0.05

0.1

CX - neutre

 

A* - positif
AG - positif
A* - négatif
AG - négatif

0.4

0.45

0.5

0.15

0.2

0.25

0.3

0.35

Temps de calcul en secondes

FIGURE 7.4 – Images 3D - expression neutre - CX.

1

0.8

0.6

0.4

0.2

s
é
n
i
m
r
e
t

s
n
u
r

e
d
x
u
a
t

-
n
o
i
s
i
c
é
r
P

0

 
0

0.05

0.1

DPX - neutre

 

A* - positif
AG - positif
A* - négatif
AG - négatif

0.4

0.45

0.5

0.15

0.2

0.25

0.3

0.35

Temps de calcul en secondes

FIGURE 7.5 – Images 3D - expression neutre - DPX.

7.1. IMAGES

1

0.8

0.6

0.4

0.2

s
é
n
i
m
r
e
t

s
n
u
r

e
d
x
u
a
t

-
n
o
i
s
i
c
é
r
P

0

 
0

0.05

0.1

DPX - souriant

153

 

A* - positif
AG - positif
A* - négatif
AG - négatif

0.4

0.45

0.5

0.15

0.2

Temps de calcul en secondes

0.25

0.3

0.35

FIGURE 7.6 – Images 3D - expression souriante - DPX.

7.1.3 COILDel

La base Columbia Object Image Library (COIL) est une base d’images destinée à la reconnais-
sance de formes (Nene et al., 1996). Elle contient 100 objets différents. Chaque objet est photogra-
phié sous tous les angles entre 0◦et 355◦par pas de 5◦. Il y a donc un total de 72 images par objet.
Les objets se trouvent isolés sur les photos sur fond noir. Il n’y a donc pas d’effets d’occlusion et
pas de nécessité d’identiﬁer quel est (et où se trouve) l’objet dans l’image.

Riesen & Bunke (2008) utilisent un détecteur de contours aﬁn d’extraire un ensemble de points
de chaque image. Une triangulation de Delaunay connecte par la suite les points aﬁn de former
un graphe. Ce dernier est non orienté et ne contient pas d’étiquettes. Riesen & Bunke utilisent la
distance d’édition de graphes pour déterminer la distance entre deux graphes. Avec un k plus proches
voisins (k-NN) basé sur cette mesure de distance, les auteurs obtiennent une précision de 93,3%. Au
premier regard ceci peut paraître très bon, mais il faut savoir que l’ensemble d’apprentissage contient
tous les objets à intervalles de rotation de 15◦. Si l’on souhaite classer un objet de l’ensemble de test,
on sait qu’il y a exactement le même objet à 5◦et à 10◦de rotation dans l’ensemble d’apprentissage.
Ces deux exemples sont tellement proches de la requête que la probabilité qu’ils soient les deux
voisins les plus proches est assez élevée, quels que soient les descripteurs visuels et la distance. Si
l’on choisit par exemple k = 3, alors même s’il y a un autre objet plus proche que ces deux exemples,
la classiﬁcation sera correcte. En pratique, par contre, il serait intéressant de voir si l’on peut classer
un objet sous la condition qu’il n’existe qu’une seule fois dans la base (avec une rotation arbitraire) ;

gênant la précision, nous avons effectué les tests dans un environnement séquentiel sur un seul processeur.

154

CHAPITRE7. APPLICATIONS

COILDel - SSGA - DPX - 1% LS - Graphes non-isomorphes

0.7

0.6

0.5

n
o
i
s
i
c
é
r
P

0.4

0.3

0.2

0.1

0

 
0

Sans attributs
Attributs de sommets
Attributs d’arcs
Les deux
Précision par défaut

 

5

10

15

Temps de calcul en secondes

20

25

30

35

40

FIGURE 7.7 – Histogramme DPX - COILDel.

d’autant plus qu’aﬁn d’exécuter le k-NN, il faut a priori comparer chaque nouvel objet à chaque
objet de la base d’apprentissage. Il est en général possible d’éliminer une partie de ses comparaisons
en structurant la base. Le scénario utilisé nécessite 2400 (moins ce que l’on peut éliminer) calculs
de la distance d’édition de graphe qui est NP-difﬁcile. En revanche, si chaque objet n’existait qu’une
fois, un maximum de 100 comparaisons serait sufﬁsant.

La structure des graphes construits par une méthode de triangulation est fortement perturbée par
des changements de points. Un très léger déplacement d’un point peut mener au changement de
pratiquement tous les arcs autour de lui. Ainsi l’introduction ou la suppression d’un point peut avoir
des effets même sur des points lointains. On peut se poser la question de savoir si un graphe issu
d’une triangulation apporte un gain d’information par rapport à l’ensemble de points sur lequel il
se base. Il est évident qu’une approche structurelle est beaucoup plus adaptée à ce problème qu’une
approche dont les déformations structurelles possibles sont limitées.

Notre objectif est d’étudier le problème d’isomorphisme sur des données réelles. Nous appli-
quons donc notre algorithme sur ce problème, même si sa limitation sur certains changements struc-
turels ne semble pas avantageuse.

Nous utilisons quatre mesures de distance différentes. Dans la première, chaque différence d’arc
(présent-non présent) augmente la distance de un. Dans la deuxième, nous attribuons à chaque arc

7.1. IMAGES

1

0.8

0.6

0.4

0.2

n
o
i
s
i
c
é
r
P

0

 
0

1

2

3

4

Temps de calcul en secondes

5

6

Résultats COILDel - SSGA - DPX - 1% LS - graphes isomorphes

155

 

Sans attributs
Attributs de sommets
Attributs d’arcs
Les deux

7

8

9

10

FIGURE 7.8 – Histogramme DPX - COILDel - cas isomorphes.

la distance euclidienne entre les sommets qu’il connecte. La troisième distance prend en compte les
coordonnées des sommets et en calcule la distance euclidienne. La dernière est une somme pondérée
de la deuxième et de la troisième.

Nous nous intéressons surtout à la question de savoir si l’on retrouve la bonne distance entre
deux graphes de la base donnés et non à la précision d’une éventuelle classiﬁcation qui serait l’étape
suivante. D’abord, il faut noter que, dans la base, il n’y a pas de graphes exactement isomorphes,
sauf quand on compare un graphe à lui-même, et que nous ne possédons pas de valeurs de référence :
les distances sont a priori inconnues. L’application d’une méthode exhaustive aﬁn de les déterminer
prendrait trop de temps, étant donné la taille de graphes (en moyenne 21 sommets avec un maximum
de 77). Nous prenons donc la meilleure valeur trouvée pour une paire de graphes parmi les 30
exécutions de l’algorithme comme référence. Ceci ne donne pas de garantie sur l’optimalité mais il
est probable que, si cette valeur est trouvée souvent, elle est la bonne. La précision par défaut est
alors 1/30, ce qui ne veut pas dire que les solutions trouvées sont réellement exactes. Par contre, si
l’on n’arrive même pas à retrouver le même optimum lors de plusieurs exécutions, on peut conclure
que l’algorithme fonctionne mal sur ce problème. Pour limiter le temps de calcul, nous effectuons les
tests préliminaires uniquement sur les images qui se trouvent dans l’ensemble d’apprentissage, déﬁni
par Riesen & Bunke, pour un seul objet. Au total nous effectuons 9000 exécutions de l’algorithme.

La ﬁgure 7.7 montre l’histogramme de la précision globale en fonction du temps, plus précisé-
ment en fonction du nombre d’appels à la fonction d’évaluation. Nous constatons que la précision
reste très faible malgré un temps considérable donné à l’algorithme (100 millions d’évaluations de la
fonction d’évaluation, ce qui prend autour de 20h sur 8 processeurs par mesure, donc plus de 500h

156

CHAPITRE7. APPLICATIONS

de temps de calcul pour un processeur). Le résultat obtenu est similaire à celui d’une recherche
uniforme pour des graphes sans attributs. La meilleure performance (sur un niveau de précision tou-
jours très faible) est obtenue avec la distance euclidienne entre les coordonnées des sommets. Quant
aux mesures utilisant des attributs sur les arcs, elles se trouvent entre les deux. En général, on peut
conclure que les différences de structure évitent la convergence, surtout en combinaison avec une
fonction d’évaluation ayant de nombreux optima locaux dispersés dans l’espace (les triangulations
sont très sensibles au bruit). Aussi, au lieu d’exécuter notre algorithme génétique sur un graphe créé
de cette façon, il apparaît préférable d’effectuer directement l’alignement entre les points.

En ce qui concerne les cas dont la distance est zéro, c’est-à-dire les graphes identiques, alors la
précision atteint 100%, si l’on prend en compte les attributs de sommets. Le temps de convergence
a aussi un ordre de grandeur beaucoup plus petit que dans le cas précédent. Si l’on n’utilise que les
attributs d’arcs, alors il reste 20% des cas où la solution n’est pas trouvée. La distance sans attributs,
l’algorithme ne converge toujours pas.

7.2 Molécules

Nous mettons en œuvre notre algorithme sur une base de molécules. Il s’agit d’une base publique
du National Cancer Institute contenant des composés chimiques qui sont susceptibles d’avoir des
effets positifs pour la lutte contre le SIDA. Les données sont sous la forme d’un ﬁchier au format
MDL SDFILE. Cette base contient 42687 molécules avec des tailles entre 2 et 438 atomes. La
taille moyenne est de 45,7 atomes. Le problème ici est celui d’un isomorphisme de graphes exact,
c’est-à-dire que, dès qu’un élément ou une liaison diffère, cela entraîne un changement brusque des
propriétés de la molécule.

Avant de passer à la résolution du problème de l’isomorphisme de graphes, nous pouvons ex-
ploiter certaines caractéristiques des molécules. Les attributs des sommets ainsi que ceux des arêtes
appartiennent à un domaine discret et ﬁni : les sommets sont des éléments chimiques ; leurs liaisons
sont décrites par des nombres entiers généralement petits.

7.2.1 Filtrage par signature

Nous effectuons un ﬁltrage préalable basé sur la comparaison des signatures des molécules.
La signature atomique comporte uniquement le nombre de sommets de chaque élément chimique
(l’ordre étant donné par le tableau périodique des éléments). Nous obtenons par exemple 2H1O et
C1O2 pour l’eau et le dioxyde de carbone respectivement. Si deux molécules n’ont pas la même
signature, alors elles ne peuvent pas être isomorphes. Dans le cas contraire, il faut toutefois procéder
à l’appariement structurel. Nous avons obtenu 27381 signatures différentes. Le tableau 7.1 montre
en détail la distribution des molécules selon leurs signatures. On construit des groupes contenant les

7.2. MOLÉCULES

157

molécules de même signature, le tableau indique le nombre de groupes de chaque taille : ainsi avec
la signature atomique, on obtient 20370 groupes à une molécule soit 20370 molécules qui peuvent
être identiﬁées directement et pour lesquelles la signature est un critère sufﬁsant. Par contre, on
observe aussi 3846 groupes à deux molécules, c’est-à-dire 3846 signatures pour lesquelles deux
molécules possibles sont à tester, 1382 groupes à trois molécules, pour lesquelles trois possibilités
sont à tester, etc. Le nombre maximal de molécules de même signature est 20. Par conséquent, il
n’est jamais nécessaire d’effectuer plus de 20 comparaisons structurelles pour identiﬁer une mo-
lécule (ou décider qu’elle n’est pas encore présente dans la base). En fait, sans l’extraction d’une
signature, quand les molécules sont ﬁltrées par leur taille uniquement, le nombre d’appariements
nécessaires dépasse mille selon la taille (cf. la deuxième colonne du Tab. 7.2, p. 161). Aussi les
graphes les plus grands nécessitent moins de comparaisons : par exemple les groupes avec le plus
grand graphe sont (taille de graphes, nombre de graphes ayant une signature atomique identique) :
(245,3),(228,2),(221,2),(217,3),(216,2),(205,2). La totalité des groupes contenant au moins dix mo-
lécules comporte des molécules entre 22 et 69 atomes.

Il est possible de rendre la signature plus pertinente en utilisant plus d’informations sur les
sommets que l’élément chimique. L’idée générale est de substituer aux étiquettes de sommets des
étiquettes plus discriminantes. Aussi, dans l’algorithme Nauty 15 de McKay (1981) un ordonnance-
ment des sommets basés sur ces nouvelles étiquettes est utilisé. Les principales informations que
l’on peut utiliser pour enrichir les étiquettes sont les étiquettes des arêtes incidentes ainsi que les
étiquettes des sommets voisins. Zampelli et al. (2007) proposent de renforcer l’étiquetage en regar-
dant les étiquettes des voisins dans un rayon plus grand : au lieu de prendre en compte uniquement
les voisins directs, ils prennent en compte tous les sommets ayant au maximum une distance α du
sommet donné.

Nous proposons deux étiquetages de ce type, qui mènent à la signature cumulée et la signa-
ture binaire respectivement. Les étiquettes se calculent à partir de l’élément chimique et du degré,
c’est-à-dire le nombre de liaisons partant de cet atome. Si l’on considère uniquement les liaisons
covalentes qui sont les plus courantes dans des molécules (une exception étant par exemple les
cycles aromatiques), il existe des liaisons doubles et des liaisons triples. Le nombre total de liai-
sons dépend le plus souvent des propriétés chimiques de l’atome uniquement et n’apporte donc pas
d’information complémentaire. Un étiquetage par le degré sous forme du nombre total de liaisons
(la signature cumulée) ne peut donc pas (sauf exceptions) beaucoup améliorer le ﬁltrage. Si l’on
suppose par contre un étiquetage qui compte les atomes auxquels l’atome est connecté directement
(que l’on appelle signature binaire car les liaisons sont binarisées), ceci peut différer pour un même
atome, car il change de valeur selon la structure. L’acide éthanoïque (cf. Fig. 7.9, p. 158) contient
deux carbones et deux oxygènes. Le nombre total de liaisons est quatre pour les carbones et deux
pour les oxygènes (chiffres en index sur la partie gauche). Par conséquent, nous ne pouvons pas

15. Disponible à l’adresse : http ://cs.anu.edu.au/ bdm/nauty/.

158

CHAPITRE7. APPLICATIONS

distinguer entre les deux atomes de carbone avec ce degré. Par contre, si l’on utilise le nombre des
voisins, c’est-à-dire si l’on compte uniquement s’il y a une liaison d’un type quelconque ou non,
alors les atomes deviennent discernables. Les valeurs (4 et 3 respectivement) sont données en index
du symbole chimique sur la partie droite.

FIGURE 7.9 – Acide éthanoïque (acétique).

Filtrage par signature

Nous examinons par la suite jusqu’à quel point le ﬁltrage de la base est possible avec les diffé-
rentes signatures. Le tableau 7.1 illustre le nombre de groupes répartis selon leur taille. La première
ligne contient notamment le nombre de molécules qui peuvent être identiﬁées directement par la si-
gnature. La deuxième ligne contient le nombre de paires de molécules qui ne sont pas distinguables,
etc. Nous constatons que même avec la signature qui mène au ﬁltrage le plus faible, aucune molé-
cule ne nécessite plus de 20 comparaisons. La signature cumulée améliore légèrement le ﬁltrage.
Quant à la signature binaire, le nombre de molécules détectées directement augmente de 50% et la
taille maximale d’un groupe se réduit à 14. La dernière ligne du tableau donne le nombre moyen de
comparaisons à faire par molécule (et non par groupe). Il faut noter que même si la signature de la
molécule est unique, nous considérons que l’appariement est quand même nécessaire, car il pourrait
s’agir d’une molécule inconnue dans la base.

7.2.2 Approche par couleur

Les mêmes idées appliquées aﬁn de ﬁltrer la base et réduire le nombre d’appariements à effectuer
peuvent être utilisées pour réduire l’espace de recherche du calcul de l’appariement en soi. Ceci
permet alors de réduire le temps de calcul de l’appariement lui-même. En effet, dans la procédure de
recherche nous pouvons utiliser les étiquettes aﬁn de formuler des contraintes dures (parce que nous
nous situons dans un cas exact) : intuitivement, un atome du type A ne peut être apparié qu’avec un
atome du même type.

Le tableau 7.2 montre la réduction de l’espace de recherche que l’on obtient sur la base. Pour
donner un exemple : la taille de l’espace de recherche pour l’appariement de la molécule de l’acide
éthanoïque dont la taille est 8, sans utilisation d’étiquettes, est 8! = 40320. Il existe donc 40320
appariements possibles. Quand on introduit la contrainte d’apparier uniquement les mêmes atomes,
le nombre d’appariements diminue à 4!2!2! = 96.

7.2. MOLÉCULES

159

On peut encore faciliter la recherche en utilisant des caractéristiques plus discriminantes comme
contraintes, comme dans les signatures binaires. L’étiquetage par degré binaire arrive à discriminer
les deux C ainsi que les deux O, il n’y a que 4!1!1!1!1! = 24 appariements possibles. Par contre,
l’espace de recherche reste le même pour l’étiquetage par degré cumulé parce que l’on ne distingue
pas les atomes O et C.

Nous calculons la taille de l’espace de recherche pour chaque atome aﬁn de déduire un résumé
pratique sur la base. Pour chaque étiquetage la taille minimale, moyenne et maximale de l’espace
de recherche sont indiquées dans le tableau 7.2. Selon la réduction apportée, les trois étiquetages
sont dans l’ordre suivant : signature atomique, signature cumulée, signature binaire. Néanmoins, le
nombre d’appariements minimal de l’étiquetage cumulé est parfois inférieur à celui de l’étiquetage
binaire, notamment pour les tailles 30, 50, 60 et 80. Quant à la moyenne et au maximum, ceci n’est
pas le cas. Il existe alors des molécules qui peuvent être mieux différenciées avec le degré cumulé
qu’avec le degré binaire. Il s’agit probablement de molécules ayant des liaisons spéciﬁques non
covalentes. Comme elles sont assez rares, l’inﬂuence sur l’ensemble est négligeable.

160

CHAPITRE7. APPLICATIONS

Taille de
groupes
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Moyenne

Signature atomique
20370
3846
1382
670
375
251
157
110
76
45
29
25
14
8
6
2
5
6
3
1
2.82

Nombre de groupes
Signature cumulée
22322
3728
1260
593
328
227
137
88
64
36
26
19
13
7
5
3
5
5
3
1
2.61

Signature binaire
31168
3557
795
222
111
41
17
5
7
6
1
2
0
1
0
0
0
0
0
0
1.48

Tableau 7.1 – Distribution des molécules selon leurs signatures : nombre de groupes de molécules
de même signature de chaque taille de groupe. La moyenne correspond au nombre moyen de com-
paraisons qu’il faut effectuer pour vériﬁer si la molécule est dans la base.

Taille Nombre

Signature atomique

Signature cumulée

Signature binaire

10
20
30
40
50
60
70
80
90
100
110
120
130
140
150
160
170
180
190
200
210
220
230
250
390

21
389
1144
1118
798
448
238
114
76
35
32
31
18
9
6
6
3
3
2
2
2
1
1
1
1

3,63E006
2,43E018
2,65E032
8,16E047
3,04E064
8,32E081
1,20E100
7,16E118
1,49E138
9,33E157
1,59E178
6,69E198
6,47E219
1,35E241
5,71E262
4,71E284
7,26E306
2,01E329
9,68E351
7,89E374
1,06E398
2,28E421
7,76E444
3,23E492
6,84E842

min
7,20E001
4,15E005
9,93E014
1,97E023
3,15E034
8,65E046
2,08E058
1,37E071
7,72E088
3,99E103
2,11E125
4,87E135
1,79E150
9,82E173
6,35E181
2,62E208
8,11E218
2,35E242
2,49E271
1,49E269
4,17E299
4,31E323
1,06E334
1,10E366
9,51E648

moy
2,18E003
3,18E014
2,26E024
5,31E035
4,03E049
1,50E062
3,00E086
1,29E092
9,38E106
1,06E120
1,26E143
1,49E158
3,29E172
1,03E191
9,46E200
3,55E219
9,75E235
2,07E245
2,36E289
9,20E278
1,23E307
4,31E323
1,06E334
1,10E366
9,51E648

max
1,73E004
1,22E017
2,42E027
3,79E038
1,61E052
2,67E064
7,14E088
6,59E093
6,68E108
2,10E121
4,05E144
4,62E159
4,18E173
8,06E191
3,85E201
1,63E220
2,93E236
3,10E245
4,73E289
1,84E279
2,46E307
4,31E323
1,06E334
1,10E366
9,51E648

min
7,20E001
4,15E004
1,79E009
7,93E021
8,07E022
3,38E035
1,05E055
1,27E059
6,60E086
3,99E103
3,77E123
4,87E135
1,79E150
9,82E173
6,35E181
2,62E208
8,11E218
2,35E242
2,49E271
4,97E268
4,17E299
4,31E323
1,06E334
1,10E366
9,51E648

moy
2,17E003
4,43E010
4,74E021
5,78E033
9,75E046
1,50E062
8,26E076
1,29E092
9,38E106
1,06E120
1,26E143
1,49E158
3,29E172
1,03E191
9,46E200
3,55E219
9,75E235
2,07E245
2,36E289
9,20E278
1,23E307
4,31E323
1,06E334
1,10E366
9,51E648

max
1,73E004
1,61E012
1,82E024
5,41E035
3,23E049
2,67E064
1,24E079
6,59E093
6,68E108
2,10E121
4,05E144
4,62E159
4,18E173
8,06E191
3,85E201
1,63E220
2,93E236
3,10E245
4,73E289
1,84E279
2,46E307
4,31E323
1,06E334
1,10E366
9,51E648

min
1,20E001
4,15E004
6,69E010
5,21E019
1,76E028
1,43E037
1,17E050
1,85E062
2,02E081
2,13E092
3,51E110
1,22E118
1,14E132
7,63E155
5,50E160
1,19E189
2,68E196
1,21E219
1,32E249
4,65E242
6,12E279
3,05E304
5,95E310
1,24E339
7,86E589

moy
2,59E002
2,21E010
8,96E020
1,97E033
2,60E046
2,83E061
6,53E076
3,79E089
3,01E101
2,79E116
2,81E135
2,01E154
2,62E170
9,55E178
3,61E183
5,52E215
9,75E235
1,53E224
3,02E268
2,77E254
1,25E294
3,05E304
5,95E310
1,24E339
7,86E589

max
1,44E003
1,61E012
4,41E023
5,41E035
1,14E049
2,54E063
1,24E079
3,57E091
1,66E103
8,46E117
8,85E136
6,23E155
4,71E171
8,60E179
1,81E184
1,66E216
2,93E236
2,30E224
6,05E268
5,53E254
2,49E294
3,05E304
5,95E310
1,24E339
7,86E589

Tableau 7.2 – Comparaison de la taille de l’espace de recherche en utilisant les différentes signatures.

7
.
2
.

M
O
L
É
C
U
L
E
S

1
6
1

162

CHAPITRE7. APPLICATIONS

Molécules isomorphes - 22 atomes - UPBX

aucun
type d’atome
degré cumulatif
degré binaire

 

1

0.8

0.6

0.4

0.2

n
o
i
s
i
c
é
r
P

0

 
0

0.1

0.2

0.3

0.4

Temps de calcul en secondes

0.5

0.6

0.7

0.8

0.9

1

FIGURE 7.10 – Appariement de molécules isomorphes en utilisant des différentes signatures.

Nous examinons par la suite l’effet de l’étiquetage en combinaison avec notre algorithme géné-
tique. Nous utilisons toutes les molécules de taille 22. Le choix est basé sur le nombre de molécules
similaires après le ﬁltrage par signature. En effet, la base contient plus de mille molécules de taille
22. Par la suite, il y a un nombre sufﬁsant de molécules non-ﬁltrables même quand on utilise la si-
gnature la plus discriminante (la signature binaire). Nous distinguons le cas positif et le cas négatif.
Il s’agit du cas positif si les molécules à apparier sont isomorphes (exactement). En pratique, on
apparie une molécule avec elle-même. Dans le cas négatif, les molécules ne sont pas isomorphes,
mais l’appariement est nécessaire car les signatures sont identiques. C’est en effet dans ce cas que
nous avons besoin d’un nombre assez grand de molécules ayant la même taille (et la même signa-
ture). Beaucoup d’algorithmes montrent un comportement différent selon le cas positif ou négatif,
en particulier l’A*. Le cas négatif est beaucoup plus complexe. Les résultats avec l’opérateur UPBX
sont donnés dans la ﬁgure 7.10 (cas positif) et la ﬁgure 7.11 (cas négatif).

Il faut noter que si l’on se trouve dans le cas inexact, et on cherche à calculer une distance, il
est possible, selon la déﬁnition du coût d’une substitution, que l’alignement d’un atome de type A
avec un atome de type B donne une distance inférieure que celle obtenue si l’on respecte les types
d’atomes. Dans un tel cas, le ﬁltrage par signature et l’approche par couleur excluent l’appariement
optimal. Par conséquent, les valeurs des distances « optimales » peuvent différer selon le type de
signature.

Molécules non isomorphes - 22 atomes - UPBX

aucun
type d’atome
degré cumulatif
degré binaire

163

 

7.3. BILAN

1

0.8

0.6

0.4

0.2

n
o
i
s
i
c
é
r
P

0

 
0

0.1

0.2

0.3

0.4

Temps de calcul en secondes

0.5

0.6

0.7

0.8

0.9

1

FIGURE 7.11 – Appariement de molécules non isomorphes en utilisant des différentes signatures.

7.3 Bilan

Dans ce chapitre, nous avons illustré trois applications concrètes de notre algorithme. Nous
constatons des grandes différences dans les performances. En fait, il s’agit de trois problèmes très
différents. Quant à l’application de l’identiﬁcation de personnes basée sur des modèles en trois
dimensions, nous nous situons dans un contexte d’isomorphisme inexact de sous-graphes. Il y a très
peu de perturbations structurelles, mais des perturbations importantes au niveau des attributs. Dans
ce cas, notre algorithme montre des très bonnes performances.

Dans l’application sur la base COIL-Del, les graphes sont soumis à des perturbations structu-
relles importantes mais n’ont a priori pas d’attributs. Dans ce cas, nous observons des performances
assez faibles. Toutefois, il est possible d’augmenter la précision en introduisant des attributs tels que
la distance entre les points d’intérêt de l’image ou encore leurs coordonnées.

En ce qui concerne les molécules, il s’agit d’un problème d’isomorphisme exact pour des graphes
avec des étiquettes discrètes. Aﬁn de ﬁltrer la base et de restreindre l’espace de recherche, nous in-
troduisons trois types de signatures des sommets. Nous constatons que les résultats sans signatures
sont assez faibles tandis qu’avec les signatures ils deviennent acceptables.

Nous pouvons conclure que notre algorithme peut être appliqué à tous les problèmes d’isomor-
phisme de graphes, soit l’isomorphisme exact ou inexact, soit l’isomorphisme des graphes de même
taille ou l’isomorphisme de sous-graphes. Quant à l’isomorphisme exact, il est important de noter
que, pour ce cas, il existe des méthodes exhaustives bien plus performantes. Les meilleures perfor-

164

CHAPITRE7. APPLICATIONS

mances sont obtenues sur des problèmes d’isomorphisme inexact. Notre algorithme n’est dans ce
cas pas sensible aux perturbations des valeurs des étiquettes mais plutôt aux perturbations structu-
relles. Contrairement aux algorithmes de recherche d’arbre, le temps de calcul diffère très peu entre
des cas où les graphes correspondent et des cas où des graphes ne correspondent pas. Pour d’autres
méthodes, notamment l’algorithme A*, le temps de calcul est beaucoup plus élevé dans le deuxième
cas.

Chapitre 8

Conclusion et perspectives

Contribution

Nous avons considéré une approche génétique pour la résolution du problème d’isomorphisme
de graphes. Nous avons réalisé des études à la fois des composants internes de l’algorithme géné-
tique, de leurs interactions et d’autres méthodes. Dans ce cadre, nous avons effectué une compa-
raison exhaustive des opérateurs et étudié plusieurs aspects différents pour améliorer les résultats.
Nous nous sommes intéressés en particulier à la combinaison avec des heuristiques qui n’entraînent
pas de contraintes spéciﬁques, dédiées à des applications concrètes. Par conséquent, nous pouvons
appliquer presque toutes les méthodes présentées ici à toutes les applications qui peuvent être modé-
lisées par l’un des quatre problèmes d’appariement de graphes, déﬁnis par les quatre combinaisons
possibles des termes opposés exact, inexact et graphe, sous-graphe.

Opérateurs

Nous avons proposé au total cinq nouveaux opérateurs de croisement, deux opérateurs de per-
mutation, PBX et UPBX, et trois opérateurs intelligents, DPX, SDPX et NDPX, qui incluent une
heuristique gloutonne. Nous avons mené une étude complète et exhaustive pour le paramétrage,
compris au sens large et incluant le type de moteur d’évolution, de ces opérateurs ainsi que pour les
opérateurs existants aﬁn de pouvoir les comparer entre eux. En effet, les paramètres optimaux ainsi
que la sensibilité à leur égard changent selon l’opérateur.

Nous avons montré que les opérateurs proposés ont des performances qualitativement et quanti-
tativement supérieures à celles des autres opérateurs. Parmi les opérateurs proposés, les opérateurs
intelligents fournissent les meilleurs résultats.

Nous avons réalisé une étude complémentaire sur la mutation, dans laquelle nous avons montré
que l’application d’une mutation plus perturbatrice au lieu d’une mutation élémentaire n’améliore
guère les résultats.

165

166

CHAPITRE8. CONCLUSIONETPERSPECTIVES

Algorithme mémétique

Aﬁn d’augmenter le taux de succès pratique et de permettre à l’algorithme génétique d’exploi-
ter les solutions prometteuses, nous avons étudié sa combinaison avec une heuristique de recherche
locale. Il est généralement admis que les algorithmes génétiques proﬁtent de l’intégration d’heuris-
tiques spécialisées au problème donné. Les expériences exhaustives que nous avons menées sur une
heuristique particulière ont permis de souligner que celle-ci peut effectivement améliorer la préci-
sion pour certains opérateurs, mais aussi réduire le temps de calcul, en particulier en combinaison
avec les opérateurs intelligents.

Parallélisation

Nous avons également réalisé une implémentation parallèle de l’algorithme dans le but d’accé-
lérer l’exécution. Le gain, estimé comme assez limité en théorie, s’est avéré plus limité encore dans
la pratique, en particulier en ce qui concerne les appariements de graphes de petite taille. On peut
pourtant proﬁter des avantages de la parallélisation lorsque la complexité du problème augmente.

Garantie du résultat optimal

La garantie de l’optimalité des résultats est un aspect plus théorique par rapport à ce qui précède.
Nous avons proposé d’abord des stratégies qui peuvent accélérer l’algorithme A*, avant de proposer
la combinaison de ce dernier avec notre approche évolutionnaire. L’algorithme génétique sert alors à
fournir une première bonne estimation pour l’algorithme de recherche exhaustive qui vériﬁe ensuite
s’il existe encore une meilleure solution. Nous avons montré que le temps de calcul de cette approche
hybride est inférieur à celui de l’approche A* seule. L’approche hybride n’est que très légèrement
plus lente que l’algorithme génétique.

Applications

Enﬁn, nous avons testé notre algorithme sur trois applications représentant différents aspects du
problème d’isomorphisme de graphes aﬁn de démontrer effectivement sa généralité. Nous l’avons
comparé en particulier avec l’algorithme A* sur des graphes décrivant des modèles faciaux. Nous
avons aussi montré que l’algorithme génétique fournit des résultats supérieurs à ce dernier, en termes
de taille de graphes que l’on peut traiter ainsi qu’en termes de temps de calcul pour une application
réelle avec des petit graphes, bien que ces derniers aient dû favoriser A*.

Pour de nombreux algorithmes, dont A*, on observe une incohérence entre le cas où l’on ap-
parie deux graphes correspondants au même objet et le cas où l’on apparie deux graphes provenant
d’objets différents (match time et no match time). Pour les approches exhaustives comme A*, ce der-
nier cas est généralement beaucoup plus difﬁcile et le temps de calcul est largement supérieur. Nous

constatons que notre approche ne montre pas de sensibilité à ce propos, à l’exception des opérateurs
de croisement intelligents pour lesquels nous observons une augmentation très faible du temps de
calcul dans le cas négatif, mais qui reste négligeable par rapport à celui de A*.

167

Perspectives

Nos travaux ouvrent des perspectives vers trois directions principales que nous détaillons ci-
dessous. Premièrement, à un niveau applicatif, on peut s’interroger sur la pertinence et les particu-
larités de la description des différents types de données sous forme de graphes. Deuxièmement, il
serait avantageux d’approfondir les combinaisons et interactions entre les différents aspects évoqués,
et isolément d’y inclure des méthodes adaptatives. Enﬁn, l’étude d’une généralisation additionnelle
de notre approche sur les appariements multivoques permettrait de traiter des problèmes plus com-
plexes encore.

Pertinence de la représentation des données

Nous avons constaté, dans le cas de l’appariement d’images en deux dimensions avec des mo-
dèles en trois dimensions, que les modèles de graphes sont aujourd’hui assez réduits aﬁn de pouvoir
résoudre les problèmes associés avec les algorithmes actuels. De plus, il s’avère que le modèle qui
correspond à la plus petite distance entre les graphes de visages ne correspond pas toujours à la
même personne, ce qui pose un problème d’importance. Une solution peut consister à augmenter
l’expressivité de la représentation. En général, cela augmente le coût du calcul.

Les travaux que nous avons menés ont permis de déﬁnir un outil performant pour traiter efﬁ-
cacement des graphes plus grands. Cependant, en même temps que l’on augmente l’expressivité,
on risque de perdre en généralité, car les descriptions deviennent de plus en plus dépendantes de
l’application concrète. De plus, les effets du bruit s’accentuent sur des modèles de granularité de
plus en plus ﬁne. Il serait donc avantageux d’étudier en détail toute la chaîne de la représentation
aﬁn de s’assurer de la pertinence de la représentation obtenue. Il serait particulièrement intéressant
d’appliquer des méthodes d’apprentissage pour déterminer les paramètres de la fonction à optimiser.
Dans un cadre applicatif il s’agit en particulier des coûts et poids associés à la correction d’erreurs
aux niveaux des attributs et de la structure du graphe.

Interactions et adaptivité

Nous avons observé que, dans un algorithme génétique, toutes les parties, comme par exemple

les opérateurs et la recherche locale, interagissent.

168

CHAPITRE8. CONCLUSIONETPERSPECTIVES

Expérimentations complémentaires En particulier, les interactions entre la recherche locale et
les différents opérateurs de croisement méritent une analyse plus approfondie.

En ce qui concerne la stratégie de parallélisation, nous avons choisi la stratégie maître-esclave
aﬁn de limiter les interactions avec les autres parties de l’algorithme. Une approche différente,
comme par exemple le modèle en îlots, change les propriétés de l’algorithme évolutionnaire sur
lequel il est basé et peut améliorer les résultats. Dans ce cas, une révision des méthodes de recherche
locale et des opérateurs utilisés pourrait mener à des résultats intéressants.

Des études plus approfondies sont également envisageables en ce qui concerne le critère d’arrêt
et des stratégies de sélection autres que la sélection par tournoi. Enﬁn, nous aimerions comparer
notre approche avec les résultats d’autres méta-heuristiques comme les colonies de fourmis, qui ont
été utilisées pour résoudre le problème d’isomorphisme de graphes.

Adaptivité
Il nous paraît particulièrement intéressant de combiner la recherche locale avec la mu-
tation scramble car les deux approches sont complémentaires. La recherche locale exploite les solu-
tions trouvées, tandis que la mutation scramble permet une exploration plus dispersée que la muta-
tion d’échange.

Une autre piste est l’utilisation d’un algorithme adaptatif dont le comportement change selon
les différentes étapes d’exécution. Il existe par exemple des stratégies de contrôle de paramètres en
fonction des mesures dynamiques de la population, comme par exemple celle de sa diversité.

Dans ce cadre, on pourrait, par exemple, adapter le type de mutation ainsi que, s’il s’agit de
la mutation scramble, son paramètre. Si la diversité est faible, alors on utilisera une mutation plus
perturbatrice aﬁn de l’augmenter. Dans le cas contraire, on utilisera la mutation d’échange aﬁn de
mieux exploiter localement les environs des individus. Ce choix pourrait également être effectué au
niveau local, plutôt que global, en regardant au lieu de la diversité le nombre d’individus dans le
voisinage proche de l’individu qui subit la mutation. S’il existe de nombreux autres individus dans
ce voisinage, alors il est plus pertinent d’appliquer plus de perturbations pour explorer une autre
partie de l’espace de recherche.

Généralisation à des appariements multivoques

Bien que notre approche soit robuste aux perturbations d’attributs comme nous l’avons observé
dans le cadre de l’application aux images en trois dimensions, elle est sensible aux perturbations
structurelles comme nous l’avons constaté dans les applications aux images de la base COIL et aux
molécules. Il existe même d’autres perturbations comme par exemple celle des effets de granularité.
En effet, dans de nombreuses applications, les graphes sont extraits automatiquement à partir de
données du domaine. Ainsi, dans le domaine des images, ils sont obtenus à partir de méthodes
de segmentation automatique. Ces méthodes ne fournissent pas des résultats parfaits, ce qui peut

169

mener, dans le cas d’images, à des effets de sous ou sur-segmentation. Dans d’autres domaines, on
peut également trouver des descriptions à différents niveaux de granularité.

Une piste intéressante pour augmenter la robustesse à tous ces changements est l’étude des ap-
pariements multivoques. Ces derniers permettent d’apparier un sommet à plusieurs autres sommets.
Par conséquent, il ne s’agit pas d’une injection comme dans le cadre des appariements univoques.
La représentation par permutation ne pouvant donc être appliquée, d’autres représentations doivent
être recherchées pour ce type de problème, mais généralement le changement de la représentation
entraîne le changement de tous les opérateurs génétiques. Néanmoins, nous pouvons constater dans
l’état de l’art que, dans le domaine des algorithmes génétiques, une meilleure représentation mène
souvent à des résultats largement supérieurs et que ceci constitue donc une voie prometteuse.

Dans un premier temps, on pourrait étudier l’ajout d’une étape complémentaire de prétraitement
qui identiﬁerait les perturbations possibles, notamment la fusion et la division de sommets mais aussi
la substitution d’arêtes. Comme l’identiﬁcation de ces perturbations peut être un autre problème
d’optimisation lié à l’isomorphisme, l’application d’une stratégie co-évolutionnaire combinant notre
approche actuelle avec une méthode génétique qui optimiserait le coût des perturbations semble une
voie prometteuse. Celle-ci permettrait en effet de conserver la représentation des permutations par
le chromosome modélisant l’appariement univoque, tandis que les relations multivoques seraient
traitées par le second chromosome.

170

CHAPITRE8. CONCLUSIONETPERSPECTIVES

Bibliographie

E. Alba & M. Tomassini. Parallelism and Evolutionary Algorithms. IEEE Trans. on Evolutionary

Computation, 6(5):443–462, 2002.

H. Almohamad & S. Duffuaa. A Linear Programming Approach for the Weighted Graph Matching
Problem. IEEE Transactions on Pattern Analysis and Machine Intelligence, 15(5):522–525, 1993.

R. Ambauen, S. Fischer, & H. Bunke. Graph Edit Distance with Node Splitting and Merging, and
its Application to Diatom Identiﬁcation. Dans E. Hancock & M. Vento, Éds., IAPR Workshop
GbRPR, vol. 2726 de LNCS, pages 95–106. Springer, 2003.

B. Amedro, V. Bodnartchouk, D. Caromel, C. Delbé, F. Huet, & G. L. Taboada. Current State of

Java for HPC. Rapport technique 0353, INRIA Sophia Antipolis, 2008.

P. J. Artymiuk, R. V. Spriggs, & P. Willett. Graph Theoretic Methods for the Analysis of Structural
Relationships in Biological Macromolecules. Journal of the American Society for Information
Science and Technology, 56(5):518–528, 2005.

A. Auger. Convergence results for the (1, λ)-SA-ES using the theory of φ-irreducible Markov chains.

Theoretical Computer Science, 334:35–69, 2005.

A. Auger & O. Teytaud. Continuous Lunches are free plus the Design of Optimal Optimization

Algorithms. Algorithmica, 2008.

S. Auwatanamongkol. Inexact Graph Matching using a Genetic Algorithm for Image Recognition.

Pattern Recognition Letters, 28:1428–1437, 2007.

L. Babai, D. Grigoryev, & D. Mount.

Isomorphism of Graphs with Bounded Eigenvalue Multi-
plicity. Dans Proceedings of the fourteenth annual ACM symposium on Theory of computing
(STOC), pages 310–324. ACM Press, New York, NY, USA, 1982. ISBN 0-89791-070-2. doi:
http://doi.acm.org/10.1145/800070.802206.

T. Bäck, U. Hammel, & H.-P. Schwefel. Evolutionary Computation: Comments on the History and

Current State. IEEE Trans. on Evolutionary Computation, 1(1):3–17, 1997.

171

172

BIBLIOGRAPHIE

J. S. Beis & D. G. Lowe. Shape Indexing using Approximate Nearest-Neighbor Search in High-
Dimensional Spaces. Dans Proc. Conf. on Computer Vision and Pattern Recognition (CVPR),
pages 1000–1006. IEEE Computer Society, San Juan, Puerto Rico, 1997.

J. S. Beis & D. G. Lowe. Indexing without Invariants in 3D Object Recognition. IEEE Trans. on

Pattern Analysis and Machine Intelligence, 21(10):1000–1015, 1999.

E. Bengoetxea, P. Larrañaga, I. Bloch, A. Perchant, & C. Boeres. Inexact Graph Matching by means

of Estimation of Distribution Algorithms. Pattern Recognition, 35:2867–2880, 2002.

S. Berretti, A. Del Bimbo, & P. Pala. 3D Face Recognition by Spatial Arrangement of Iso-Geodesic
Surfaces. Dans 3DTV Conference: The True Vision - Capture, Transmission and Display of 3D
Video. IEEE, 2008.

S. Berretti, A. Del Bimbo, & E. Vicario. Efﬁcient Matching and Indexing of Graph Models in
IEEE Transactions on Pattern Analysis and Machine Intelligence,

Content-Based Retrieval.
23(10):1089–1105, 2001.

M. Birattari, T. Stützle, L. Paquete, & K. Varrentrapp. A Racing Algorithm for Conﬁguring Meta-

heuristics. Dans Proc. GECCO’02. 2002.

T. Blickle & L. Thiele. A Comparison of Selection Schemes used in Evolutionary Algorithms.

Evolutionary Computation, 4(4):361–394, 1996.

M. C. Boeres, C. C. Ribeiro, & I. Bloch. A Randomized Heuristic for Scene Recognition by Graph
Matching. Dans C. Ribeiro & S. Martins, Éds., WEA, vol. 3059 de LNCS, pages 100–113. Sprin-
ger, 2004.

A. Bouvier, F. Le Lionnais, & M. George. Dictionnaire des Mathématiques. Presses Universitaires

de France, 2009.

Y. Boykov & V. Kolmogorov. An Experimental Comparison of Min-Cut/Max-Flow Algorithms
for Energy Minimization in Vision. IEEE Trans. on Pattern Analysis and Machine Intelligence,
26(9):1124–1137, 2004.

Y. Boykov, O. Veksler, & R. Zabih. Fast Approximate Energy Minimization via Graph Cuts. IEEE

Trans. on Pattern Analysis and Machine Intelligence, 23(11):1222–1239, 2001.

T. Bärecke. Rapport sur les Résultats d’Agrégation des Mesures de plus de 3 Composantes des
Coûts - Cas d’Étude: Du Puits à la Roue. Rapport technique, Université Pierre et Marie Curie -
Paris6 UMR 7606, DAPA, LIP6, 2007.

173

T. Bärecke & M. Detyniecki. Combining Exhaustive and Approximate Methods for Improved Sub-
Graph Matching. Dans Proc. of the Int. Workshop on Advances in Pattern Recognition. Springer,
2007a.

T. Bärecke & M. Detyniecki. Memetic Algorithms for Inexact Graph Matching. Dans Proc. of the

IEEE Congress on Evolutionary Computation - CEC’07. IEEE, Singapore, 2007b.

H. Bunke. On a Relation between Graph Edit Distance and Maximum Common Subgraph. Pattern

Recognition Letters, 18:689–694, 1997.

H. Bunke. Error Correcting Graph Matching: On the Inﬂuence of the Underlying Cost Function.
IEEE Transactions on Pattern Analysis and Mach Intelligence, 21(9):917–922, 1999. ISSN 0162-
8828. doi:http://dx.doi.org/10.1109/34.790431.

H. Bunke, P. Foggia, C. Guidobaldi, & M. Vento. Graph Clustering using the Weighted Minimum
Common Supergraph. Dans E. Hancock & M. Vento, Éds., IAPR Workshop GbRPR, vol. 2726 de
LNCS, pages 235–246. Springer, 2003.

H. Bunke & K. Shearer. A Graph Distance Metric based on the Maximal Common Subgraph.

Pattern Recognition Letters, 19:255–259, 1998.

L. S. Buriol, P. M. França, & P. Moscato. A New Memetic Algorithm for the Asymmetric Traveling

Salesman Problem. Journal of Heuristics, 10(5):483–506, 2004.

K. Burjorjee. Sufﬁcient Conditions for Coarse-Graining Evolutionary Dynamics. Dans Foundations

of Genetic Algorithms, vol. 4436 de LNCS, pages 35–53. Springer, 2007.

T. Caelli & S. Kosinov. Inexact Graph Matching using Eigen-Subspace Projection Clustering. In-

ternational Journal of Pattern Recognition and Artiﬁcial Intelligence, 18(3):329–354, 2004.

T. S. Caetano & T. Caelli. Approximating the Problem, not the Solution: An Alternative View of

Point Set Matching. Pattern Recognition, 39(4):552–561, 2006.

T. S. Caetano, T. Caelli, D. Schuurmans, & D. A. C. Barone. Graphical Models and Point Pattern
Matching. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(10):1646–1663,
2006.

V. Campos, M. Laguna, & R. Martí. Context-Independent Scatter and Tabu Search for Permutation

Problems. INFORMS Journal on Computing, 17(1):111–122, 2005.

E. Cantú-Paz. Migration Policies, Selection Pressure, and Parallel Evolutionary Algorithms. Journal

of Heuristics, 7(4):311–334, 2001.

174

BIBLIOGRAPHIE

E. Cantú-Paz. Parameter Setting in Parallel Genetic Algorithms. Dans F. G. Lobo, C. F. Lima,
& Z. Michalewicz, Éds., Parameter Setting in Evolutionary Algorithms, vol. 54 de Studies in
Computational Intelligence. Springer, 2007.

E. Cantú-Paz & D. E. Goldberg. On the Scalability of Parallel Genetic Algorithms. Evolutionary

Computation, 7(4):429–449, 1999.

M. Carcassoni & E. R. Hancock. Weighted Graph-Matching using Modal Clusters. Dans GbRPR,

pages 260–269. 2001.

M. Carcassoni & E. R. Hancock. Spectral Correspondence for Point Pattern Matching. Pattern

Recognition, 36(1):193–204, 2003.

J. Cervantes & C. R. Stephens. « Optimal » Mutation Rates for Genetic Search. Dans GECCO,

pages 1313–1320. 2006.

R. M. Cesar, Jr., E. Bengoetxea, I. Bloch, & P. Larrañaga.

Inexact Graph Matching for Model-
Based Recognition: Evaluation and Comparison of Optimization Algorithms. Pattern Recogni-
tion, 38(11):2099–2113, 2005.

P.-A. Champin & C. Solnon. Measuring the Similarity of Labeled Graphs. Dans K. Ashley &

B. D.G., Éds., ICCBR, vol. 2689 de LNCS, pages 80–95. Springer, 2003.

F. Chevalier, J.-P. Domenger, J. Benois-Pineau, & M. Delest. Retrieval of Objects in Video by

Similarity based on Graph Matching. Pattern Recognition Letters, 28:939–949, 2007.

F. R. Chung. Spectral Graph Theory. Numéro 92 dans CBMS Regional Conference Series in

Mathematics. American Mathematical Society, 1997.

V. Cicirello & S. Smith. Modeling GA Performance for Control Parameter Optimization. Dans
GECCO-2000: Proc. of the Genetic and Evolutionary Computation Conference, pages 235–242.
Morgan Kaufmann Publishers, 2000.

V. A. Cicirello. Non-Wrapping Order Crossover: An Order Preserving Crossover Operator that

Respects Absolute Position. Dans GECCO. Seattle, Washington, USA, 2006.

D. Conte, P. Foggia, C. Sansone, & M. Vento. Thirty Years of Graph Matching in Pattern Recogni-

tion. Int. Journal of Pattern Recognition and Artiﬁcial Intelligence, 18(3):265–298, 2004.

L. P. Cordella, P. Foggia, C. Sansone, & M. Vento. A (Sub)Graph Isomorphism Algorithm for
IEEE Transactions on Pattern Analysis and Machine Intelligence,

Matching Large Graphs.
26(10):1367–1372, 2004. ISSN 0162-8828. doi:http://dx.doi.org/10.1109/TPAMI.2004.75.

175

A. D. Cross & E. R. Hancock. Convergence of a Hill Climbing Genetic Algorithm for Graph
Matching. Dans E. Hancock & M. Pelillo, Éds., EMMCVPR, vol. 1654 de LNCS, pages 221–236.
1999.

A. D. J. Cross, R. C. Wilson, & E. R. Hancock.

Inexact Graph Matching using Genetic Search.

Pattern Recognition, 30(6):953–970, 1997.

L. Davis. Applying Adaptive Algorithms to Epistatic Domains. Dans Proc. Int. Joint Conf. on

Artiﬁcial Intelligence. 1985.

L. Davis. Handbook of Genetic Algorithms. Van Nostrand Reinhold, 1991.

B. Dawson.

Comparing

Floating

Point Numbers.

http://www.cygnus-

software.com/papers/comparingﬂoats/comparingﬂoats.htm, 2008. Consulté le 14/11/2008.

K. A. De Jong. An Analysis of the Behavior of a Class of Genetic Adaptive Systems. Thèse de

doctorat, University of Michigan, 1975.

K. A. De Jong & J. Sarma. Generation Gaps Revisited. Dans Foundations of Genetic Algorithms.

Morgan Kaufmann, 1993.

M. F. Demirci, A. Shokoufandeh, Y. Keselman, L. Bretzner, & S. Dickinson. Object Recognition
as Many-to-Many Feature Matching. International Journal of Computer Vision, 69(2):203–222,
2006.

A. Deruyver, Y. Hodé, E. Leammer, & J.-M. Jolion. Adaptive Pyramid and Semantic Graph: Know-

ledge Driven Segmentation. Dans GbRPR, vol. 3434 de LNCS, pages 213–222. Springer, 2005.

P. Dickinson, M. Kraetzl, H. Bunke, M. Neuhaus, & A. Dadej. Similarity Measures for Hierarchical
Representations of Graphs with Unique Node Labels. International Journal of Pattern Recogni-
tion and Artiﬁcial Intelligence, 18(3):425–442, 2004.

R. Diestel. Graph Theory, vol. 173 de Graduate Texts in Mathematics. Springer, 3ème édition,

2005.

A. Eiben, Z. Michalewicz, M. Schoenauer, & J. Smith. Parameter Control in Evolutionary Algo-
rithms. Dans J. Kacprzyk, Éd., Parameter Setting in Evolutionary Algorithms, Studies in Compu-
tational Intelligence, chapitre Parameter Control in Evolutionary Algorithms, pages 19–46. Sprin-
ger, 2007.

A. Eiben & J. Smith. Introduction to Evolutionary Computing. Natural Computing Series. Springer,

2nd édition, 2007.

176

BIBLIOGRAPHIE

A. E. Eiben, R. Hinterding, & Z. Michalewicz. Parameter Control in Evolutionary Algorithms. IEEE

Trans. on Evolutionary Computation, 3(2):124–141, 1999.

M. Eshera & K.-S. Fu. A Graph Distance Measure for Image Analysis.

IEEE Transactions on

Systems, Man, and Cybernetics, 14:398–407, 1984.

P. F. Felzenszwalb & D. P. Huttenlocher. Efﬁcient Graph-Based Image Segmentation. International

Journal of Computer Vision, 59(2):167–181, 2004.

I. Filotti & J. Mayer. A Polynomial-Time Algorithm for Determining the Isomorphism of Graphs of
Fixed Genus. Dans Proc. 12th Ann. ACM Symposium on Theory of Computing, pages 236–243.
1980.

L. Finkelstein & S. Markovitch. Optimal Schedules for Monitoring Anytime Algorithms. Artiﬁcial
Intelligence - Special Issue on Computational Tradeoffs under Bounded Resources, 126(1-2):63–
108, 2001.

P. Foggia, C. Sansone, & M. Vento. A Performance Comparison of Five Algorithms for Graph

Isomorphism. Dans GbRPR, pages 188–199. 2001.

S. Fortin. The Graph Isomorphism Problem. Rapport technique, University of Alberta, Canada,

1996.

B. Freisleben & P. Merz. New Genetic Local Search Operators for the Traveling Salesman Problem.

Dans Parallel Problem Solving from Nature - PPSN IV, pages 890–899. 1996.

M. Garey & D. Johnson. Computers and Intractability - A Guide to the Theory of NP-Completeness.

Freeman and Company, 1979.

S. Günter & H. Bunke. Self-Organizing Map for Clustering in the Graph Domain. Pattern Recogni-

tion Letters, 23:405–417, 2002.

S. Günter & H. Bunke. Validation Indices for Graph Clustering. Pattern Recognition Letters,

24:1107–1113, 2003.

C. Godsil & G. Royle. Algebraic Graph Theory, vol. 207 de Graduate texts in mathematics. Sprin-

ger, 2001.

S. Gold & A. Rangarajan. A Graduated Assignment Algorithm for Graph Matching. IEEE Tran-

sactions on Pattern Analysis and Machine Intelligence, 18(4):377–388, 1996.

S. Gold, A. Rangarajan, C.-P. Lu, P. Suguna, & E. Mjolsness. New Algorithms for 2D and 3D Point

Matching: Pose Estimation and Correspondence. Pattern Recognition, 31(8):1019–1031, 1998.

177

D. Goldberg. Genetic Algorithms in Search, Optimization & Machine Learning. Addison Wesley,

Reading, MA, 1989.

D. Goldberg. What every Computer Scientist should know about Floating-Point Arithmetic. ACM

Computing Surveys, 23(1):5–48, 1991a.

D. E. Goldberg. Genetic Algorithms, Noise, and the Sizing of Populations. Rapport technique

91010, University of Illinois, 1991b.

D. E. Goldberg & R. Lingle. Alleles, Loci and the Traveling Salesman Problem. Dans Proc. 1st Int.

Conf. on Genetic Algorithms, pages 154–159. 1985.

M. Gori, M. Maggini, & L. Sarti. Exact and Approximate Graph Matching using Random Walks.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(7):1100–1111, 2005. ISSN
0162-8828. doi:http://dx.doi.org/10.1109/TPAMI.2005.138.

J. J. Grefenstette. Optimization of Control Parameters for Genetic Algorithms. IEEE Transactions

Systems, Man, and Cybernetics, 16(1):122–128, 1986.

T. Gärtner, P. Flach, & S. Wrobel. On Graph Kernels: Hardness Results and Efﬁcient Alternatives.
Dans B. Schölkopf & M. K. Warmuth, Éds., Learning Theory and Kernel Machines, vol. 2777 de
Lecture Notes in Artiﬁcial Intelligence, pages 129–143. Springer, 2003.

E. A. Hansen & S. Zilberstein. Monitoring and Control of Anytime Algorithms: A Dynamic Pro-
gramming Approach. Artiﬁcial Intelligence - Special Issue on Computational Tradeoffs under
Bounded Resources, 126(1-2):139–157, 2001.

N. Hansen, S. Müller, & P. Koumoutsakos. Reducing the Time Complexity of the Derandomized
Evolution Strategy with Covariance Matrix Adaptation (CMA-ES). Evolutionary Computation,
11(1):1–18, 2003.

Z. Harchaoui & F. Bach.

Image Classiﬁcation with Segmentation Graph Kernels. Dans CVPR.

IEEE, 2007.

G. Harik, E. Cantú-Paz, D. E. Goldberg, & B. L. Miller. The Gambler’s Ruin Problem, Genetic

Algorithms, and the Sizing of Populations. Evolutionary Computation, 7(3):231–253, 1999.

G. R. Harik & F. G. Lobo. A Parameter-less Genetic Algorithm. Dans Proc. of the Genetic and

Evolutionary Computation Conference (GECCO), pages 258–265. Morgan Kaufman, 1999.

V. Heidrich-Meisner & C. Igel. Hoeffding and Bernstein Races for Selecting Policies in Evolutionary

Direct Policy Search. Dans Proc. Int. Conf. on Machine Learning (ICML). 2009.

178

BIBLIOGRAPHIE

D. Hidovic & M. Pelillo. Metrics for Attributed Graphs based on the Maximal Similarity Common
Subgraph. International Journal of Pattern Recognition and Artiﬁcial Intelligence, 18(3):299–
313, 2004.

Y. Ho & D. Pepyne. Simple Explanation of the No-Free-Lunch Theorem and its Implications.

Journal of Optimization Theory and Applications, 115(3):549–570, 2002.

J. Holland. Adaptation in Natural and Artiﬁcial Systems. The University of Michigan Press, Ann

Arbor, MI, 1975.

J. Hopcroft & R. Karp. An n5/2 Algorithm for Maximum Matchings in Bipartite Graphs. SIAM

Journal on Computing, 2(4):225–231, 1973.

J. Hopcroft & J. Wong. A Linear Time Algorithm for the Isomorphism of Planar Graphs. Dans

Proc. 6th Annual ACM Symp. on Theory of Computing, pages 172–184. 1974.

E. Horvitz & S. Zilberstein. Computational Tradeoffs under Bounded Resources. Artiﬁcial In-
telligence - Special Issue on Computational Tradeoffs under Bounded Resources, 126(1-2):1–4,
2001.

IEEE754. IEEE Standard for Binary Floating-Point Arithmetic. Standards Committee of the IEEE

Computer Society, 2008. doi:10.1109/IEEESTD.2008.4610935.

C. Igel & M. Toussaint. A No-Free-Lunch Theorem for Non-Uniform Distributions of Target Func-

tions. Journal of Mathematical Modelling and Algorithms, 3:313–322, 2004.

A. Inokuchi, T. Washio, & H. Motoda. A General Framework for Mining Frequent Subgraphs from

Labeled Graphs. Fundamenta Informaticae, 66:53–82, 2005.

C. Irniger & H. Bunke. Decision Trees for Error-Tolerant Graph Database Filtering. Dans L. Brun

& M. Vento, Éds., GbRPR, vol. 3434 de LNCS, pages 301–311. Springer, 2005.

C.-A. M. Irniger. Graph Matching - Filtering Databases of Graphs Using Machine Learning Tech-
niques. Thèse de doctorat, Institut für Informatik und angewandte Mathematik, Universität Bern,
2005.

H. Ishibuchi & T. Murata. A Multi-Objective Genetic Local Search Algorithm and its Applica-
tion to Flowshop Scheduling. IEEE Transactions on Systems, Man, and Cybernetics, Appl. Rev.,
28(3):392–403, 1998.

H. Ishibuchi, T. Yoshida, & T. Murata. Balance between Genetic Search and Local Search in Me-
metic Algorithms for Multiobjective Permutation Flowshop Scheduling. IEEE Trans. on Evolu-
tionary Computation, 7(2):204–223, 2003.

179

B. J. Jain & F. Wysotzki. Solving Inexact Graph Isomorphism Problems using Neural Networks.

Neurocomputing, 63:45–67, 2005.

A. Katok, B. Hasselblatt, & L. Mendoza. Introduction to the Modern Theory of Dynamical Systems.

Cambridge University Press, 1997.

J. Köbler, U. Schöning, & J. Torán. The Graph Isomorphism Problem: Its Structural Complexity.

Birkhäuser, Boston, 1993.

K. G. Khoo & P. N. Suganthan. Structural Pattern Recognition using Genetic Algorithms with
IEEE Trans. on Systems, Man, and Cybernetics - Part B: Cybernetics,

Specialized Operators.
33(1):156–165, 2003.

R. I. Kondor & J. Lafferty. Diffusion Kernels on Graphs and other Discrete Structures. Dans Proc.

of the ICML. 2002.

Z. Konfršt. Parallel Genetic Algorithms: Advances, Computing Trends, Applications and Perspec-
tives. Dans Proc. 18th Int. Parallel and Distributed Processing Symposium (IPDPS’04). IEEE,
2004.

N. Krasnogor & J. Smith. A Tutorial for Competent Memetic Algorithms: Model, Taxonomy, and

Design Issues. IEEE Transactions on Evolutionary Computation, 9(5):474–488, 2005.

R. Krishnapuram, S. Medasani, S.-H. Jung, Y.-S. Choi, & R. Balasubramaniam. Content-based
Image Retrieval based on a Fuzzy Approach. IEEE Transactions on Knowledge and Data Engi-
neering, 16(10):1185–1199, 2004.

W. G. Kropatsch, Y. Haxhimusa, & A. Ion. Multiresolution Image Segmentations in Graph Pyra-
mids. Dans Applied Graph Theory in Computer Vision and Pattern Recognition, vol. 52 de Studies
in Computational Intelligence. Springer, 2007.

P. Larranaga, C. Kuijpers, R. Murga, I. Inza, & S. Dizdarevic. Genetic Algorithms for the Travelling
Salesman Problem: A Review of Representations and Operators. Artiﬁcial Intelligence Review,
13:129–170, 1999.

O. Lezoray, A. Elmoataz, & S. Bougleux. Graph Regularization for Color Image Processing. Com-

puter Vision and Image Understanding, 107:38–55, 2007.

Y. Li, D. Blostein, & P. Abolmaesumi. Asymmetric Inexact Matching of Spatially-Attributed
Graphs. Dans L. Brun & M. Vento, Éds., GbRPR, vol. 3434 de LNCS, pages 253–262. Sprin-
ger, 2005.

180

BIBLIOGRAPHIE

Y. Liu, D. Zhang, G. Lu, & W.-Y. Ma. A Survey of Content-based Image Retrieval with High-Level

Semantics. Pattern Recognition, 40:262–282, 2007.

H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, & C. Watkins. Text Classiﬁcation using

String Kernels. Journal of Machine Learning Research, 2:419–444, 2002.

D. G. Lowe. Distinctive Image Features from Scale-Invariant Keypoints. International Journal of

Computer Vision, 60(2):91–110, 2004.

E. Luks. Isomorphism of Graphs of Bounded Valence can be tested in Polynomial Time. Journal of

Computer and System Sciences, 25:42–65, 1982.

B. Luo & E. R. Hancock. Structural Graph Matching Using the EM Algorithm and Singular Value
Decomposition. IEEE Trans. on Pattern Analysis and Machine Intelligence, 23(10):1120–1136,
2001.

S. Marini, M. Spagnuolo, & B. Falcidieno. From Exact to Approximate Maximum Common Sub-
graph. Dans L. Brun & M. Vento, Éds., GbRPR, vol. 3434 de LNCS, pages 263–272. Springer,
2005.

O. Maron & A. Moore. Hoeffding Races: Accelerating Model Selection Search for Classiﬁcation
and Function Approximation. Dans Proc. Advances in Neural Information Processing Systems.
1994.

M. Matsumoto & T. Nishimura. Mersenne Twister: A 623-Dimensionally Equidistributed Uniform
Pseudo-Random Number Generator. ACM Trans. Model. Comput. Simul., 8(1):3–30, 1998. ISSN
1049-3301. doi:http://doi.acm.org/10.1145/272991.272995.

J. McCall. Genetic Algorithms for Modelling and Optimisation. Journal of Computational and

Applied Mathematics, 184(1):205–222, 2005.

R. McGill, J. W. Tukey, & W. Larsen. Variations of Box Plots. The American Statistician, 32:12–16,

1978.

B. D. McKay. Practical Graph Isomorphism. Congressus Numerantium, 30:45–87, 1981.

P. Merz. Memetic Algorithms for Combinatorial Optimization Problems: Fitness Landscapes and
Effective Search Strategies. Thèse de doctorat, Parallel Syst. Res. Group, Dept. Electr. Eng. Com-
put. Sci., Univ. Siegen, Germany, 2000.

P. Merz. Advanced Fitness Landscape Analysis and the Performance of Memetic Algorithms. Evo-
lutionary Computation, Special Issue on Memetic Evolutionary Algorithms, 12(3):303–326, 2004.

181

P. Merz & B. Freisleben. A Genetic Local Search Approach to the Quadratic Assignment Problem.
Dans Proc. of the 7th Int. Conf. on Genetic Algorithms, pages 465–472. Morgan Kaufmann, East
Lansing, MI, USA, 1997.

P. Merz & B. Freisleben. Fitness Landscapes, Memetic Algorithms, and Greedy Operators for Graph

Bipartitioning. Evolutionary Computation, 8(1):61–91, 2000.

P. Merz & B. Freisleben. Memetic Algorithms for the Traveling Salesman Problem. Complex

Systems, 13(4):297–345, 2001.

B. T. Messmer & H. Bunke. Efﬁcient Subgraph Isomorphism Detection - A Decomposition Ap-

proach. IEEE Trans. on Knowledge and Data Engineering, 12(2):307–323, 2000.

H. Mühlenbein. How Genetic Algorithms Really Work - Part I: Mutation and Hillclimbing. Dans

Proc. Conf. Parallel Problem Solving from Nature, pages 15–25. 1992.

Z. Michalewicz. Genetic Algorithms + Data Structures = Evolution Programs. Springer, Berlin,

Germany, 3 édition, 1996.

K. Mikolajczyk & C. Schmid. An Afﬁne Invariant Interest Point Detector. Dans Proc. 7th European

Conference on Computer Vision (ECCV), pages 128–142. Copenhagen, Denmark, 2002.

K. Mikolajczyk & C. Schmid. A Performance Evaluation of Local Descriptors.

IEEE Trans. on

Pattern Analysis and Machine Intelligence, 27(10):1615–1630, 2005.

G. Miller. Isomorphism Testing for Graphs of Bounded Genus. Dans Proc. 12th Ann. ACM Sympo-

sium on Theory of Computing, pages 225–235. 1980.

P. Moscato. On Evolution, Optimization, Genetic Algorithms and Martial Arts: Towards Memetic
Algorithms. Tech. Report. Caltech Concurrent Computation Program 826, California Institute of
Technology, Pasadena, CA, 1989.

T. Motzkin & E. Straus. Maxima for Graphs and a new Proof of a Theorem of Turán. Canadian

Journal of Mathematics, 17:533–540, 1965.

R. Myers & E. R. Hancock. Least-Commitment Graph Matching with Genetic Algorithms. Pattern

Recognition, 34(2):375–394, 2001.

R. O. Myers. Genetic Algorithms for Ambiguous Labelling Problems. Thèse de doctorat, Department

of Computer Science, University of York, 1999.

Y. Nagata & S. Kobayashi. Edge Assembly Crossover: A High-Power Genetic Algorithm for the
Traveling Salesman Problem. Dans T. Bäck, Éd., Proc. 7th Int. Conf. on GAs, pages 450–457.
Morgan Kaufman, 1997.

182

BIBLIOGRAPHIE

V. Nannen & A. Eiben. Relevance Estimation and Value Calibration of Evolutionary Algorithm

Parameters. Dans IJCAI’07, pages 975–980. 2007.

S. Nene, S. Nayar, & H. Murase. Columbia Object Image Library (COIL-100). Rapport technique

CUCS-006-96, Columbia University, 1996.

A. Neubauer. Theory of the Simple Genetic Algorithm with α-Selection. Dans Genetic and Evolu-

tionary Computation Conference, pages 1009–1016. ACM, 2008.

M. Neuhaus & H. Bunke. Automatic Learning of Cost Functions for Graph Edit Distance. Informa-

tion Sciences, 177(1):239–247, 2007.

C.-W. Ngo, Y.-F. Ma, & H.-J. Zhang. Video Summarization and Scene Detection by Graph Mode-

ling. IEEE Trans. on Circuits and Systems for Video Technology, 15(2):296–305, 2005.

H. D. Nguyen, I. Yoshihara, K. Yamamori, & M. Yasunaga. Implementation of an Effective Hy-
IEEE Trans. on Systems, Man, and

brid GA for Large-Scale Traveling Salesman Problems.
Cybernetics-Part B, 37(1):92–99, 2007.

M. Nowostawski & R. Poli. Parallel Genetic Algorithm Taxonomy. Dans KES. 1999.

I. Oliver, D. Smith, & J. Holland. A Study of Permutation Crossover Operators on the Traveling
Salesman Problem. Dans Proc. of the 2nd Int. Conf. on Genetic Algorithms, pages 224–230.
Mahwah, NJ, USA, 1987.

J.-F. Omhover & M. Detyniecki. Fast Gradual Matching Measure for Image Retrieval based on

Visual Similarity and Spatial Relations. Int. J. Intell. Syst., 21(7):711–723, 2006.

Y. Ong, M. Lim, N. Zhu, & K. Wong. Classiﬁcation of Adaptive Memetic Algorithms: A Com-
parative Study. IEEE Transactions on Systems, Man, and Cybernetics, Part B, 36(1):141–152,
2006.

M. Pelillo. Replicator Equations, Maximal Cliques, and Graph Isomorphism. Neural Computation,

11:1933–1955, 1999.

R. C. Read & D. G. Corneil. The Graph Isomorphism Disease. Journal of Graph Theory, 1:339–363,

1977.

I. Rechenberg. Evolutionsstrategie : Optimierung technischer Systeme nach Prinzipien der biologi-

schen Evolution. Frommann-Holzboog, Stuttgart-Bad Cannstadt, Germany, 1973.

C. Reeves & J. E. Rowe. Genetic Algorithms - Principles and Perspectives, A Guide to GA Theory.

Kluwer Academic Publishers, 2003.

183

J.-M. Renders & S. P. Flasse. Hybrid Methods using Genetic Algorithms for Global Optimization.

IEEE Transactions on Systems, Man, and Cybernetics, Part B, 26(2):243–258, 1996.

R. Reynolds. An Introduction to Cultural Algorithms. Dans 3rd Conf. on Evolutionary Program-

ming. 1994.

K. Riesen & H. Bunke. IAM Graph Database Repository for Graph based Pattern Recognition and
Machine Learning. Dans Structural, Syntactic, and Statistical Pattern Recognition, vol. 5342 de
LNCS, pages 287–297. Springer, 2008.

A. Rogers & A. Prügel-Bennett. Genetic Drift in Genetic Algorithm Selection Schemes.

IEEE

Trans. on Evolutionary Computation, 3(4):298–303, 1999.

J. E. Rowe, M. D. Vose, & A. H. Wright. Reinterpreting No Free Lunch. Evolutionary Computation,

17(1), 2008.

G. Rudolph. Convergence Analysis of Canonical Genetic Algorithms.

IEEE Trans. on Neural

Networks, 5(1):96–101, 1994.

G. Rudolph. Convergence Analysis of Canonical Genetic Algorithms. Dans Proc. IEEE Conf. on

Evolutionary Computation. 1996.

O. Sammoud, S. Sorlin, C. Solnon, & K. Ghédira. A Comparative Study of Ant Colony Optimiza-
tion and Reactive Search for Graph Matching Problems. Dans J. Gottlieb & G. R. Raidl, Éds.,
EvoCOP, vol. 3906 de LNCS, pages 234–246. Springer, Budapest, Hungary, 2006.

A. Sanfeliu & K.-S. Fu. A Distance Measure between Attributed Relational Graphs for Pattern

Recognition. IEEE Transactions on Systems, Man, and Cybernetics, 13(3):353–362, 1983.

H.-P. Schwefel. Numerical Optimization of Computer Models. Wiley, 1981.

J. Shapiro. Theoretical Aspects of Evolutionary Computation, chapitre Statistical mechanics theory

of genetic algorithms, pages 87–108. Springer, 2001.

J. Shi & J. Malik. Normalized Cuts and Image Segmentation. IEEE Trans. on Pattern Analysis and

Machine Intelligence, 22(8):888–905, 2000.

A. Shokoufandeh, D. Macrini, S. J. Dickinson, K. Siddiqi, & S. W. Zucker. Indexing Hierarchical
Structures using Graph Spectra. IEEE Transactions on Pattern Analysis and Machine Intelligence,
27(7):1125–1140, 2005.

M. Singh, A. Chatterjee, & S. Chaudhury. Matching Structural Shape Descriptions using Genetic

Algorithms. Pattern Recognition, 30(9):1451–1462, 1997.

184

BIBLIOGRAPHIE

S. Smit & A. Eiben. Comparing Parameter Tuning Methods for Evolutionary Algorithms. Dans

IEEE Congress on Evolutionary Computation (CEC). 2009.

J. E. Smith. Coevolving Memetic Algorithms: A Review and Progress Report.

IEEE Trans. on

Systems, Man, and Cybernetics-Part B, 37(1):6–17, 2007.

A. J. Smola & R. Kondor. Kernels and Regularization on Graphs. Dans B. Schölkopf & M. K.
Warmuth, Éds., Learning Theory and Kernel Machines, vol. 2777 de Lecture Notes in Artiﬁcial
Intelligence, pages 144–158. Springer, 2003.

S. Sorlin, C. Solnon, & J.-M. Jolion. A Generic Graph Distance Measure Based on Multivalent
Matchings. Dans A. Kandel, H. Bunke, & M. Last, Éds., Applied Graph Theory in Computer
Vision and Pattern Recognition, vol. 52 de Studies in Computational Intelligence, chapitre A
Generic Graph Distance Mesasure Based on Multivalent Matchings, pages 151–181. Springer,
2007.

T. Starkweather, S. McDaniel, K. Mathias, D. Whitley, & C. Whitley. A Comparison of Genetic Se-
quencing Operators. Dans R. Belew & L. Booker, Éds., Proc. of the 4th International Conference
on Genetic Algorithms, pages 69–76. Morgan Kaufman, San Mateo, CA, 1991.

C. R. Stephens & J. Cervantes.

Just What are Building Blocks? Dans Foundations of Genetic

Algorithms, vol. 4436 de LNCS, pages 15–34. Springer, 2007.

M. J. Streeter. Two Broad Classes of Functions for which a No Free Lunch Result does not Hold.
Dans Genetic and Evolutionary Computation - GECCO, vol. 2724 de LNCS, pages 1418–1430.
2003.

P. N. Suganthan. Structural Pattern Recognition using Genetic Algorithms. Pattern Recognition,

35(9):1883–1893, 2002.

G. Syswerda. Schedule Optimization using Genetic Algorithms. Dans L. Davis, Éd., Handbook of

Genetic Algorithms, pages 332–349. Van Nostrand Reinhold, New York, 1991.

N. Thome, D. Merad, & S. Miguet. Learning Articulated Appearance Models for Tracking Humans:
A Spectral Graph Matching Approach. Signal Processing: Image Communication, 23(10):769–
787, 2008. doi:doi:10.1016/j.image.2008.09.003.

J. Torán. On the Hardness of Graph Isomorphism. SIAM Journal on Computing, 33(5):1093–1108,

2004.

R. Torres-Velázquez & V. Estivill-Castro. A Memetic Algorithm Guided by Quicksort for the Error-
Correcting Graph Isomorphism Problem. Dans S. Cagnoni, Éd., EvoWorkshops, pages 173–182.
2002.

185

W. Tsai & K.-S. Fu. Error-Correcting Isomorphism of Attributed Relational Graphs for Pattern

Analysis. IEEE Transactions on Systems, Man, and Cybernetics, 9:757–768, 1979.

J. W. Tukey. Exploratory Data Analysis. Addison Wesley, 1977.

J. R. Ullmann. An Algorithm for Subgraph Isomorphism. J. ACM, 23(1):31–42, 1976. ISSN 0004-

5411. doi:http://doi.acm.org/10.1145/321921.321925.

S. Umeyama. An Eigendecomposition Approach to Weighted Graph Matching Problems.

IEEE

Transactions on Pattern Analysis and Machine Intelligence, 10(5):695–703, 1988.

P. F. Velleman & D. C. Hoaglin. Applications, Basics, and Computing of Exploratory Data Analysis.

Duxbury Press, 1981.

M. D. Vose. The Simple Genetic Algorithm: Foundations and Theory. MIT Press, Cambridge,

Massachusetts, 1999.

Y.-K. Wang, K.-C. Fan, & J.-T. Horng. Genetic-Based Search for Error-Correcting Graph Isomor-

phism. IEEE Transactions on Systems, Man, and Cybernetics, 27(4):588–597, 1997.

R. C. Wilson & E. R. Hancock. Structural Matching by Discrete Relaxation. IEEE Trans. on Pattern

Analysis and Machine Intelligence, 19:634–648, 1997.

D. H. Wolpert & W. G. Macready. No Free Lunch Theorems for Search. Rapport technique SFI-

TR-95-02-010, The Santa Fe Institute, 1399 Hyde Park Road, Santa Fe, NM, USA, 1996.

D. H. Wolpert & W. G. Macready. No Free Lunch Theorems for Optimization. IEEE Transactions

on Evolutionary Computation, 1(1):67–82, 1997.

D. H. Wolpert & W. G. Macready. Coevolutionary Free Lunches.

IEEE Trans. on Evolutionary

Computation, 9(6):721–735, 2005.

B. Yuan & M. Gallagher. Statistical Racing Techniques for Improved Empirical Evaluation of Evo-

lutionary Algorithms. Dans Parallel Problem Solving from Nature - PPSN. 2004.

S. Zampelli, Y. Deville, C. Solnon, & P. Dupont. Filtering for Subgraph Isomorphism. Dans Int.
Conf. on Principles and Practice of Constraint Programming, vol. 4741 de LNCS, pages 728–742.
Springer, 2007.

186

BIBLIOGRAPHIE

Annexes

187

Annexe A

Sensibilité au bruit des paramètres

A.1 Moteur générationnel

A.1.1 Bruit uniforme

189

190

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

GGA - CX - 40 sommets - Croisement 0.99 - Mutation 0.6

s
è
c
c
u
s

e
d

x
u
a
T

s
è
c
c
u
s

e
d

x
u
a
T

1

0.9

0.8

0.7

0.6

0.5

0.4

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
10

20

30

40

50

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - CX - 40 sommets - Population 400 - Mutation 0.6

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

95

96

97

98

0
99

60

70

80

85

90

Probabilité de croisement en pourcentage

GGA - CX - 40 sommets - Population 400 - Croisement 0.99

×105
5

 

4

3

2

1

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

10

9
20
Probabilité de mutation en pourcentage

13 14

11

15

12

4

3

2

1

25

30

40

50

60

0
70

FIGURE A.1 – Sensibilité au bruit uniforme des paramètres du CX dans un moteur générationnel.

A.1. MOTEURGÉNÉRATIONNEL

191

GGA - PMX - 40 sommets - Croisement 0.99 - Mutation 0.4

s
è
c
c
u
s

e
d

x
u
a
T

s
è
c
c
u
s

e
d

x
u
a
T

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
10

20

30

40

50

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - PMX - 40 sommets - Population 120 - Mutation 0.4

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

95

96

97

98

0
99

60

70

80

85

90

Probabilité de croisement en pourcentage

GGA - PMX - 40 sommets - Population 120 - Croisement 0.99

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

10

9
20
Probabilité de mutation en pourcentage

13 14

12

11

15

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

4

3

2

1

25

30

40

50

60

0
70

FIGURE A.2 – Sensibilité au bruit uniforme des paramètres du PMX dans un moteur générationnel.

192

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

GGA - UPMX - 40 sommets - Croisement 0.8 - Mutation 0.12

4

3

2

1

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - UPMX - 40 sommets - Population 90 - Mutation 0.12

s
è
c
c
u
s

e
d

x
u
a
T

s
è
c
c
u
s

e
d

x
u
a
T

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
1 1.5 2 2.5 3

4

5

6

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
10

20

30

40

50

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

4

3

2

1

95

96

97

98

0
99

60

70

80

85

90

Probabilité de croisement en pourcentage

GGA - UPMX - 40 sommets - Population 90 - Croisement 0.8

4

3

2

1

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

11

15

12

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

FIGURE A.3 – Sensibilité au bruit uniforme des paramètres du UPMX dans un moteur générationnel.

A.1. MOTEURGÉNÉRATIONNEL

193

GGA - PBX - 40 sommets - Croisement 0.96 - Mutation 0.3

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - PBX - 40 sommets - Population 120 - Mutation 0.3

s
è
c
c
u
s

e
d

x
u
a
T

s
è
c
c
u
s

e
d

x
u
a
T

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
10

20

30

40

50

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

95

96

97

98

0
99

60

70

80

85

90

Probabilité de croisement en pourcentage

GGA - PBX - 40 sommets - Population 120 - Croisement 0.96

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

10

9
20
Probabilité de mutation en pourcentage

13 14

11

12

15

25

30

40

50

60

0
70

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

FIGURE A.4 – Sensibilité au bruit uniforme des paramètres du PBX dans un moteur générationnel.

194

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

GGA - UPBX - 40 sommets - Croisement 0.99 - Mutation 0.2

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - UPBX - 40 sommets - Population 90 - Mutation 0.2

s
è
c
c
u
s

e
d

x
u
a
T

s
è
c
c
u
s

e
d

x
u
a
T

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
10

20

30

40

50

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

95

96

97

98

0
99

60

70

80

85

90

Probabilité de croisement en pourcentage

GGA - UPBX - 40 sommets - Population 90 - Croisement 0.99

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

10

9
20
Probabilité de mutation en pourcentage

13 14

11

15

12

25

30

40

50

60

0
70

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

FIGURE A.5 – Sensibilité au bruit uniforme des paramètres du UPBX dans un moteur générationnel.

A.1. MOTEURGÉNÉRATIONNEL

195

GGA - UOX - 40 sommets - Population 10 - Mutation 0.14

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

95

96

97

98

0
99

60

70

80

85

90

Probabilité de croisement en pourcentage

GGA - UOX - 40 sommets - Population 10 - Croisement 0.6

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
10

20

30

40

50

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

11

12

15

FIGURE A.6 – Sensibilité au bruit uniforme des paramètres du UOX dans un moteur générationnel.

196

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

GGA - DPX - 40 sommets - Croisement 0.25 - Mutation 0.25

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - DPX - 40 sommets - Population 50 - Croisement 0.25

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

10

11

9
20
Probabilité de mutation en pourcentage

13 14

12

15

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

25

30

40

50

60

0
70

FIGURE A.7 – Sensibilité au bruit uniforme des paramètres du DPX dans un moteur générationnel.

A.1. MOTEURGÉNÉRATIONNEL

197

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

GGA - SDPX - 40 sommets - Croisement 0.35 - Mutation 0.3

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - SDPX - 40 sommets - Population 25 - Croisement 0.35

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

10

11

9
20
Probabilité de mutation en pourcentage

13 14

12

15

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

25

30

40

50

60

0
70

FIGURE A.8 – Sensibilité au bruit uniforme des paramètres du SDPX dans un moteur générationnel.

198

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

GGA - NDPX - 40 sommets - Croisement 0.4 - Mutation 0.3

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - NDPX - 40 sommets - Population 25 - Croisement 0.4

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

25

30

40

50

60

0
70

10

11

9
20
Probabilité de mutation en pourcentage

13 14

12

15

FIGURE A.9 – Sensibilité au bruit uniforme des paramètres du NDPX dans un moteur générationnel.

A.1. MOTEURGÉNÉRATIONNEL

199

A.1.2 Bruit gaussien

200

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

GGA - CX - 40 sommets - Croisement 0.99 - Mutation 0.6

s
è
c
c
u
s

e
d

x
u
a
T

s
è
c
c
u
s

e
d

x
u
a
T

1

0.9

0.8

0.7

0.6

0.5

0.4

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
10

20

30

40

50

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

×105
5

 

4

3

2

1

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - CX - 40 sommets - Population 400 - Mutation 0.6

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

95

96

97

98

0
99

60

70

80

85

90

Probabilité de croisement en pourcentage

GGA - CX - 40 sommets - Population 400 - Croisement 0.99

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

11

15

12

FIGURE A.10 – Sensibilité au bruit gaussien des paramètres du CX dans un moteur générationnel.

A.1. MOTEURGÉNÉRATIONNEL

201

GGA - PMX - 40 sommets - Croisement 0.99 - Mutation 0.4

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - PMX - 40 sommets - Population 120 - Mutation 0.4

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

95

96

97

98

0
99

60

70

80

85

90

Probabilité de croisement en pourcentage

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

×105
5

 

4

3

2

1

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

s
è
c
c
u
s

e
d

x
u
a
T

s
è
c
c
u
s

e
d

x
u
a
T

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
10

20

30

40

50

GGA - PMX - 40 sommets - Population 120 - Croisement 0.99
Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

10

9
20
Probabilité de mutation en pourcentage

13 14

12

11

15

25

30

40

50

60

0
70

FIGURE A.11 – Sensibilité au bruit gaussien des paramètres du PMX dans un moteur générationnel.

202

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

GGA - UPMX - 40 sommets - Croisement 0.8 - Mutation 0.12

4

3

2

1

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - UPMX - 40 sommets - Population 90 - Mutation 0.12

s
è
c
c
u
s

e
d

x
u
a
T

s
è
c
c
u
s

e
d

x
u
a
T

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
10

20

30

40

50

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

4

3

2

1

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

95

96

97

98

0
99

60

70

80

85

90

Probabilité de croisement en pourcentage

GGA - UPMX - 40 sommets - Population 90 - Croisement 0.8

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

10

9
20
Probabilité de mutation en pourcentage

13 14

11

12

15

4

3

2

1

25

30

40

50

60

0
70

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

FIGURE A.12 – Sensibilité au bruit gaussien des paramètres du UPMX dans un moteur génération-
nel.

A.1. MOTEURGÉNÉRATIONNEL

203

GGA - PBX - 40 sommets - Croisement 0.96 - Mutation 0.3

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - PBX - 40 sommets - Population 120 - Mutation 0.3

s
è
c
c
u
s

e
d

x
u
a
T

s
è
c
c
u
s

e
d

x
u
a
T

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
10

20

30

40

50

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

95

96

97

98

0
99

60

70

80

85

90

Probabilité de croisement en pourcentage

GGA - PBX - 40 sommets - Population 120 - Croisement 0.96

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

10

9
20
Probabilité de mutation en pourcentage

13 14

12

11

15

25

30

40

50

60

0
70

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

FIGURE A.13 – Sensibilité au bruit gaussien des paramètres du PBX dans un moteur générationnel.

204

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

GGA - UPBX - 40 sommets - Croisement 0.99 - Mutation 0.2

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - UPBX - 40 sommets - Population 90 - Mutation 0.2

s
è
c
c
u
s

e
d

x
u
a
T

s
è
c
c
u
s

e
d

x
u
a
T

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

 
10

20

30

40

50

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

95

96

97

98

0
99

60

70

80

85

90

Probabilité de croisement en pourcentage

GGA - UPBX - 40 sommets - Population 90 - Croisement 0.99

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

10

9
20
Probabilité de mutation en pourcentage

13 14

11

12

15

25

30

40

50

60

0
70

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

FIGURE A.14 – Sensibilité au bruit gaussien des paramètres du UPBX dans un moteur générationnel.

A.1. MOTEURGÉNÉRATIONNEL

205

GGA - UOX - 40 sommets - Population 10 - Mutation 0.14

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

95

96

97

98

0
99

60

70

80

85

90

Probabilité de croisement en pourcentage

GGA - UOX - 40 sommets - Population 10 - Croisement 0.6

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
10

20

30

40

50

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

11

12

15

FIGURE A.15 – Sensibilité au bruit gaussien des paramètres du UOX dans un moteur générationnel.

206

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

GGA - DPX - 40 sommets - Croisement 0.25 - Mutation 0.25

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

0.3

 
1 1.5 2 2.5 3

4

5

6

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - DPX - 40 sommets - Population 50 - Croisement 0.25

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

10

11

9
20
Probabilité de mutation en pourcentage

13 14

12

15

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

25

30

40

50

60

0
70

FIGURE A.16 – Sensibilité au bruit gaussien des paramètres du DPX dans un moteur générationnel.

A.1. MOTEURGÉNÉRATIONNEL

207

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

GGA - SDPX - 40 sommets - Croisement 0.35 - Mutation 0.3

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - SDPX - 40 sommets - Population 25 - Croisement 0.35

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

10

11

12

9
20
Probabilité de mutation en pourcentage

13 14

15

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

25

30

40

50

60

0
70

FIGURE A.17 – Sensibilité au bruit gaussien des paramètres du SDPX dans un moteur générationnel.

208

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

GGA - NDPX - 40 sommets - Croisement 0.4 - Mutation 0.3

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

GGA - NDPX - 40 sommets - Population 25 - Croisement 0.4

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

25

30

40

50

60

0
70

10

11

9
20
Probabilité de mutation en pourcentage

13 14

12

15

FIGURE A.18 – Sensibilité au bruit gaussien des paramètres du NDPX dans un moteur générationnel.

A.2. MOTEUR STEADY-STATE

A.2 Moteur steady-state

A.2.1 Bruit uniforme

SSGA - CX - 40 sommets - Mutation 0.6

209

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

SSGA - CX - 40 sommets - Population 700

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

11

15

12

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

FIGURE A.19 – Sensibilité au bruit uniforme des paramètres du CX dans un moteur steady-state.

210

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

SSGA - PMX - 40 sommets - Mutation 0.7

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

SSGA - PMX - 40 sommets - Population 1000

×105
5

 

×105
5

 

4

3

2

1

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

4

3

2

1

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

11

15

12

FIGURE A.20 – Sensibilité au bruit uniforme des paramètres du PMX dans un moteur steady-state.

A.2. MOTEUR STEADY-STATE

211

SSGA - UPMX - 40 sommets - Mutation 0.06

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

SSGA - UPMX - 40 sommets - Population 400

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

12

11

15

FIGURE A.21 – Sensibilité au bruit uniforme des paramètres du UPMX dans un moteur steady-state.

212

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

SSGA - PBX - 40 sommets - Mutation 0.3

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

SSGA - PBX - 40 sommets - Population 300

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

11

12

15

FIGURE A.22 – Sensibilité au bruit uniforme des paramètres du PBX dans un moteur steady-state.

A.2. MOTEUR STEADY-STATE

213

SSGA - UPBX - 40 sommets - Mutation 0.06

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

SSGA - UPBX - 40 sommets - Population 400

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

4

3

2

1

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

12

11

15

FIGURE A.23 – Sensibilité au bruit uniforme des paramètres du UPBX dans un moteur steady-state.

214

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

SSGA - UOX - 40 sommets - Mutation 0.07

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

SSGA - UOX - 40 sommets - Population 400

×105
5

 

×105
5

 

4

3

2

1

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

4

3

2

1

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

11

12

15

FIGURE A.24 – Sensibilité au bruit uniforme des paramètres du UOX dans un moteur steady-state.

A.2. MOTEUR STEADY-STATE

215

SSGA - DPX - 40 sommets - Mutation 0.25

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

0.3

 
1 1.5 2 2.5 3

4

5

6

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

SSGA - DPX - 40 sommets - Population 20

10

11

12

9
20
Probabilité de mutation en pourcentage

13 14

15

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

25

30

40

50

60

0
70

FIGURE A.25 – Sensibilité au bruit uniforme des paramètres du DPX dans un moteur steady-state.

216

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

SSGA - SDPX - 40 sommets - Mutation 0.25

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

0.3

 
1 1.5 2 2.5 3

4

5

6

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

SSGA - SDPX - 40 sommets - Population 15

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

25

30

40

50

60

0
70

10

11

9
20
Probabilité de mutation en pourcentage

13 14

12

15

FIGURE A.26 – Sensibilité au bruit uniforme des paramètres du SDPX dans un moteur steady-state.

A.2. MOTEUR STEADY-STATE

217

SSGA - NDPX - 40 sommets - Mutation 0.25

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

SSGA - NDPX - 40 sommets - Population 20

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

Bruit unif. 0
Bruit unif. 2
Bruit unif. 4
Bruit unif. 6
Bruit unif. 8
Bruit unif. 10

25

30

40

50

60

0
70

10

11

12

9
20
Probabilité de mutation en pourcentage

13 14

15

FIGURE A.27 – Sensibilité au bruit uniforme des paramètres du NDPX dans un moteur steady-state.

218

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

A.2.2 Bruit gaussien

SSGA - CX - 40 sommets - Mutation 0.6

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

SSGA - CX - 40 sommets - Population 700

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

12

11

15

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

FIGURE A.28 – Sensibilité au bruit gaussien des paramètres du CX dans un moteur steady-state.

A.2. MOTEUR STEADY-STATE

219

SSGA - PMX - 40 sommets - Mutation 0.7

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

SSGA - PMX - 40 sommets - Population 1000

×105
5

 

×105
5

 

4

3

2

1

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

4

3

2

1

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

12

11

15

FIGURE A.29 – Sensibilité au bruit gaussien des paramètres du PMX dans un moteur steady-state.

220

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

SSGA - UPMX - 40 sommets - Mutation 0.06

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

SSGA - UPMX - 40 sommets - Population 400

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

11

12

15

FIGURE A.30 – Sensibilité au bruit gaussien des paramètres du UPMX dans un moteur steady-state.

A.2. MOTEUR STEADY-STATE

221

SSGA - PBX - 40 sommets - Mutation 0.3

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

SSGA - PBX - 40 sommets - Population 300

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

11

12

15

FIGURE A.31 – Sensibilité au bruit gaussien des paramètres du PBX dans un moteur steady-state.

222

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

SSGA - UPBX - 40 sommets - Mutation 0.06

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

SSGA - UPBX - 40 sommets - Population 400

×105
5

 

×105
5

 

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

4

3

2

1

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

11

15

12

FIGURE A.32 – Sensibilité au bruit gaussien des paramètres du UPBX dans un moteur steady-state.

A.2. MOTEUR STEADY-STATE

223

SSGA - UOX - 40 sommets - Mutation 0.07

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

SSGA - UOX - 40 sommets - Population 400

×105
5

 

×105
5

 

4

3

2

1

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

4

3

2

1

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

25

30

40

50

60

0
70

10

9
20
Probabilité de mutation en pourcentage

13 14

11

12

15

FIGURE A.33 – Sensibilité au bruit gaussien des paramètres du UOX dans un moteur steady-state.

224

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

SSGA - DPX - 40 sommets - Mutation 0.25

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

0.3

 
1 1.5 2 2.5 3

4

5

6

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

SSGA - DPX - 40 sommets - Population 20

10

11

9
20
Probabilité de mutation en pourcentage

13 14

12

15

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

25

30

40

50

60

0
70

FIGURE A.34 – Sensibilité au bruit gaussien des paramètres du DPX dans un moteur steady-state.

A.2. MOTEUR STEADY-STATE

225

SSGA - SDPX - 40 sommets - Mutation 0.25

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
1 1.5 2 2.5 3

4

5

6

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

SSGA - SDPX - 40 sommets - Population 15

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

25

30

40

50

60

0
70

10

11

12

9
20
Probabilité de mutation en pourcentage

13 14

15

FIGURE A.35 – Sensibilité au bruit gaussien des paramètres du SDPX dans un moteur steady-state.

226

ANNEXEA. SENSIBILITÉAUBRUITDESPARAMÈTRES

SSGA - NDPX - 40 sommets - Mutation 0.25

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

0.3

 
1 1.5 2 2.5 3

4

5

6

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

7

8

0
9 10 12 14 16 18 20 25 30 40 50 60 70 100

Taille de la population (en dizaines)

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
0.1

1

5

6

7

8

SSGA - NDPX - 40 sommets - Population 20

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d

s
p
m
e
T

Bruit gauss. 0
Bruit gauss. 2
Bruit gauss. 4
Bruit gauss. 6
Bruit gauss. 8
Bruit gauss. 10

25

30

40

50

60

0
70

10

11

9
20
Probabilité de mutation en pourcentage

13 14

12

15

FIGURE A.36 – Sensibilité au bruit gaussien des paramètres du NDPX dans un moteur steady-state.

Annexe B

Comparaison d’opérateurs

227

228

ANNEXEB. COMPARAISOND’OPÉRATEURS

Moteur générationnel - paramètres optimaux - 40 sommets - bruit unif. 6

s
è
c
c
u
s

e
d

x
u
a
T

1

0.8

0.6

0.4

0.2

0

 
0

0.2

0.4

0.6

 

CX
PMX
UPMX
PBX
UPBX
UOX
DPX
SDPX
NDPX

1.4

1.6

1.8

2

0.8

1

1.2

Temps de calcul en secondes

FIGURE B.1 – Comparaison des opérateurs avec un moteur générationnel en fonction du temps de
calcul.

Moteur générationnel - paramètres optimaux - 40 sommets - bruit unif. 6

1

0.8

0.6

0.4

0.2

s
è
c
c
u
s

e
d

x
u
a
T

0

 
0

0.5

1

1.5
3.5
Nombre d’appels à la fonction d’évaluation

2.5

2

3

 

CX
PMX
UPMX
PBX
UPBX
UOX
DPX
SDPX
NDPX

4

4.5

5
×105

FIGURE B.2 – Comparaison des opérateurs avec un moteur générationnel en fonction du nombre
d’appels à la fonction d’évaluation.

229

Moteur steady-state - paramètres optimaux - 40 sommets - bruit unif. 6

s
è
c
c
u
s

e
d
x
u
a
T

1

0.8

0.6

0.4

0.2

0

 
0

0.2

0.4

0.6

 

CX
PMX
UPMX
PBX
UPBX
UOX
DPX
SDPX
NDPX

1.4

1.6

1.8

2

0.8

1

1.2

Temps de calcul en secondes

FIGURE B.3 – Comparaison des opérateurs avec un moteur steady-state en fonction du temps de
calcul.

Moteur steady-state - paramètres optimaux - 40 sommets - bruit unif. 6

1

0.8

0.6

0.4

0.2

s
è
c
c
u
s

e
d
x
u
a
T

0

 
0

0.5

1

1.5
3.5
Nombre d’appels à la fonction d’évaluation

2.5

2

3

 

CX
PMX
UPMX
PBX
UPBX
UOX
DPX
SDPX
NDPX

4

4.5

5
×105

FIGURE B.4 – Comparaison des opérateurs avec un moteur steady-state en fonction du nombre
d’appels à la fonction d’évaluation.

230

ANNEXEB. COMPARAISOND’OPÉRATEURS

Annexe C

Mutation par brassage

C.1 Moteur générationnel

231

232

ANNEXEC. MUTATIONPARBRASSAGE

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
5

7.5

10

GGA - PMX - Bruit unif. 6 - Pop. 120 - Croisement 0.99 - Mut. 0.4

20 sommets
30 sommets
40 sommets

12.5

15

32.5
Probabilité de mutation des gènes individuels en pourcentage

17.5

27.5

22.5

25

20

30

×105
5

 

4

3

2

1

35

37.5

0
40

FIGURE C.1 – Performance du PMX en fonction du paramètre de la mutation par brassage.

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
5

7.5

10

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
5

7.5

10

GGA - UPMX - Bruit unif. 6 - Pop. 90 - Croisement 0.8 - Mut. 0.12

×105
5

 

20 sommets
30 sommets
40 sommets

4

3

2

1

35

37.5

0
40

12.5

15

32.5
Probabilité de mutation des gènes individuels en pourcentage

27.5

22.5

17.5

25

20

30

GGA - PBX - Bruit unif. 6 - Pop. 120 - Croisement 0.96 - Mut. 0.3

×105
5

 

20 sommets
30 sommets
40 sommets

4

3

2

1

35

37.5

0
40

12.5

15

32.5
Probabilité de mutation des gènes individuels en pourcentage

17.5

27.5

22.5

25

20

30

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

FIGURE C.2 – Performance du UPMX en fonction du paramètre de la mutation par brassage.

FIGURE C.3 – Performance du PBX en fonction du paramètre de la mutation par brassage.

replacements

C.1. MOTEURGÉNÉRATIONNEL

233

GGA - UPBX - Bruit unif. 6 - Pop. 90 - Croisement 0.99 - Mut. 0.2

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d
x
u
a
T

20 sommets
30 sommets
40 sommets

0.3

 
5

7.5

10

12.5

15

32.5
Probabilité de mutation des gènes individuels en pourcentage

17.5

27.5

22.5

20

25

30

×105
5

 

4

3

2

1

35

37.5

0
40

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

FIGURE C.4 – Performance du UPBX en fonction du paramètre de la mutation par brassage.

GGA - DPX - Bruit unif. 6 - Pop. 50 - Croisement 0.25 - Mut. 0.25

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d
x
u
a
T

0.3

 
5

7.5

10

12.5

15

32.5
Probabilité de mutation des gènes individuels en pourcentage

17.5

22.5

27.5

20

25

30

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d
s
p
m
e
T

20 sommets
30 sommets
40 sommets

35

37.5

0
40

FIGURE C.5 – Performance du DPX en fonction du paramètre de la mutation par brassage.

GGA - SDPX - Bruit unif. 6 - Pop. 25 - Croisement 0.35 - Mut. 0.3

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d
x
u
a
T

0.3

 
5

7.5

10

20 sommets
30 sommets
40 sommets

12.5

15

32.5
Probabilité de mutation des gènes individuels en pourcentage

27.5

22.5

17.5

25

20

30

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d
s
p
m
e
T

35

37.5

0
40

FIGURE C.6 – Performance du SDPX en fonction du paramètre de la mutation par brassage.

234

ANNEXEC. MUTATIONPARBRASSAGE

GGA - NDPX - Bruit unif. 6 - Pop. 25 - Croisement 0.4 - Mut. 0.3

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
5

7.5

10

20 sommets
30 sommets
40 sommets

12.5

15

32.5
Probabilité de mutation des gènes individuels en pourcentage

27.5

17.5

20

22.5

25

30

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d
s
p
m
e
T

35

37.5

0
40

FIGURE C.7 – Performance du NDPX en fonction du paramètre de la mutation par brassage.

C.2 Moteur steady-state

C.2. MOTEUR STEADY-STATE

235

SSGA - CX - Bruit unif. 6 - Pop. 700

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d
x
u
a
T

20 sommets
30 sommets
40 sommets

0.3

 
5

7.5

10

12.5

15

32.5
Probabilité de mutation des gènes individuels en pourcentage

17.5

27.5

22.5

20

30

25

×105
5

 

4

3

2

1

35

37.5

0
40

FIGURE C.8 – Performance du CX en fonction du paramètre de la mutation scramble.

SSGA - PMX - Bruit unif. 6 - Pop. 1000

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d
x
u
a
T

20 sommets
30 sommets
40 sommets

0.3

 
5

7.5

10

12.5

15

32.5
Probabilité de mutation des gènes individuels en pourcentage

27.5

22.5

17.5

25

30

20

×105
5

 

4

3

2

1

35

37.5

0
40

FIGURE C.9 – Performance du PMX en fonction du paramètre de la mutation scramble.

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d
x
u
a
T

0.3

 
5

7.5

10

SSGA - UPMX - Bruit unif. 6 - Pop. 400

×105
5

 

20 sommets
30 sommets
40 sommets

4

3

2

1

35

37.5

0
40

12.5

15

32.5
Probabilité de mutation des gènes individuels en pourcentage

17.5

27.5

22.5

20

25

30

FIGURE C.10 – Performance du UPMX en fonction du paramètre de la mutation scramble.

236

ANNEXEC. MUTATIONPARBRASSAGE

SSGA - PBX - Bruit unif. 6 - Pop. 300

×105
5

 

20 sommets
30 sommets
40 sommets

4

3

2

1

35

37.5

0
40

12.5

15

32.5
Probabilité de mutation des gènes individuels en pourcentage

17.5

27.5

22.5

20

25

30

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
5

7.5

10

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
5

7.5

10

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

FIGURE C.11 – Performance du PBX en fonction du paramètre de la mutation scramble.

SSGA - UPBX - Bruit unif. 6 - Pop. 400

×105
5

 

20 sommets
30 sommets
40 sommets

4

3

2

1

35

37.5

0
40

12.5

15

32.5
Probabilité de mutation des gènes individuels en pourcentage

27.5

22.5

17.5

25

30

20

FIGURE C.12 – Performance du UPBX en fonction du paramètre de la mutation scramble.

SSGA - UOX - Bruit unif. 6 - Pop. 400

×105
5

 

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d

x
u
a
T

0.3

 
5

7.5

10

20 sommets
30 sommets
40 sommets

4

3

2

1

35

37.5

0
40

12.5

15

32.5
Probabilité de mutation des gènes individuels en pourcentage

17.5

27.5

22.5

30

20

25

n
o
i
t
a
u
l
a
v
é
’
d

n
o
i
t
c
n
o
f

a
l

à

s
l
e
p
p
a
’
d
e
r
b
m
o
N

FIGURE C.13 – Performance du UOX en fonction du paramètre de la mutation scramble.

C.2. MOTEUR STEADY-STATE

237

SSGA - SDPX - Bruit unif. 6 - Pop. 15

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d
x
u
a
T

20 sommets
30 sommets
40 sommets

0.3

 
5

7.5

10

12.5

15

32.5
Probabilité de mutation des gènes individuels en pourcentage

27.5

22.5

17.5

20

30

25

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d
s
p
m
e
T

35

37.5

0
40

FIGURE C.14 – Performance du SDPX en fonction du paramètre de la mutation scramble.

SSGA - NDPX - Bruit unif. 6 - Pop. 20

1

0.9

0.8

0.7

0.6

0.5

0.4

s
è
c
c
u
s

e
d
x
u
a
T

20 sommets
30 sommets
40 sommets

0.3

 
5

7.5

10

12.5

15

32.5
Probabilité de mutation des gènes individuels en pourcentage

22.5

17.5

27.5

25

30

20

 

1

0.8

0.6

0.4

0.2

s
e
d
n
o
c
e
s

n
e

l
u
c
l
a
c

e
d
s
p
m
e
T

35

37.5

0
40

FIGURE C.15 – Performance du NDPX en fonction du paramètre de la mutation scramble.

238

ANNEXEC. MUTATIONPARBRASSAGE

Annexe D

Analyse de la variance (Kruskal-Wallis)

L’analyse de la variance permet d’identiﬁer si les résultats observés d’un processus stochastique
sont signiﬁcatifs. Etant donné deux ou plusieurs échantillons, l’hypothèse H0 est qu’ils sont iden-
tiques. On calcule la valeur p qui représente la probabilité que l’on obtienne un résultat au moins
aussi extrême que la valeur observée sous l’hypothèse H0. Si cette probabilité est faible, alors le
résultat est statistiquement signiﬁcatif. Il s’agit donc très probablement de différences réelles entre
les échantillons et non des effets du hasard.

Le test de Kruskal-Wallis est une méthode classique sans paramètre et basée sur l’ordonnance-
ment, qui permet de comparer plus de deux échantillons en même temps. De plus, il ne présuppose
pas une distribution gaussienne des données. En général, on choisit 1% ou 5% comme seuil de
signiﬁance. Ce choix est arbitraire mais largement répandu.

Nous effectuons le test de Kruskal-Wallis pour déterminer si les différences observées sont
réelles. En général, nos échantillons sont décrits par deux valeurs et non par une seule. Il s’agit
d’une part, du taux de succès et d’autre part du temps de calcul jusqu’à l’obtention de la solution.
Nous utilisons alors deux p-valeurs, pSR pour le taux de réussite (success rate) et pAES (average
evaluations to success) pour le nombre d’appels à la fonction d’évaluation nécessaires pour atteindre
la solution, respectivement pARS (average runtime to success) si l’on mesure le temps de calcul (no-
tamment pour les opérateurs qui incluent une heuristique gloutonne). Si la valeur de p n’est pas
déﬁnie (ceci se produit notamment dans le cas où il n’y a pas de variance dans les échantillons),
nous le signalons par un tiret dans les tableaux. Par exemple si l’algorithme ne converge jamais ou
s’il converge toujours vers la bonne solution. Nous considérons les résultats comme signiﬁcatifs, si
l’une des deux valeurs p est inférieure au seuil. Si, par exemple, les différences du taux de succès
sont signiﬁcatives, alors les résultats sont évidemment différents dans le sens de la précision, même
si la distribution du temps de calcul nécessaire ne montre pas de changement signiﬁcatif. A l’inverse,
s’il n’y a pas de changement signiﬁcatif de la précision mais un changement signiﬁcatif du temps de
calcul, alors le résultat est aussi différent, au sens où les deux algorithmes à comparer ont la même

239

240

ANNEXED. ANALYSEDELAVARIANCE(KRUSKAL-WALLIS)

précision mais non la même complexité.

Nous utilisons des seuils de 1% et 5%. Les expériences qui dépassent ces seuils (et qui ne sont
donc pas forcément statistiquement signiﬁcatives) sont notées en italique ou en gras respectivement.
Il est important de noter que, dans de nombreux cas, nous évaluons le changement des para-
mètres en continu, par exemple le taux de croisement. Nous effectuons les tests entre deux valeurs
consécutives de ce paramètre. Ce procédé est motivé par le fait que nous ne cherchons pas des
optima globaux (qui sont d’ailleurs pratiquement impossibles à obtenir), mais un optimum local
d’assez bonne qualité. Les intervalles sont choisis arbitrairement en prenant en compte la théorie
existante. En général, la différence entre les résultats augmente avec la largeur de l’intervalle. Une
insigniﬁance peut donc venir d’un intervalle très court. Cependant, il est possible qu’il y ait une ten-
dance dans une série de valeurs. Les p-valeurs ne sont pas transitives. On ne peut donc pas conclure
sur des séries. En général, la plupart des différences non signiﬁcatives apparaissent pour des va-
leurs qui sont éloignées des paramètres optimaux. Dans le cas contraire, comme nous cherchons un
bon paramétrage en sachant qu’il ne sera jamais optimal, nous choisissons la valeur qui donne le
meilleur résultat en sachant qu’il est probablement possible d’ajuster le paramètre un peu sans trop
changer la qualité des résultats. Une conclusion valide de la non-signiﬁance entre deux bornes d’un
intervalle est qu’il n’est probablement pas nécessaire d’étudier des intervalles plus courts entre ces
deux valeurs, si l’on suppose que la probabilité d’un pic à l’intérieur est assez faible.

Par la suite nous détaillons les résultats non signiﬁcatifs observés, la totalité des études se trou-

vant dans les tableaux D.1 à D.19.

D.1 Taille de population

D.1.1 Algorithme générationnel

L’optimum observé pour CX est une population de 400 individus. Les différences entre 300 et
400 (pSR = 0, 025 et pAES = 0, 026) ne sont pas signiﬁcatives avec le seuil à 1%, mais le sont
pour le seuil à 5%. En ce qui concerne l’opérateur UOX, il n’y a pas de différence signiﬁcative entre
une population de 9 et 10 (pSR = 0, 32 et pAES = 0, 77). Ici, nous avons fait une expérience avec
les intervalles les plus petits possibles. Pour les trois opérateurs qui incluent des heuristiques, les
différences sont non signiﬁcatives entre l’optimum et seulement une de ses deux valeurs voisines,
pour DPX entre 40 et 50 individus (pSR non déﬁni, pARS = 0, 86), pour SDPX entre 25 et 30 (pSR
non déﬁni, pARS = 0, 12) et pour NDPX entre 25 et 30 (pSR non déﬁni, pARS = 0, 24).

D.1.2 Algorithme steady-state

Seul SDPX montre des différences non signiﬁcatives entre son optimum et une valeur voisine :

entre 10 et 15 (pSR = 0, 083, pARS = 0, 5).

D.2. PROBABILITÉDECROISEMENT

241

D.2 Probabilité de croisement

La probabilité de croisement s’applique uniquement dans le cadre des algorithmes génération-
nels. On peut noter que la granularité des observations entre 95% et 99% est très ﬁne. Par conséquent,
les différences ne sont pas nécessairement signiﬁcatives. De plus, pour de nombreux opérateurs,
nous avons trouvé l’optimum à 99% et donc au maximum. Notamment, pour PMX les différences
entre 99 et 98 ne sont pas signiﬁcatives (pSR = 0, 65 et pAES = 0, 97), tandis qu’entre 97 et 98
(pSR = 0, 016 et pAES = 0, 013) elles le sont. Pour UPBX toute la série entre 95% et 99% ne
montre pas de différence signiﬁcative si l’on utilise le seuil à 1%. Quant au seuil à 5%, la différence
entre 98% et 99% est signiﬁcative (pSR = 0, 0672 et pAES = 0, 043).

PBX présente un optimum à 96% mais les différences entre 95% et 96% (pSR = 0, 46 et pAES =
0, 53) et celles entre 96% et 97% (pSR = 0, 059 et pAES = 0, 1) ne sont pas signiﬁcatives. Quand on
regarde les valeurs voisines, elles sont signiﬁcativement différentes : de 97% à 98%, pSR = 0, 48 et
pAES = 0, 0014 ; de 90% à 95%, pSR = 0, 023 et pAES = 1, 1E − 15. Pour UPMX nous effectuons
une étude autour de 80% car nous observons un pic autour de cette valeur avec la granularité assez
grossière initiale. Les valeurs sont signiﬁcativement différentes à l’exception du passage de 80% à
81% dont les p-valeurs sont : pSR = 1 et pAES = 0, 12.

Les croisements qui incluent une stratégie gloutonne ont des caractéristiques particulières :
quand on augmente le taux de croisement, il faut en même temps diminuer ou augmenter la taille
de la population selon l’opérateur. Pour DPX, on n’observe aucune différence signiﬁcative pour les
probabilités entre 20% et 45%

D.3 Probabilité de mutation

D.3.1 Algorithme générationnel

Si l’on utilise le seuil strict (1%), les différences entre 11% et 12% ne sont pas signiﬁcatives
pour l’opérateur UPMX (pSR = 0, 056 et pAES = 0, 034). De même pour UOX, la différence entre
14% et 15% n’est pas signiﬁcative (pSR = 0, 71 et pAES = 0, 013). Quand on passe au seuil de 5%,
les différences sont signiﬁcatives dans les deux cas. En ce qui concerne DPX, les différences ne sont
pas signiﬁcatives entre 20% et 25% (pSR = 0, 32 et pARS = 0, 069) indépendamment du seuil.

D.3.2 Algorithme steady-state

Pour les algorithmes steady-state la probabilité de mutation n’est pas aussi importante que pour
le moteur générationnel. Par conséquent, des résultats non signiﬁcatifs sont nombreux. Les p-valeurs
qui dépassent 1% sont en particulier :

242

ANNEXED. ANALYSEDELAVARIANCE(KRUSKAL-WALLIS)

UPMX : entre 5% et 6% (pSR = 0, 046 et pAES = 0, 81) ; entre 6% et 7% (pSR = 0, 86 et

pAES = 0, 031).

UPBX : entre 5% et 6% (pSR = 0, 78 et pAES = 0, 6) ; entre 6% et 7% (pSR = 1 et pAES = 0, 2).

UOX : entre 6% et 7% (pSR = 0, 27 et pAES = 1).

DPX : entre 20% et 25% (pSR non déﬁnie et pARS = 0, 14) ; entre 25% et 30% (pSR non déﬁnie

et pARS = 0, 16).

SDPX : entre 25% et 30% (pSR = 0, 32 et pARS = 0, 83).

NDPX : entre 20% et 25% (pSR = 0, 83 et pARS = 0, 62) ; entre 25% et 30% (pSR = 0, 83 et

pARS = 0, 092)

D.4 Autres tests

Les tests qui concernent la comparaison des différents opérateurs (croisement ou mutation),
des différents moteurs d’évolution (GGA-SSGA) sont tous signiﬁcatifs, à la différence des tests
d’optimisation de paramètres pour un opérateur.

D.5 Tableaux associés

sp
10-15
15-20
20-25
25-30
30-40
40-50
50-60
60-70
70-80
80-90
90-100
100-120
120-140
140-160
160-180
180-200
200-250
250-300
300-400
400-500
500-600
600-700
700-1000

CX

PMX

UPMX

PBX

UPBX

pSR
-
-
-
-
-
-
4,6E-03
0
0
0
0
1,9E-02
3,2E-01
1,5E-03
8,3E-01
3,3E-01
5,0E-01
4,0E-01
2,5E-02
1,0E-01
3,2E-01
-
-

pAES
-
-
-
-
-
-
1
9,2E-01
7,2E-01
7,2E-13
0
0
0
2,1E-10
1,9E-04
5,7E-05
4,1E-12
4,2E-07
2,6E-02
3,1E-05
1,3E-14
0
0

pSR
0
-
0
0
0
3,5E-01
1,0E-07
2,9E-12
2,2E-16
1,1E-10
1,9E-04
1,0E-03
3,2E-01
3,2E-01
3,2E-01
3,4E-02
8,8E-04
5,3E-03
3,6E-09
2,9E-05
6,6E-03
7,5E-03
1,2E-06

pAES
1
0
1,0E-09
4,1E-01
7,8E-16
9,1E-15
2,2E-06
8,3E-01
1,1E-04
1,1E-10
1,6E-04
0
0
0
3,0E-14
6,3E-13
0
0
0
0
0
2,0E-15
0

pSR
6,3E-01
9,9E-02
6,5E-02
9,1E-01
2,6E-03
7,5E-11
3,9E-13
0
0
5,6E-15
5,4E-04
0
0
4,4E-11
2,7E-08
8,2E-07
0
3,0E-07
1,5E-04
2,4E-02
3,5E-02
5,7E-02
1,6E-08

pAES
2,1E-04
6,7E-01
6,1E-01
2,6E-03
1,6E-01
5,3E-04
3,7E-11
7,2E-04
5,3E-12
0
0
0
6,0E-13
3,6E-08
1,1E-04
3,1E-02
1,1E-01
2,5E-01
5,0E-05
2,2E-05
3,5E-02
3,4E-03
3,3E-01

pSR
0
0
7,5E-03
2,3E-01
1,7E-11
1,6E-12
4,7E-10
9,2E-11
1,6E-09
8,1E-06
1,6E-03
4,2E-03
1,3E-01
2,6E-01
1,6E-01
1,6E-01
6,5E-01
4,8E-01
8,0E-06
1,1E-01
2,3E-02
6,4E-02
5,0E-07

pAES
0
0
6,4E-06
2,3E-03
5,1E-08
3,2E-11
2,2E-05
5,7E-01
7,3E-02
1,9E-08
1,2E-12
0
0
0
0
0
0
0
0
0
0
0
0

pSR
0
7,3E-01
9,4E-01
3,1E-03
6,9E-13
0
5,8E-15
0
5,3E-10
7,1E-04
6,5E-01
8,9E-04
-
-
3,2E-01
3,8E-03
1,7E-05
2,5E-01
1,3E-08
1,5E-04
2,8E-02
7,8E-02
8,2E-06

pAES
0
4,7E-05
4,6E-01
1,0E-03
2,0E-13
0
2,4E-11
4,8E-03
4,1E-03
9,8E-13
2,2E-16
0
0
1,1E-16
0
2,8E-15
0
0
0
0
0
0
0

Tableau D.1 – p-valeurs entre les résultats de deux expériences consécutives de la taille de population (GGA - opérateurs sans heuristique sauf UOX).

D
.
5
.
T
A
B
L
E
A
U
X
A
S
S
O
C
I
É
S

2
4
3

CX

PMX

UPMX

PBX

UPBX

UOX

pSR

pAES

pSR

pSR

pSR

pAES

pSR

pAES

pSR

pAES

pAES

sp
pAES
10-15
2,1E-03 1,5E-01 1,8E-03 2,3E-01 9,5E-03 7,0E-03 2,9E-02 7,7E-01 5,7E-02 5,6E-03 1,9E-04 4,0E-01
1,2E-02 3,8E-01 4,8E-02 1,2E-01 3,2E-03 1,1E-01 2,1E-03 4,6E-01 2,3E-02 3,3E-02 4,2E-01 1,1E-01
15-20
1,8E-01 8,0E-01 7,3E-01 9,3E-01 6,6E-01 2,4E-02 9,1E-01 1,4E-01 1,4E-02 5,2E-01 2,5E-01 2,2E-01
20-25
2,2E-01 8,3E-02 7,3E-01 1,3E-01 8,4E-02 7,6E-01 4,1E-01 6,5E-01 8,5E-01 6,6E-02 1,5E-03 9,2E-01
25-30
3,4E-01 7,9E-01 3,8E-03 5,7E-01 1,4E-02 9,8E-03 5,7E-02 3,9E-02 1,2E-02 3,1E-02 7,1E-01 7,2E-01
30-40
1,7E-01 4,0E-01 2,9E-01 6,9E-02 9,7E-01 2,9E-02 4,0E-04 5,2E-02 1,8E-02 5,6E-03 6,8E-01 2,5E-01
40-50
2,2E-02 9,3E-01 5,5E-04 6,4E-01 8,2E-01 2,6E-01 8,5E-02 6,4E-01 1,3E-01 1,2E-03 6,4E-02 3,7E-01
50-60
8,2E-01 9,8E-01 7,7E-01 2,0E-01 6,4E-02 7,5E-03 8,7E-02 4,8E-01 3,4E-01 8,2E-04 7,3E-01 6,3E-01
60-70
6,1E-01 6,6E-02 2,8E-01 2,1E-01 8,8E-04 1,2E-01 4,4E-01 2,4E-01 3,5E-02 2,3E-03 3,2E-01 1,8E-01
70-80
2,0E-01 5,5E-01 7,6E-01 6,8E-01 1,5E-01 4,0E-02 5,6E-01 4,5E-01 1,8E-01 5,1E-01 9,1E-02 9,8E-02
80-90
1 8,6E-02 6,3E-01 3,5E-02 3,1E-01 3,9E-01 9,6E-03 4,5E-01 3,3E-01 1,3E-03 2,7E-01 7,2E-01
90-100
6,5E-02 4,5E-01 1,6E-02 1,3E-01 3,1E-01 1,7E-03 2,4E-01 9,5E-01 8,2E-03 1,8E-03 1,5E-02 2,1E-01
100-120
4,8E-02 8,2E-01 4,9E-01 4,7E-01 1,3E-01 3,8E-01 1,4E-03 8,1E-02 1,8E-02 2,6E-05 1,4E-03 6,6E-03
120-140
7,4E-01 8,4E-01 2,7E-01 6,1E-02 2,0E-01 6,8E-01 8,9E-01 1,3E-02 3,9E-01 1,7E-01 3,8E-01 4,1E-02
140-160
5,7E-01 1,3E-01 6,1E-01 2,3E-01 8,5E-02 2,9E-01 5,3E-02 1,4E-04 1,8E-06 4,3E-01 6,8E-02 1,4E-01
160-180
1,7E-01 2,8E-02 1,9E-02 9,9E-01 5,6E-02 1,1E-01 7,4E-03 8,2E-03 7,6E-01 3,5E-01 5,1E-01 1,4E-01
180-200
200-250
0 1,1E-04 1,5E-04
1,5E-02 2,3E-02 5,6E-01 2,3E-07 1,3E-03 2,2E-13 5,2E-02
4,3E-01 1,3E-01 3,9E-02 2,0E-02 1,1E-02
250-300
0 1,8E-04
0 2,5E-04 4,1E-13
300-400
0
0 2,0E-09
0 1,1E-01
1,6E-04 3,0E-05 9,2E-01
0 1,1E-09
400-500
0
0 3,9E-03
0 5,9E-02
9,6E-03 2,6E-10 1,0E-01 5,1E-13 9,3E-02
0
0 2,2E-01
0 3,2E-01
0 6,6E-03
1,5E-01 3,3E-15 1,9E-01
500-600
600-700
3,3E-02
0 4,0E-02
0 3,2E-01
0
-
0 4,8E-01
0
0
0 1,5E-02
-
0
-
0
0 7,0E-03
700-1000 4,6E-03

0 1,7E-03
0 1,0E-01
0 1,1E-04
0 1,1E-01
0 1,8E-01
0 3,2E-01
-
0

Tableau D.2 – p-values entre les résultats de deux expériences consécutives de la taille de population (SSGA - opérateurs sans heuristique).

2
4
4

A
N
N
E
X
E
D

.

A
N
A
L
Y
S
E
D
E
L
A
V
A
R
I
A
N
C
E
(
K
R
U
S
K
A
L
-
W
A
L
L
I
S
)

sp
10-15
15-20
20-25
25-30
30-40
40-50
50-60
60-70
70-80
80-90
90-100
100-120
120-140
140-160
160-180
180-200
200-250
250-300
300-400
400-500
500-600
600-700
700-1000

DPX

SDPX

NDPX

pSR
0
1,6E-05
8,0E-04
2,5E-01
4,5E-02
-
3,2E-01
3,2E-01
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

pARS
0
0
8,3E-01
9,9E-02
2,7E-03
8,6E-01
1,7E-05
6,8E-01
9,0E-01
7,2E-02
4,9E-05
7,7E-03
8,6E-01
1,4E-13
7,7E-01
2,5E-04
1,7E-06
2,1E-06
2,4E-06
1,0E-09
4,4E-04
6,1E-08
1,1E-07

pSR
0
1
3,2E-01
-
3,2E-01
5,6E-01
5,6E-01
5,6E-01
6,5E-01
6,5E-01
1
5,6E-01
3,2E-01
-
3,2E-01
3,2E-01
-
-
3,2E-01
0
0
0
0

pARS
0
0
1,5E-08
1,2E-01
0
2,0E-01
8,3E-11
1,2E-01
0
1,0E-02
2,0E-04
0
0
0
0
0
0
0
0
0
0
0
2,5E-07

pSR
0
3,2E-01
-
-
-
1,6E-01
1
5,6E-01
1
3,2E-01
-
5,1E-15
0
0
0
0
0
2,8E-03
8,3E-02
-
-
-
-

pARS
0
0
5,6E-12
2,4E-01
0
0
0
0
0
0
0
0
0
3,4E-12
8,6E-06
6,5E-01
3,6E-01
5,8E-01
1
-
-
-
-

Tableau D.3 – p-values entre les résultats de deux expériences consécutives de la taille de population (GGA - opérateurs avec heuristique gloutonne).

D
.
5
.
T
A
B
L
E
A
U
X
A
S
S
O
C
I
É
S

2
4
5

sp
10-15
15-20
20-25
25-30
30-40
40-50
50-60
60-70
70-80
80-90
90-100
100-120
120-140
140-160
160-180
180-200
200-250
250-300
300-400
400-500
500-600
600-700
700-1000

DPX

SDPX

NDPX

pARS
5,0E-04
7,9E-03
0
3,4E-14
0
0
0
0
0
0
5,3E-14
0
0
0
4,6E-12
1,7E-08
0
3,3E-14
4,2E-06
1,1E-02
2,3E-02
7,5E-01
7,5E-01

pSR
8,3E-02
-
-
3,2E-01
3,2E-01
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
0

pARS
5,0E-01
7,1E-03
6,3E-13
0
0
0
0
0
0
6,1E-05
0
0
0
0
0
0
0
0
0
0
0
0
0

pSR
6,0E-02
6,2E-01
6,2E-01
3,6E-01
1
7,1E-01
6,5E-01
6,5E-01
7,1E-01
7,1E-01
6,5E-01
5,6E-01
1
5,6E-01
1
1,6E-01
1,6E-01
5,6E-01
1
3,2E-01
1,6E-01
1,2E-04
0

pARS
1,9E-02
2,4E-05
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

pSR
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

Tableau D.4 – p-values entre les résultats de deux expériences consécutives de la taille de population (SSGA - opérateurs avec heuristique gloutonne).

2
4
6

A
N
N
E
X
E
D

.

A
N
A
L
Y
S
E
D
E
L
A
V
A
R
I
A
N
C
E
(
K
R
U
S
K
A
L
-
W
A
L
L
I
S
)

CX

PMX

UPMX

PBX

UPBX

UOX

pSR

pAES

pSR

pSR

pAES

pSR

pAES

pSR

pAES

pSR

pAES

1 2,2E-01 1,9E-01 3,1E-06 1,0E-01 1,9E-04

pc
0,1-0,2
0,2-0,3
0,3-0,4
0,4-0,5
0,5-0,6
0
0,6-0,7
0
0,7-0,8
0
0,8-0,85
0
0,85-0,9
0 7,4E-02 2,3E-02 1,1E-15 1,2E-13 1,0E-02
0,9-0,95
0,95-0,96 9,5E-02 3,0E-03 8,1E-01 3,1E-01 1,4E-02 3,0E-01 4,6E-01 5,3E-01 2,7E-01 3,9E-01
0,96-0,97 6,2E-01 8,2E-01 4,4E-01 1,6E-01 3,7E-03 5,2E-01 5,9E-02 1,0E-01 1,2E-02 8,5E-01
0,97-0,98 6,2E-01 5,8E-01 1,6E-01 1,3E-02 3,5E-03 5,9E-01 4,8E-01 1,4E-03
1 1,2E-01
1 3,5E-03 6,5E-01 9,7E-01 2,2E-03 7,9E-01 7,1E-01 2,2E-02 6,7E-02 4,3E-02
0,98-0,99

pAES
0 6,1E-01 8,5E-01 1,1E-04 4,5E-01 2,1E-01 4,4E-01 8,2E-01 6,6E-01 4,1E-02 5,2E-05 1,0E-01
0 4,2E-01 2,9E-02 3,5E-01 1,2E-01 7,6E-01 6,3E-04 3,2E-01 1,9E-02 1,5E-09 5,8E-02
0
0 2,4E-01 1,1E-01 4,0E-01 3,3E-01 3,6E-01 3,1E-04 8,1E-01 9,8E-01 7,7E-13 3,9E-01
8,4E-01
4,8E-01
0 6,7E-02 1,4E-03
0 4,1E-01
0 1,5E-05 1,0E-07 2,5E-11 7,9E-01
0 2,7E-02 2,0E-07 1,1E-03 7,8E-04 1,6E-14
3,9E-02
0
0 3,0E-12 1,6E-13
0
0 1,1E-09 7,3E-10
5,6E-02
0
0
5,1E-01
0
0 1,0E-12
0 7,6E-08
0
0 8,6E-01
0
0 1,5E-03 1,6E-01 1,1E-16 1,1E-16 1,3E-01 6,8E-01
1,5E-02 2,8E-13 4,6E-15 6,8E-02
1
0 1,5E-07 4,4E-06 2,5E-06 4,4E-03 4,6E-03
6,4E-01 7,8E-05 3,3E-16 1,6E-07
2,1E-02 2,8E-07 1,1E-14 7,4E-08
-
-
-
-
-
-
-
-
-
-

0

Tableau D.5 – p-values entre les résultats de deux expériences consécutives de la probabilité de croisement (GGA - opérateurs sans heuristique).

D
.
5
.
T
A
B
L
E
A
U
X
A
S
S
O
C
I
É
S

2
4
7

CX

PMX

UPMX

PBX

UPBX

UOX

pAES
0 1,3E-07

0 1,8E-14

0 8,3E-02

0 2,7E-04

pSR

pAES
0 6,0E-04

pSR

pAES
0 1,1E-01

pSR

pAES
0 4,0E-01

pSR
0
0 8,0E-11

pSR

pAES
1
0 7,9E-02

pAES
pSR
1
0
7,2E-03
0
9,7E-01 8,0E-11 5,1E-01 1,5E-07 4,9E-01 1,3E-01 5,2E-01 3,6E-08 1,7E-01 1,3E-03 7,5E-01 1,2E-02
2,4E-01 1,5E-04 6,1E-01 7,2E-07 8,9E-01 5,4E-01 4,8E-01 1,3E-05 5,0E-01 1,8E-01 4,0E-07 9,9E-01
5,8E-01 2,7E-05 2,0E-01 1,7E-04 2,5E-03 3,4E-01 6,3E-01 5,0E-04 9,4E-01 2,5E-02 1,1E-02 5,3E-01
2,0E-01 6,1E-04 1,9E-01 1,3E-02 4,2E-01 3,0E-02 8,5E-01 5,4E-01 5,0E-01 1,6E-01
0 7,4E-02
1,5E-03 3,8E-02 5,9E-02 6,7E-03 6,3E-01 2,8E-01 5,6E-01 3,7E-02 2,3E-01 7,3E-01 1,5E-10 7,7E-02
5,1E-02 6,2E-04 6,4E-02 7,5E-03 5,5E-01 6,2E-02 6,9E-01 5,1E-02 6,8E-01 6,9E-01
0 8,9E-01
4,1E-01 2,1E-01 3,3E-01 8,5E-03 5,6E-02 3,4E-02 3,1E-01 3,2E-02 1,0E-01 3,4E-01 2,9E-14 1,9E-01
8,6E-01 6,0E-04 4,7E-01 2,7E-01 9,0E-01 4,7E-03 9,8E-02 2,7E-02 7,5E-02 6,8E-01 5,8E-14 6,8E-05
5,1E-01 7,8E-01 2,9E-01 8,3E-02 5,5E-01 4,3E-03 7,8E-01 2,3E-01 6,7E-01 4,6E-01 1,7E-03 5,1E-01
8,5E-02 1,3E-01 1,2E-01 8,9E-02 8,2E-01 1,2E-04 2,6E-01 2,7E-01 2,8E-02 6,1E-01 7,1E-01 1,3E-02
1
3,6E-01 6,4E-08 9,8E-03 5,6E-06
-
1,6E-02 6,3E-04 6,2E-05 5,9E-04
8,0E-02 2,4E-02 2,8E-09 1,1E-02
-
-
0
3,1E-05 6,5E-01
-
0
6,5E-10 5,0E-06
1,6E-10 2,2E-02
1
-
-
-
0
7,1E-12

0 7,1E-06 8,0E-01 2,2E-03 1,0E-09
0
0 3,5E-05 1,0E-14 1,5E-03
-
0
0
0
1
0
-
-
-
-

0
0
0 2,9E-12 9,6E-09
1 2,5E-01
0
-
-
0
-
- 1,5E-14
-
-
-

0
-
-
-
-
-
-

0
0
0
1
-

pm
0,001-0,01
0,01-0,05
0,05-0,06
0,06-0,07
0,07-0,08
0,08-0,09
0,09-0,1
0,1-0,11
0,11-0,12
0,12-0,13
0,13-0,14
0,14-0,15
0,15-0,2
0,2-0,25
0,25-0,3
0,3-0,4
0,4-0,5
0,5-0,6
0,6-0,7

0
0
0
-

Tableau D.6 – p-values entre les résultats de deux expériences consécutives de la probabilité de mutation (GGA - opérateurs sans heuristique).

2
4
8

A
N
N
E
X
E
D

.

A
N
A
L
Y
S
E
D
E
L
A
V
A
R
I
A
N
C
E
(
K
R
U
S
K
A
L
-
W
A
L
L
I
S
)

CX

PMX

UPMX

PBX

UPBX

UOX

pm
0,001-0,01
0,01-0,05
0,05-0,06
0,06-0,07
0,07-0,08
0,08-0,09
0,09-0,1
0,1-0,11
0,11-0,12
0,12-0,13
0,13-0,14
0,14-0,15
0,15-0,2
0,2-0,25
0,25-0,3
0,3-0,4
0,4-0,5
0,5-0,6
0,6-0,7

pAES

pSR

pAES

pSR

pAES

pSR

1 2,0E-01 2,7E-01

1 1,1E-02 7,1E-01

pSR

pAES

pSR

pSR
0
1,6E-01
8,8E-01
6,4E-01 1,6E-07 6,8E-01 7,8E-01 8,6E-01 3,1E-02 8,9E-01 9,1E-01
4,3E-01 1,8E-10
8,7E-01 2,4E-04 5,2E-01 9,8E-01 3,0E-01 4,6E-01
7,4E-01 1,0E-04 9,4E-02 4,4E-01 5,4E-01 3,8E-01 4,8E-01 8,8E-01 5,6E-01

pAES
pAES
0 7,3E-01
0 1,4E-15 1,0E-03 1,5E-02 4,4E-01 6,8E-03 5,3E-01 3,8E-04 9,5E-01
0 1,1E-02 5,1E-04 3,4E-02 8,8E-02 5,5E-01 5,6E-01 7,8E-01 1,2E-03 1,2E-02 2,2E-02
0 6,5E-01 9,9E-01 4,6E-02 8,1E-01 5,2E-01 7,3E-01 7,8E-01 6,0E-01 7,8E-01 4,1E-01
1
1 6,8E-01 9,1E-02 8,0E-01 5,7E-01 7,1E-01 7,6E-01 9,8E-01 4,3E-03 3,1E-01
1 4,1E-01 5,6E-01 2,0E-01 7,7E-02 1,4E-01
1 7,7E-01 8,4E-01
1 7,7E-01 2,1E-01 4,8E-01 1,0E-01 7,6E-01 8,4E-01 7,7E-01 2,4E-01
2,1E-01 1,9E-03 9,5E-01 6,0E-01 3,3E-01 4,9E-01 7,6E-01 6,4E-01 2,2E-01 6,6E-02 7,1E-01 4,6E-01
2,1E-01 8,2E-02 2,8E-01 9,8E-01 6,8E-02 3,9E-01 7,6E-01 3,6E-01 6,5E-01 9,8E-01 7,2E-01 1,1E-01
7,2E-01 2,8E-05 1,9E-02 1,3E-02 6,8E-01 4,4E-01 6,7E-01 8,3E-01 8,3E-02 3,1E-01 3,0E-01 3,5E-01
2,5E-01 2,9E-02 2,2E-02 2,7E-01 6,6E-01 3,2E-01 6,7E-01 5,6E-02 7,1E-01 6,0E-03 3,9E-01 7,3E-01
1 5,1E-01 4,1E-01 5,5E-02 6,6E-01 2,4E-02
4,2E-01
6,2E-01 5,9E-11 9,5E-01 1,5E-01 8,7E-01 6,8E-03 8,8E-01 5,8E-04 9,5E-02 5,9E-06 8,0E-01 4,8E-02
5,0E-01 1,3E-03 5,3E-01 9,8E-04 9,3E-02 1,1E-04 2,7E-01 1,8E-03 7,8E-01 1,4E-01 5,5E-01 2,1E-02
0 2,7E-01 1,6E-14
0
0 1,5E-01
0 9,4E-01
0
0
0 4,3E-01

6,1E-01 2,7E-01 6,0E-01 3,8E-11 8,5E-01
4,9E-01 7,2E-06 2,4E-01
0 8,5E-01
0 6,8E-01
0 1,8E-01
1,4E-01

1 3,0E-08 5,3E-01
0 8,7E-01 6,2E-15 4,5E-02
0 3,7E-01
0 1,6E-01
1
0
0 2,9E-02

1 1,8E-06 8,5E-01 4,5E-04 3,2E-01 3,9E-14

0 2,1E-01 4,4E-01 7,6E-01 1,5E-03

Tableau D.7 – p-values entre les résultats de deux expériences consécutives de la probabilité de mutation (SSGA - opérateurs sans heuristique).

D
.
5
.
T
A
B
L
E
A
U
X
A
S
S
O
C
I
É
S

2
4
9

pm
0,001-0,01
0,01-0,05
0,05-0,06
0,06-0,07
0,07-0,08
0,08-0,09
0,09-0,1
0,1-0,11
0,11-0,12
0,12-0,13
0,13-0,14
0,14-0,15
0,15-0,2
0,2-0,25
0,25-0,3
0,3-0,4
0,4-0,5
0,5-0,6
0,6-0,7

DPX

SDPX

NDPX

pSR
0
-
-
3,2E-01
3,2E-01
-
-
-
3,2E-01
3,2E-01
3,2E-01
3,2E-01
-
3,2E-01
3,2E-01
-
0
2,1E-01
0

pARS
0
0
1,5E-07
4,8E-11
6,0E-03
1,6E-02
4,6E-06
9,4E-01
2,6E-01
2,7E-05
3,2E-01
5,9E-01
5,0E-06
6,9E-02
1,7E-04
0
1,4E-14
6,3E-01
0

pSR
0
2,4E-01
5,1E-01
1,0E-01
5,0E-01
7,8E-01
7,9E-01
2,7E-01
1,0E-01
8,9E-01
7,8E-01
6,9E-01
5,6E-02
1,8E-01
1,5E-03
0
-
-
-

pARS
1
0
0
0
0
4,1E-15
5,0E-13
4,2E-08
9,3E-06
7,6E-07
1,6E-04
5,3E-04
0
1,6E-05
4,4E-16
1
-
-
-

pSR
0
4,1E-04
1
3,5E-01
2,3E-01
7,4E-01
8,9E-01
8,4E-01
9,8E-02
4,0E-01
3,6E-01
6,2E-01
4,3E-02
2,0E-11
6,1E-05
0
-
-
-

pARS
1
0
0
0
3,7E-13
6,3E-11
1,2E-08
5,3E-06
6,4E-07
8,8E-04
2,8E-05
2,5E-03
0
1,4E-03
0
1
-
-
-

Tableau D.8 – p-values entre les résultats de deux expériences consécutives de la probabilité de mutation (GGA - opérateurs avec heuristique glou-
tonne).

2
5
0

A
N
N
E
X
E
D

.

A
N
A
L
Y
S
E
D
E
L
A
V
A
R
I
A
N
C
E
(
K
R
U
S
K
A
L
-
W
A
L
L
I
S
)

pm
0,001-0,01
0,01-0,05
0,05-0,06
0,06-0,07
0,07-0,08
0,08-0,09
0,09-0,1
0,1-0,11
0,11-0,12
0,12-0,13
0,13-0,14
0,14-0,15
0,15-0,2
0,2-0,25
0,25-0,3
0,3-0,4
0,4-0,5
0,5-0,6
0,6-0,7

DPX

SDPX

NDPX

pSR
0
4,5E-02
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

pARS
5,4E-02
4,7E-03
1,6E-02
5,0E-01
6,6E-02
7,9E-01
2,4E-01
5,7E-01
5,6E-01
6,6E-01
5,6E-02
1,2E-01
1,1E-01
1,4E-01
1,6E-01
9,1E-04
1,4E-03
1,4E-10
0

pSR
0
1
3,2E-01
-
-
-
-
3,2E-01
3,2E-01
-
-
-
-
-
3,2E-01
3,2E-01
3,2E-01
1
1

pARS
0
0
5,5E-11
4,4E-04
2,8E-03
1,5E-03
3,0E-05
5,6E-02
4,2E-02
8,4E-01
4,1E-01
2,4E-01
4,5E-09
4,4E-07
8,3E-01
6,4E-01
4,0E-04
1,2E-15
0

pSR
0
6,8E-03
4,9E-01
8,3E-01
8,3E-01
1,3E-01
2,8E-01
6,2E-01
3,6E-01
5,3E-01
1
5,9E-01
4,9E-01
8,3E-01
8,3E-01
3,4E-01
2,5E-01
3,7E-01
5,9E-01

pARS
0
0
1,7E-06
6,3E-04
9,0E-06
7,7E-01
3,2E-02
9,4E-03
1,8E-01
1,7E-01
2,1E-01
1,2E-02
1,0E-07
6,2E-01
9,2E-02
2,4E-02
3,5E-08
6,0E-12
8,5E-14

Tableau D.9 – p-values entre les résultats de deux expériences consécutives de la probabilité de mutation (SSGA - opérateurs avec heuristique
gloutonne).

D
.
5
.
T
A
B
L
E
A
U
X
A
S
S
O
C
I
É
S

2
5
1

252

ANNEXED. ANALYSEDELAVARIANCE(KRUSKAL-WALLIS)

PMX

pSR
6,3E-01
1,9E-04
1,4E-02
7,6E-02
7,0E-02
4,1E-01
1,6E-01
-
-
4,5E-02

pAES
9,2E-04
1,3E-05
1,3E-02
4,8E-06
1,9E-03
4,2E-06
4,0E-07
8,7E-07
2,1E-08
3,8E-12

pm
0,35-0,36
0,36-0,37
0,37-0,38
0,38-0,39
0,39-0,40
0,40-0,41
0,41-0,42
0,42-0,43
0,43-0,44
0,44-0,45

Tableau D.10 – p-valeurs entre les expériences détaillées de la probabilité de mutation (GGA-PMX,
Fig. 3.3).

UPMX

pSR
3,2E-03
7,7E-06
2,6E-05
6,4E-04
1,3E-04
1
3,4E-01
5,5E-05
8,3E-08
1,5E-12

pAES
9,9E-01
1,4E-03
4,8E-02
4,7E-08
0
1,2E-01
9,5E-15
6,4E-10
3,0E-11
3,5E-05

pc
0,75-0,76
0,76-0,77
0,77-0,78
0,78-0,79
0,79-0,8
0,8-0,81
0,81-0,82
0,82-0,83
0,83-0,84
0,84-0,85

Tableau D.11 – p-valeurs entre les expériences détaillées de la probabilité de croisement (GGA-
UPMX, Fig. 3.4).

D.5. TABLEAUXASSOCIÉS

253

UOX pc = 0, 6

UOX 4 individus sans croisement

sp
8-9
9-10
10-11
11-12
12-13
13-14
14-15
15-16
16-17
17-18
18-19
19-20

pSR
6,5E-01
3,2E-01
0
3,0E-04
0
2,9E-01
1,7E-01
0
7,1E-01
6,9E-05
6,7E-01
7,6E-01

pAES
1,2E-06
7,7E-01
7,7E-02
7,3E-01
9,3E-01
5,3E-01
1,6E-01
1,9E-06
4,1E-01
1,3E-04
5,5E-01
8,8E-02

pSR
6,5E-01
3,2E-01
1,1E-01
6,9E-02
2,2E-03
1,1E-05
7,6E-06
4,0E-04
4,3E-11
5,2E-04
5,8E-08
1,3E-07

pAES
1,2E-06
7,7E-01
2,0E-01
4,5E-03
1,3E-03
4,5E-03
2,9E-04
4,1E-01
1,9E-02
2,2E-01
7,9E-01
9,2E-02

Tableau D.12 – p-valeurs entre les expériences détaillées de la taille de population (GGA-UOX,
Figs. 3.8, 3.9).

UOX

pSR
0
3,2E-12
2,1E-13
1,5E-03
7,8E-01
9,7E-11
0
0
5,6E-16
1,6E-01

pAES
4,1E-02
8,6E-01
4,7E-07
5,2E-01
1,2E-04
0
0
2,2E-01
8,1E-02
1

pm
0,1-0,11
0,11-0,12
0,12-0,13
0,13-0,14
0,14-0,15
0,15-0,16
0,16-0,17
0,17-0,18
0,18-0,19
0,19-0,2

Tableau D.13 – p-valeurs entre les expériences détaillées de la probabilité de mutation (GGA-UOX,
Fig. 3.8).

254

ANNEXED. ANALYSEDELAVARIANCE(KRUSKAL-WALLIS)

DPX

SDPX

NDPX

pARS
8,98E-05
1,94E-01
5,48E-01
4,54E-01
8,34E-01
9,76E-01

pARS
0
5,98E-01
5,46E-03
9,30E-07
0
0

pARS
2,23E-02
0
1,62E-01
7,35E-03
0
0

pc
0,15-0,20
0,20-0,25
0,25-0,30
0,30-0,35
0,35-0,40
0,40-0,45

Tableau D.14 – p-valeurs entre les valeurs consécutives de croisement pour les opérateurs *DPX
(Figs. 3.10, 3.12, 3.13).

pc
0,15
0,20
0,25
0,30
0,35
0,40
0,45

sp
70
60
50
50
40
30
25

pARS
6,96E-03
1,94E-01
1
5,48E-01
8,81E-01
9,53E-01
9,40E-01

Tableau D.15 – p-valeurs par rapport à la troisième ligne - pas de signiﬁance (GGA-DPX, Fig. 3.10).

Opérateur
CX
PMX
UPMX
PBX
UPBX
UOX
DPX
SDPX
NDPX

pSR pAES
0
0
0
0
0
0
0
0
0

6,0E-02
0
8,0E-01
9,0E-03
4,4E-01
2,9E-13
-
-
4,6E-03

pSR
6,0E-02
0
8,0E-01
9,0E-03
4,4E-01
2,9E-13
-
-
4,6E-03

pARS
3,6E-03
0
0
0
0
0
0
0
0

Tableau D.16 – p-valeurs entre les résultats obtenus par le GGA par rapport au SSGA pour chaque
opérateur.

D.5. TABLEAUXASSOCIÉS

255

Paramètre
5-7,5
7,5-10
10-12,5
12,5-15
15-17,5
17,5-20
20-22,5
22,5-25
25-27,5
27,5-30
30-32,5
32,5-35
35-37,5
37,5-40

Taille 20

Taille 30

Taille 40

3,2E-01
2,0E-01
4,8E-01
8,3E-02
-
-
-
-
-
-
-
1,1E-16
0
1,1E-10

9,9E-12
6,3E-01
1,0E-01
1,2E-12
1,9E-15
0
0
0
0
0
0
0
3,4E-10
1

9,7E-02
3,2E-03
3,3E-02
6,6E-03
3,2E-01
-
-
-
0
0
-
-
-
-

0
7,0E-02
1,6E-02
1,5E-06
0
0
0
0
0
1
-
-
-
-

1,2E-06
1,7E-06
1,5E-09
3,0E-04
-
-
0
0
-
-
-
-
-
-

4,5E-01
3,1E-05
7,8E-07
9,0E-01
0
0
0
1
-
-
-
-
-
-

Tableau D.17 – p-valeurs entre les valeurs consécutives du paramètre de la mutation scramble (uni-
forme, GGA-CX).

Paramètre
5-7,5
7,5-10
10-12,5
12,5-15
15-17,5
17,5-20
20-22,5
22,5-25
25-27,5
27,5-30
30-32,5
32,5-35
35-37,5
37,5-40

Taille 20

Taille 30

Taille 40

-
-
-
-
-
-
-
-
-
-
-
-
-
-

1,7E-06
7,7E-01
5,4E-08
3,3E-01
7,6E-06
1,9E-01
1,2E-01
1,4E-01
1,5E-01
7,3E-03
4,8E-01
3,0E-01
9,1E-04
1,2E-01

-
-
-
-
-
-
-
-
8,3E-02
7,5E-03
1,6E-04
3,0E-10
1,4E-03
1,2E-05

3,0E-01
1,0E-03
1,4E-02
2,3E-01
8,8E-01
2,3E-02
2,4E-01
1,3E-03
4,8E-01
1,1E-03
6,3E-02
2,0E-01
2,8E-02
2,4E-02

-
-
-
-
-
2,7E-03
8,7E-10
4,8E-08
9,1E-08
2,4E-05
6,0E-02
2,5E-05
4,5E-04
4,1E-01

5,8E-02
2,1E-04
1,2E-02
6,0E-02
8,6E-03
2,4E-02
3,6E-02
2,9E-02
7,1E-01
4,1E-04
4,6E-01
1,5E-02
1,6E-02
7,0E-02

Tableau D.18 – p-valeurs entre les valeurs consécutives du paramètre de la mutation scramble (uni-
forme, SSGA-DPX).

256

ANNEXED. ANALYSEDELAVARIANCE(KRUSKAL-WALLIS)

Paramètre
5
7,5
10
12,5
15
17,5
20
22,5
25
27,5
30
32,5
35
37,5
40

GGA-CX

SSGA-DPX

pSR
0
0
2,8E-12
1,8E-01
8,1E-03
8,1E-03
8,1E-03
0
0
0
0
0
0
0
0

pAES
0
1,4E-15
2,9E-03
6,0E-03
1,3E-03
0
0
0
1
1
1
1
1
1
1

pSR
-
-
-
-
-
-
2,7E-03
8,8E-15
0
0
0
0
0
0
0

pARS
6,4E-02
4,2E-05
8,9E-16
0
0
0
0
0
0
0
7,8E-16
4,1E-12
1,1E-04
4,2E-01
1,3E-01

Tableau D.19 – Comparaison entre la mutation d’échange et la mutation scramble avec tous les
paramètres.

Annexe E

Note sur la représentation binaire des
nombres réels

Le problème de l’appariement inexact de graphes nécessite la minimisation d’une distance. En
raison du bruit inhérent, nous avons besoin d’utiliser des nombres réels. La représentation en bi-
naire de ceux-ci pour la grande majorité d’architectures d’aujourd’hui suit le standard IEEE 754
pour l’arithmétique à virgule ﬂottante (IEEE754, 2008). Bien que la question de l’arithmétique
des nombres réels soit rarement posée, elle présente des particularités inhérentes qui méritent et
nécessitent leur examen (Goldberg, 1991a). En effet, il se pourrait que l’exactitude du calcul soit
affectée si l’on traite des nombres réels comme des entiers. Les erreurs sont souvent dues à l’étape
d’arrondi, ou à des valeurs particulières comme l’inﬁni, +/- 0, ou le concept Not-a-Number, NaN.
Une représentation binaire de longueur ﬁxe distincte pour chaque nombre est impossible, même en
appliquant des bornes comme pour les entiers, car les nombres réels sont un corps archimédien.
Il existe toujours une valeur intermédiaire entre deux nombres donnés. Par conséquence, les va-
leurs binaires sont arrondies. Ainsi pour la valeur 0,2, la représentation en 32 bit en arrondissant
avec la méthode round-to-nearest-value de l’IEEE-754 a 0 pour signe, 01111100 pour exposant et
110011001100110011001101 pour mantisse 16.

Ceci représente une valeur proche de 0,2 mais non la valeur 0,2 exacte qui nécessiterait une
longueur inﬁnie de successions de 1100. La différence est assez petite (dans ce cas elle vaut environ
3E-9), et dans un même logiciel, sur la même machine, les valeurs sont arrondies exactement de la
même façon. Par conséquent, cela ne devrait pas poser problème.

Cependant, ces différences peuvent se renforcer au fur et à mesure après l’application des opé-
rations arithmétiques. Ainsi, considérons la fonction d’évaluation f (~x) qui évalue un chromosome.
Quand nous appliquons la recherche locale nous obtenons un individu ~y qui est très proche de ~x.

16. Ce calcul a été effectué en utilisant
http://babbage.cs.qc.edu/IEEE-754/Decimal.html).

la page web intitulée IEEE-754 Floating-Point Conversion

257

258

ANNEXEE. NOTESURLAREPRÉSENTATIONBINAIREDESNOMBRESRÉELS

Nous connaissons aussi la valeur d’amélioration de la ﬁtness (∆f (~x, ~y)) par la recherche locale. En
théorie, nous pouvons utiliser l’égalité f (~y) = f (~x) + ∆f (~x, ~y) pour remplacer l’évaluation de
l’individu ~y qui est très coûteuse, par l’addition de deux valeurs connues. Néanmoins il s’avère que
cette égalité n’est pas respectée dans la représentation binaire. Sur des graphes de taille 40, nous
observons par exemple des différences de l’ordre de 3 × 10−4 en single précision et 5 × 10−13 en
double précision. De fait, tous les opérateurs de comparaison (==,>,<,etc.) ne peuvent pas fonction-
ner de manière habituelle. Une approche simple à utiliser et efﬁcace que nous avons adoptée est de
ﬁxer un seuil de tolérance. Elle se montre assez robuste durant nos expérimentations. Toutefois, il
faut être conscient que cette méthode présente certains inconvénients quand elle est appliquée à des
valeurs particulières ou de grande variabilité.

Une discussion de méthodes plus robustes encore et incluant les nouvelles valeurs particulières

de la norme IEEE754-2008 est proposée par Dawson (2008).

La norme IEEE754 déﬁnit des représentations classiques de nombres réels par trois entiers si-
gniﬁcatifs : le signe, la mantisse et l’exposant. Il existe plusieurs niveaux de précision, avec des
longueurs différentes de la séquence de bits. En général, sont observés deux effets : plus la séquence
utilisée est longue, plus les calculs sont ﬁables, c’est-à-dire représentent une faible déviation de la
valeur théorique. Toutefois, une séquence plus longue nécessite plus de ressources et mène ainsi à
des temps d’exécution plus longs. La question est alors de savoir s’il existe un choix optimal pour
toutes les applications ?

Selon la norme mentionnée ci-dessus, la longueur peut varier entre 32 bits (single précision)
et 80 bits (double précision étendue), voire 128 bits (quadruple précision) depuis la mise à jour en
2008. La version initiale de la norme prévoit la base 2 uniquement, dans la version actuelle on trouve
aussi des représentations utilisant la base 10. Single et double (64 bits) précisions sont les formats
classiques dans des langages de programmation de haut niveau. En Java, ils correspondent aux types
primitifs ﬂoat et double respectivement. De nos jours, l’architecture des ordinateurs est en train de
changer vers des système 64 bits, notamment le x86-64 : les registres, le bus d’adressage et les bus
mémoire peuvent tous contenir 64 bits au lieu de 32 auparavant. Une valeur double rentre donc aussi
facilement qu’un single sur des systèmes 32 bits. Quant aux unités d’arithmétique à virgule ﬂottante
(FPU), elles utilisent déjà des registres d’au moins 80 bits depuis le Pentium Pro en 1995. Une perte
de performance est néanmoins possible due aux accès aux données (transferts cache par exemple)
sur les machines 32 bits. Sur les architectures à 64 bits, les doubles précisions sont naturelles et plus
précises. Nous n’avons pas observé de perte de vitesse en utilisant la double précision.

Néanmoins, l’utilisation du single peut offrir quelques avantages dans des cas spéciﬁques. Lorsque

l’on est confronté à une grande quantité de valeurs à stocker en mémoire/cache, on peut en stocker
deux fois plus. Avec deux fois plus de valeurs dans le cache, le temps d’accès à la mémoire est réduit.
Les processeurs actuels peuvent exécuter des opérations SIMD (single instruction multiple data) sur
des registres de 128 bits qui peuvent contenir par exemple soit deux valeurs en double précision soit

quatre en single précision. Aﬁn d’exploiter ces fonctions il est nécessaire d’optimiser le code pour
avoir plusieurs opérations du même genre à effectuer en même temps et choisir un compilateur/une
machine virtuelle qui les transmette sous la forme d’une commande SIMD.

259

