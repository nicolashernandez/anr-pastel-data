http://mediamining.univ-lyon2.fr/people/guille/publications/guille_phd.pdf

Thèse présentée pour obtenir le grade de
Docteur de l’Université Lumière Lyon 2

École Doctorale Informatique et Mathématiques (ED 512)

Laboratoire ERIC (EA 3083)
Discipline : Informatique

Diffusion de l’information dans les médias sociaux

Modélisation et analyse

Par : Adrien Guille

Présentée et soutenue publiquement le 25 novembre 2014, devant un jury composé de :

Pascal Poncelet, Professeur des Universités, Université Montpellier 2
Rapporteur
Emmanuel Viennet, Professeur des Universités, Université Paris 13
Rapporteur
Vincent Labatut, Maître de Conférences, Université d’Avignon et des Pays du Vaucluse
Examinateur
Christine Largeron, Professeur des Universités, Université Jean Monnet Saint-Etienne Examinatrice
Cécile Favre, Maître de Conférences, Université Lumière Lyon 2
Co-directrice
Djamel Zighed, Professeur des Universités, Université Lumière Lyon 2
Directeur

Abstract

Social media have greatly modiﬁed the way we produce, diffuse and consume
information, and have become powerful information vectors. The goal of this thesis
is to help in the understanding of the information diffusion phenomenon in social
media by providing means of modeling and analysis.

First, we propose MABED (Mention-Anomaly-Based Event Detection), a statistical
method for automatically detecting events that most interest social media users from
the stream of messages they publish. In contrast with existing methods, it doesn’t
only focus on the textual content of messages but also leverages the frequency of
social interactions that occur between users. MABED also differs from the literature in
that it dynamically estimates the period of time during which each event is discussed
rather than assuming a predeﬁned ﬁxed duration for all events. Secondly, we propose
T-BASIC (Time-Based ASynchronous Independent Cascades), a probabilistic model
based on the network structure underlying social media for predicting information
diffusion, more speciﬁcally the evolution of the number of users that relay a given
piece of information through time. In contrast with similar models that are also based
on the network structure, the probability that a piece of information propagate from
one user to another isn’t ﬁxed but depends on time. We also describe a procedure

i=a

Cette formulation permet à l’anomalie d’être négative en certains points de l’in-
tervalle, si et seulement si cela permet d’étendre l’intervalle tout en augmentant la
magnitude. C’est une propriété intéressante, puisque cela permet d’éviter la fragmen-
tation de longs évènements s’étendant sur plusieurs jours et dont l’anomalie associée
devient négative par exemple la nuit, du fait du faible niveau d’activité nocturne sur

63

3.3. Méthode proposée

le média social étudié. Une autre propriété intéressante de cette formulation est qu’un
mot donné ne peut être considéré comme le mot principal que d’un seul évènement.
Cela augmente la lisibilité des résultats pour la raison suivante. Plus le nombre d’évè-
nements pouvant être décrits par un même mot est grand, moins ce mot est spéciﬁque
à chaque évènement. Par conséquent, ce mot devrait plutôt être considéré comme un
mot lié que comme un mot principal. La ﬁgure 3.7 montre comment un évènement
lié au mot « kadhaﬁ » est détecté. On observe que l’anomalie négative au milieu de
l’intervalle identiﬁé est compensée par l’anomalie positive mesurée sur la deuxième
partie de l’intervalle.

Nous résolvons ce problème à l’aide de l’algorithme en temps linéaire décrit par
Bentley (1984). Finalement, chaque évènement détecté suivant ce processus est décrit
par : (i) un mot principal t, (ii) une période de temps I et (iii) la magnitude de son
impact sur le comportement des utilisateurs, Mag(t, I).

3.3.4 Sélection des mots décrivant les évènements

Partant du constat que les méthodes à base de clustering tendent à produire des
descriptions longues et bruitées, nous adoptons une approche différente, que nous
décrivons par la suite, avec pour objectif de produire des descriptions sémantiquement
plus claires.

Dans le but de limiter la surcharge informationnelle, nous choisissons de limiter
le nombre de mots utilisés pour décrire un évènement. Cette limite est un paramètre
ﬁxé manuellement noté p. Ce choix se justiﬁe par la brièveté des messages publiés sur
les médias sociaux. En effet, comme ces messages ne contiennent que peu de mots,
il ne semble pas rationnel qu’un évènement soit décrit par un grand nombre de mots
(Weng et Lee, 2011).

Identiﬁcation de mots candidats. L’ensemble des mots candidats pour décrire un
évènement est l’ensemble des mots avec les p plus fortes cooccurrences avec le mot t
durant la période de temps I. Les mots les plus pertinents sont sélectionnés parmi les
candidats selon la similarité entre leur dynamique temporelle et celle du mot t durant
l’intervalle I. Pour ce faire, nous calculons un poids wq pour chaque mot candidat
(cid:48)
q. Nous proposons d’estimer ce poids à partir des séries temporelles N i
t
et du
coefﬁcient de corrélation proposé par Erdem et al. (2012). Ce coefﬁcient – initialement

t et N i
t(cid:48)

q

64

Détecter les évènements

conçu pour analyser des données boursières, réputées non-stationnaires – possède
deux propriétés intéressantes pour notre application : (i) il est non-paramétrique et
(ii) il ne requiert pas d’hypothèse de stationnarité contrairement, par exemple, au
coefﬁcient de Pearson. Ce coefﬁcient prend en compte le décalage temporel aﬁn de
capturer au mieux la direction de la co-variation des deux séries temporelles au ﬁl
du temps. Par souci de concision, nous ne donnons ici que la formule permettant
(cid:48)
q et l’intervalle temporel I =
d’approximer ce coefﬁcient, étant donnés les mots t, t
[a; b] :

q

i=a+1

At,t(cid:48)

b(cid:88)
(b− a− 1)AtAt(cid:48)
− N i−1
(cid:80)b
(N i
(cid:80)b
b− a− 1
t
(N i
t(cid:48)
b− a− 1

)(N i
t(cid:48)
− N i−1
− N i−1
t(cid:48)

i=a+1

i=a+1

q

t

t

q

q

)2

,

)

q

− N i−1
t(cid:48)
)2

q

ρOt,t(cid:48)

q

=

où At,t(cid:48)

q

= (N i
t

=

A2
t

=

A2
t(cid:48)

q

relles N i

Cela correspond quasiment à l’auto-corrélation du premier ordre des séries tempo-
t et N i
. Erdem et al. (2012) fournissent la preuve que ρO satisfait la condition
t(cid:48)
|ρO| (cid:182) 1 en utilisant l’inégalité de Cauchy-Schwartz. Enﬁn, nous déﬁnissons le poids
(cid:48)
q comme une fonction afﬁne de ρO aﬁn de se conformer à notre déﬁnition
du mot t
de thématique saillante, i.e. 0 (cid:182) wq (cid:182) 1 :

q

wq =

+ 1

ρOt,t(cid:48)
q
2

Parce que la dynamique temporelle des mots toujours très fréquents est moins
impactée par un évènement particulier, cette formulation – d’une certaine manière

comme tf · idf – diminue le poids des mots généralement fréquents dans le ﬂux de

messages et augmente le poids des mots qui le sont moins, i.e. les mots plus spéci-
ﬁques.

Sélection des mots les plus pertinents. L’ensemble ﬁnal des mots retenus pour

décrire un évènement est l’ensemble S, tel que ∀t

∈ S, wq > θ. Les paramètres p et θ

(cid:48)
q

65

3.3. Méthode proposée

permettent à l’utilisateur de la méthode MABED d’ajuster la quantité et la granularité
de l’information dont il a besoin.

3.3.5 Génération de la liste des évènements

Chaque fois qu’un évènement a été traité par le second composant, il est passé au
troisième composant. Ce composant est chargé de sauvegarder la description des évè-
nements détectés tout en limitant la redondance (i.e. la duplication d’évènements).
Pour cela, il utilise deux structures de graphes : (i) le graphe des évènements et (ii)
le graphe des redondances. Le premier est un graphe orienté, pondéré et étiqueté qui
modélise les descriptions des évènements. La représentation d’un évènement e dans
ce graphe est comme suit. Un nœud représente le mot principal t et est étiqueté avec
(cid:48)
l’intervalle I et le score Mag. Chaque mot lié t
q est représenté par un nœud et possède
un arc dirigé vers le mot principal, dont le poids est égal à wq. La ﬁgure 3.8 montre un
graphe des évènements stockant deux évènements. Nous pouvons voir que l’un d’eux
a pour mot principal « kadhaﬁ » et qu’il y a quatre mots liés, à savoir « ﬁnancé », « cam-
pagne », « 2007 », « sarkozy ». La seconde structure est un simple graphe non-orienté
utilisé pour modéliser les éventuelles redondances entre les évènements détectés, où
un évènement est représenté par son mot principal.

Soit e1 l’évènement traité par ce composant. Tout d’abord, il vériﬁe s’il est redon-
dant avec un évènement déjà sauvegardé dans le graphe des évènements ou non. Si
ce n’est pas le cas, sa description est introduite dans le graphe des évènements et le
nombre d’évènements distincts détectés est incrémenté d’un. Dans le cas contraire,
en admettant que e1 est redondant avec l’évènement e0 déjà présent dans le graphe,
une arête est ajoutée dans le graphe des redondances entre les nœuds t1 et t0 (et
le nombre d’évènements détectés reste inchangé). Lorsque le nombre d’évènements
distincts détectés atteint k, le composant fusionne les évènements redondants et re-
tourne la liste contenant les k évènements aux plus fortes magnitudes d’impact. Par la
suite, nous décrivons comment les évènements redondants sont identiﬁés et comment
ils sont fusionnés.

Identiﬁcation des évènements redondants. L’évènement e1 est considéré comme
redondant avec l’évènement e0 déjà représenté dans le graphe des évènements si (i)
les mots principaux t1 et t0 seraient mutuellement connectés et (ii) si le coefﬁcient de

66

Détecter les évènements

FIGURE 3.8 – Un graphe des évènements stockant deux évènements. Les mots princi-
paux sont représentés par les nœuds blancs avec des bordures noires.

min(I1,I0) et le seuil est noté σ, σ ∈]0; 1].

recouvrement des deux périodes de temps I1 et I0 dépasse un seuil ﬁxe. Le coefﬁcient
de recouvrement est déﬁni comme r(I0, I1) = |I1∩I0|
Dans ce cas, la description de l’évènement e1 est sauvegardée en marge et une relation
est ajoutée entre t1 et t0 dans le graphe des redondances. La partie gauche de la ﬁgure
3.9 montre la représentation graphique de l’évènement ﬁctif e0, dont le mot principal
est noté A. Sur la partie droite de cette même ﬁgure, on observe la structure du graphe
des évènements si l’on insérait un évènement ﬁctif e1, dont les éléments sont tracés
en pointillés et dont le mot principal est noté D. Il apparaît que le mot D est un mot
lié à l’évènement e0 et que A est un mot lié à l’évènement e1. Par conséquent, si la
condition r(I0, I1) > σ est vériﬁée, l’évènement e1 est jugé redondant avec e0 et n’est
pas inséré dans le graphe des évènements.

Fusion des évènements redondants. Identiﬁer quels évènements redondants
doivent être fusionnés ensemble revient à identiﬁer les composantes connexes dans
le graphe des redondances. Cela se fait en temps linéaire à l’aide de l’algorithme
décrit par Hopcroft et Tarjan (1973). Dans chaque composante connexe se trouve
exactement un nœud correspondant à un évènement représenté dans le graphe des

67

3.3. Méthode proposée

FIGURE 3.9 – Identiﬁcation de la redondance entre deux évènements ﬁctifs e0 et e1.

évènements. Sa magnitude d’impact (qui est nécessairement supérieure à celles des
évènements redondants, du fait du tri réalisé à la ﬁn de la première phase) et son
intervalle temporel restent inchangés, par contre, sa description textuelle est enrichie
selon le principe suivant. La thématique décrivant cet évènement est mise à jour selon
les informations supplémentaires apportées par les descriptions des évènements re-
dondants. Le mot principal devient l’agrégation des mots principaux des évènements
redondants. Les mots liés décrivant l’évènement mis à jour sont les p mots parmi tous
les mots liés des évènements redondants avec les p plus grands poids. En reprenant
l’exemple utilisé à la ﬁgure 3.9, on fusionnerait les évènements e0 et e1 comme sur la
ﬁgure 3.10. Le terme principal de l’évènement e0 devient l’agrégation des mots A et
D, ce dernier étant le mot principal de l’évènement e1. Aussi, en supposant que p > 3,
F est ajouté comme mot lié à l’évènement e0.

3.3.6 Algorithme général

Pour conclure cette section, nous donnons l’enchaînement des étapes que nous

venons de décrire avec l’algorithme 1 (page 69).

68

Détecter les évènements

Algorithme 1 : Déroulement général de la méthode MABED.

Données : Un corpus (cid:67) de messages partitionné en n tranches temporelles, le

vocabulaire correspondant V@

Paramètres : nombre d’évènements k, limite de mots liés p, poids minimal des

mots liés θ ∈ [0; 1], seuil de recouvrement temporel pour la fusion σ ∈]0; 1]

Résultat : Une liste ordonnée L contenant les k évènements au plus fort impact
/* Phase 1
*/
Initialiser la pile P utilisée pour stocker les évènements lors de la phase 1;

pour chaque mot t ∈ V@ faire
Mag(t, I) = max{(cid:80)b
Identiﬁer l’intervalle I = [a; b] tel que :
i=a anomalie(t, i)|1 (cid:182) a (cid:182) b (cid:182) n};
Ajouter l’évènement e = [t,(cid:59), I, Mag(t, I)] à la pile P;

ﬁn
Trier la pile d’évènements P par ordre décroissant de magnitude d’impact;
/* Phase 2
Initialiser le graphe des évènements GE et le graphe des redondances GR;
Initialiser la variable compteur à 0;
tant que compteur < k et |P| > 0 faire

*/

Dépiler l’évènement e au sommet de la pile P;
Sélectionner les mots liés à l’évènement e, avec la limite p et le seuil θ;
si e est redondant avec un évènement e

présent dans GE pour le seuil σ alors

(cid:48)

Ajouter une relation entre e et l’évènement e
Sauvegarder la description de e en marge;

(cid:48)

dans GR;

sinon

Insérer la description de l’évènement e dans le graphe GE;
Incrémenter la variable compteur;

ﬁn

ﬁn
Identiﬁer les évènements à fusionner à partir de GR puis mettre à jour GE;
Transformer le graphe GE en la liste d’évènements L;
Trier la liste L par ordre décroissant de magnitude d’impact;
retourner L;

69

3.4. Expérimentations

FIGURE 3.10 – Résultat de la fusion entre les évènements ﬁctifs e0 et e1.

3.4 Expérimentations

Dans cette section, nous présentons la synthèse des résultats obtenus lors de
l’étude expérimentale que nous avons menée à l’aide de donnée issues de Twitter pour
évaluer MABED. Tout d’abord, à travers une évaluation quantitative, nous démontrons
la pertinence de l’approche basée sur la mesure de l’anomalie dans la fréquence de
création de mentions, et nous mesurons les performances de MABED par rapport à
plusieurs méthodes de la littérature. Pour évaluer la précision et le rappel, nous avons
demandé à des annotateurs humains de juger si les évènements détectés sont com-
préhensibles et signiﬁcatifs. Lors de l’évaluation qualitative, nous montrons que les
descriptions d’évènements détectés par MABED sont plus précises, temporellement et
sémantiquement, que celles extraites par les méthodes existantes, ce qui favorise une
compréhension aisée des résultats.

3.4.1 Protocole expérimental

Corpus. Étant donné que les corpus utilisés par les auteurs des méthodes exis-
tantes ne sont pas accessibles, nous basons nos expérimentations sur deux corpus

différents. Le premier corpus – noté (cid:67)en – contient 1 437 126 tweets rédigés en an-

glais, collectés avec une stratégie centrée-utilisateur par Yang et Leskovec (2011). Ils

70

Détecter les évènements

TABLE 3.4 – Statistiques sur les corpus (@ : proportion de tweets qui contiennent des
mentions, RT : proportion de retweets).

Corpus # tweets # auteurs @

(cid:67)en
(cid:67) f r

1 437 126
2 086 136

52 494
150 209

RT
0,54 0,17
0,68 0,43

correspondent à l’intégralité des tweets publiés durant le mois de novembre 2009 par
52 494 utilisateurs américains de Twitter. Ce corpus contient beaucoup de bruit. Se-
lon l’étude menée par PearAnalytics (2009), la proportion de tweets sans rapport avec

aucun évènement pourrait atteindre 50%. Le second corpus – noté (cid:67) f r – contient

2 086 136 de tweets rédigés en français collectés avec une stratégie centrée-mots-clés
en mars 2012, durant la campagne pour l’élection présidentielle. Nous avons obtenu
ces tweets via l’API streaming de Twitter, en utilisant les noms des principaux can-
didats comme mots-clés. Ce corpus cible donc les thématiques politiques en rapport
avec la France. Les mots triviaux sont retirés des messages à l’aide de listes de mots
vides francophone et anglophone. Les dates de publication des messages sont au for-
mat UTC. La table 3.4 donne des détails supplémentaires à propos de chaque corpus.
Méthodes comparées. Nous considérons d’une part une variante de la méthode
MABED – α-MABED – qui ignore la présence ou l’absence de mentions dans les mes-
sages. Cela signiﬁe que le premier composant détecte les évènements et estime la
magnitude de leur impact à partir des valeurs de N i
@t. Nous considérons
d’autre part deux méthodes récentes tirées de la littérature, que nous avons décrites
dans la section consacrée à l’état de l’art (3.2), à savoir : TS et ET. TS est une méthode
de pondération statistique des N-grammes développée par Benhardus et Kalita (2013).
Nous appliquons cette méthode aux bigrammes (TS2) et aux trigrammes (TS3). ET
(Parikh et Karlapalem, 2013) crée des clusters de bigrammes en réalisant une classiﬁ-
cation ascendante hiérarchique basée sur la similarité temporelle et de contenu entre
bigrammes. Faute d’avoir pu obtenir l’implémentation de ces méthodes auprès des
auteurs, nous les avons réimplémentées en Java. Nous avons également implémenté
la méthode EDCoW (Weng et Lee, 2011), néanmoins nous avons décidé de ne pas inté-
grer les résultats obtenus dans la suite de cette section, ceux-ci étant inexploitables –
chaque évènement étant décrit par un grand nombre de mots sans rapport entre eux.

t au lieu de N i

71

3.4. Expérimentations

Il faut noter que Valkanas et Gunopulos (2013) ont également réimplémenté cette mé-
thode et ont obtenu des résultats médiocres en l’appliquant à des données extraites
de Twitter, puisque la précision qu’ils mesurent est de 1/588. Nous excluons égale-
ment la comparaison avec les méthodes basées sur la modélisation des thématiques
latentes, du fait des temps de calcul prohibitifs qui les rendent pratiquement inutili-
sables. À titre d’exemple, l’implémentation parallélisée d’On-line LDA fournie par Lau
et al. (2012) 4 ne parvient à traiter que la moitié des messages contenus dans le cor-

pus (cid:67)en (autrement dit les messages publiés durant les 15 premiers jours du mois de

novembre 2009, environ 700 000) après un mois de calcul monopolisant totalement
les ressources de la machine utilisée.

Choix des paramètres. Pour MABED et α-MABED nous partitionnons les deux
corpus en tranches temporelles de 30 minutes, ce qui permet une bonne précision
temporelle tout en maintenant un nombre de tweets sufﬁsant en chaque tranche
temporelle. Les paramètres p et θ – respectivement le nombre maximum de mots
décrivant chaque évènement et le poids minimum des mots liés – permettent aux uti-
lisateurs de MABED de déﬁnir le niveau de détail requis. Étant donné que les messages
publiés sur Twitter contiennent en moyenne 10, 7 mots par phrase d’après l’étude me-
née par Oxford (2009), nous ﬁxons p = 10. Pour les besoins de cette évaluation, nous
ﬁxons θ = 0, 7 de telle sorte que seuls les mots spéciﬁques à chaque évènement soient
présentés aux annotateurs. Il y a un paramètre qui peut affecter les performances de
MABED, à savoir σ, le seuil de recouvrement temporel des évènements à fusioner. Par
la suite, nous présentons les résultats pour σ = 0, 5 (nous examinons l’impact de σ
dans la section 3.4.2).

Pour ET et TS, puisque ces méthodes supposent une durée ﬁxée commune à tous
les évènements – qui correspond à la durée d’une tranche temporelle – nous partition-
nons les corpus par tranches de 24 heures, comme cela est typiquement fait dans la
littérature. ET a deux paramètres, pour lesquels nous utilisons les valeurs optimales
indiquées par les auteurs.

Métriques d’évaluation. En l’absence de vérité terrain, nous avons demandé à
deux annotateurs humains 5 de juger si les évènements détectés sont compréhensibles
et signiﬁcatifs, en attribuant à chaque évènement une note de 0 (i.e. non signiﬁcatif)
4. L’implémentation est téléchargeable à l’adresse suivante : http://ww2.cs.mu.oz.au/~tim/etc/

online_lda.zip.

5. Les annotateurs sont des étudiants de master et ne sont pas autrement impliqués dans ce projet.

72

Détecter les évènements

ou 1 (i.e. signiﬁcatif). Un évènement est considéré comme étant signiﬁcatif s’il pour-
rait être repris par les médias traditionnels (e.g. via la publication d’un article à son
sujet). Globalement, un évènement détecté est signiﬁcatif si les deux annotateurs lui
ont attribué la note de 1. Les annotateurs indiquent également si un évènement est
redondant avec un évènement qu’ils ont précédemment annoté dans la même liste.
Étant donné que chaque corpus couvre une période d’un mois et que l’annotation
des évènements détectés est une tâche chronophage, nous limitons l’évaluation aux
40 évènements les plus impactants détectés par chaque méthode (i.e. k = 40) dans
chaque corpus. Nous mesurons la précision comme la fraction d’évènements détectés
notés 1 par les deux annotateurs, c’est-à-dire :

P = nombre d’évènements détectés signiﬁcatifs

k

Nous mesurons le rappel comme la fraction d’évènements signiﬁcatifs distincts

parmi tous les évènements détectés, de la même façon que Li et al. (2012) :

R = nombre d’évènements détectés signiﬁcatifs− nombre d’évènements redondants

k

Nous mesurons également le DERate, qui correspond à la proportion d’évènements
redondants parmi tous les évènements signiﬁcatifs détectés (Li et al., 2012), autre-
ment dit :

DERate =

nombre d’évènements redondants

nombre d’évènements détectés signiﬁcatifs

3.4.2 Évaluation quantitative

Par la suite, nous examinons les performances des différentes méthodes sur la
base des notes données par les annotateurs. L’accord inter-annotateurs mesuré selon
le Kappa de Cohen vaut κ = 0, 76, ce qui dénote un fort accord. La table 3.5 (page 75)
reporte la précision, la F-mesure déﬁnie comme la moyenne harmonique entre préci-
sion et rappel, le DERate et le temps de calcul (temps moyen pour trois exécutions)
de chaque méthode pour chaque corpus.

Performances de MABED par rapport à celles des méthodes de référence. La

73

3.4. Expérimentations

table 3.5 (page 75) indique que MABED obtient les meilleures performances concer-
nant la précision et la F-mesure, avec une précision de 0,775 pour une F-mesure de

0,682 sur le corpus (cid:67)en, et une précision de 0,825 pour une F-mesure de 0,825 sur le
corpus (cid:67) f r. Par rapport à α-MABED, nous observons un gain relatif moyen concernant

la F-mesure de 17,2%. Cela vériﬁe de manière empirique notre principale hypothèse,
dans le cadre de Twitter, à savoir que la prise en compte de la fréquence de création de
mentions liées aux évènements conduit à une détection plus précise des évènements
signiﬁcatifs. Globalement, nous observons que MABED surpasse les méthodes de réfé-

rence avec une marge plus importante du point de vue de la F-mesure sur (cid:67)en que sur
(cid:67) f r. Or, (cid:67)en contient beaucoup plus de bruit que (cid:67) f r. Cela suggère que prendre en

compte le comportement des utilisateurs des médias sociaux en matière de création
de mentions permet une détection plus robuste des évènements à partir d’un ﬂux de
tweets bruité. Le DERate révèle que MABED n’a dédoublé aucun évènement signiﬁ-

catif parmi ceux détectés dans (cid:67) f r, mais que – en dépit de la gestion explicite de la
redondance par le troisième composant – 6 (DERate = 0, 193) des 31 (P = 0, 775)
évènements signiﬁcatifs détectés dans (cid:67)en sont redondants. Ce DERate reste toutefois

inférieur à celui mesuré pour les méthodes TS2 ou TS3, et MABED obtient néanmoins
le meilleur rappel sur ce corpus.

Explication de la performance de MABED. Il apparaît que les évènements signi-
ﬁcatifs détectés par les méthodes de référence sont un sous-ensemble de ceux détectés
par MABED. L’analyse plus approfondie des résultats d’α-MABED, TS2 et TS3 révèle
que la plupart des évènements jugés non-signiﬁcatifs sont aisément assimilables à du
spam. La non-détection de ces évènements non-signiﬁcatifs par MABED suggère que
la prise en compte des mentions limite la sensibilité au spam, ce qui expliquerait en

partie l’amélioration plus importante de la F-mesure de MABED sur (cid:67)en que (cid:67) f r par

rapport aux méthodes de référence. En ce qui concerne ET, nous remarquons que
la longueur moyenne des descriptions des évènements est de 17,25 bigrammes (i.e.
plus de 30 mots). Nous constatons que les descriptions des évènements détectés par
cette méthode à base de classiﬁcation non supervisée sont bruitées. Les descriptions
des évènements non-signiﬁcatifs sont essentiellement des ensembles de bigrammes
sans rapport les uns avec les autres, dont il est impossible d’extraire le moindre sens.
Comme le font remarquer Valkanas et Gunopulos (2013), cela s’explique par une stra-
tégie de regroupement des termes trop aggressive.

74

Détecter les évènements

TABLE 3.5 – Performances des cinq méthodes sur les deux corpus.

Méthode Précision F-mesure DERate Temps de calcul

Corpus : (cid:67)en

MABED
α-MABED
ET
TS2
TS3

0,775
0,625
0,575
0,600
0,375

0,682
0,571
0,575
0,514
0,281

0,193
0,160

0

0,250
0,4

96s
126s
3480s
80s
82s

Méthode Précision F-mesure DERate Temps de calcul

Corpus : (cid:67) f r

MABED
α-MABED
ET
TS2
TS3

0,825
0,725
0,700
0,725
0,700

0,825
0,712
0.674
0,671
0,616

0

0,025
0,071
0,138
0,214

88s
113s
4620s
69s
74s

Efﬁcacité. En ce qui concerne l’efﬁcacité, il apparaît que les temps de calcul de
MABED sont du même ordre que ceux de TS, tandis que les temps de calcul d’ET sont
notoirement plus longs. Nous remarquons également que MABED est plus rapide que

α-MABED. La principale raison à cela est que |V@| (cid:182) |V|, ce qui accélère la première

phase du traitement. Il est à noter que les temps indiqués dans la table 3.5 n’incluent
pas le temps nécessaire à la préparation des données, c’est-à-dire préparer les voca-
bulaires et indexer les messages pour extraire les fréquences des termes.

Impact de σ sur MABED. Lors de la construction de la liste d’évènements par MA-
BED, le seuil de recouvrement σ contrôle la sensibilité aux évènements redondants.
La ﬁgure 3.11 donne la précision, la F-mesure et le DERate obtenus par MABED sur

le corpus (cid:67)en pour des valeurs de σ allant de 0, 2 à 1 par pas de 0, 1. On peut ob-

server que la valeur de σ agit principalement sur le DERate. Plus spéciﬁquement, le
DERate augmente avec σ, puisque de moins en moins d’évènements redondants sont
fusionnés. Pour σ = 1, la précision augmente à 0, 825 du fait de la proportion impor-
tante d’évènements signiﬁcatifs redondants. Globalement, il apparaît que la meilleure
F-mesure est atteinte pour des valeurs de σ allant de 0, 2 à 0, 5. Malgré cela, même
en ﬁxant σ = 1, MABED atteint une F-mesure de 0, 582, ce qui est supérieur à la

75

3.4. Expérimentations

e
t
a
R
E
D
/

e
r
u
s
e
m
-
F

/
n
o
i
s
i
c
é
r
P

0, 8

0, 6

0, 4

0, 2

Précision
F-mesure
DERate

0, 2 0, 3 0, 4 0, 5 0, 6 0, 7 0, 8 0, 9

1

FIGURE 3.11 – Précision, F-mesure et DERate de MABED sur le corpus (cid:67)en pour diffé-

rentes valeurs de σ.

Valeur de σ

F-mesure obtenue avec les méthodes comparées.

3.4.3 Évaluation qualitative

Maintenant, nous analysons d’un point de vue qualitatif les résultats de MABED
et montrons en quoi la méthode fournit des informations pertinentes à propos des
évènements détectés. La table 3.6 (page 77) liste les 20 évènements avec les plus

fortes magnitudes d’impact sur les utilisateurs dans (cid:67)en. Nous basant sur cette liste,

nous faisons plusieurs observations ayant trait à trois aspects : lisibilité, précision
temporelle et redondance.

Lisibilité. La mise en avant des mots principaux facilite la lecture des descriptions
des évènements, d’autant plus qu’ils correspondent souvent à des entités nommées,
e.g. Fort Hood (évènement #6), Chrome (évènement #7), Tiger Woods (évènement
#8), Obama (évènement #13). La distinction entre mots principaux et mots liés fa-
vorise la compréhension rapide des évènements en mettant en lumière les person-
nages/produits/lieux clés, ce que les méthodes existantes ne permettent pas direc-
tement. Qui plus est, la pondération des mots liés et la limitation de leur nombre
améliore la lisibilité des résultats, notamment en comparaison avec les méthodes à

76

Détecter les évènements

TABLE 3.6 – Liste des 20 évènements ayant eu le plus fort impact sur les utilisateurs,

détectés par MABED à partir du corpus (cid:67)en. Les mots principaux sont en gras et le

poids de chaque mot lié est donné entre parenthèses. Les intervalles temporels sont
exprimés en temps UTC.

# Intervalle

healthcare, reform : house (0.91), bill (0.88), passes (0.83), vote (0.83)
Lié à l’évènement #5
facebook : app (0.74), twitter (0.73)
Pas d’évènement correspondant

77

chrome : os (0.95), google (0.87), desktop (0.71)

tiger, woods : accident (0.91), car (0.88), crash (0.88), injured (0.80)
Tiger Woods est victime d’un accident de la route
tweetie, 2.1, app : retweets (0.93), store (0.90), native (0.89), geotagging (0.88)
L’application Tweetie sort sur l’apple store et inclut de nouvelles fonctions, e.g. retweets

Thématique
thanksgiving, turkey : hope (0.72), happy (0.71)
Les twittos célèbrent Thanksgiving
thankful : happy (0.77), thanksgiving (0.71)
Lié à l’évènement # 1
veterans : served (0.80), country (0.78), military (0.73), happy (0.72)
Commémoration du 11 novembre, « Veterans Day »
black : friday (0.95), amazon (0.75)
Les twittos discutent des offres proposées par Amazon la veille du « Black Friday »
hcr, bill, health, house, vote : reform (0.92), passed (0.91), passes (0.88)
La Chambre des représentants des États-Unis adopte la réforme de santé
hood, fort : ft (0.92), shooting (0.83), news (0.78), army (0.75), forthood (0.73)

du 25 09h30
au 28 06h30
du 25 09h30
au 27 09h00
du 10 16h00
au 12 08h00
du 26 13h00
au 28 10h30
du 07 13h30
au 09 04h30
du 05 19h30
au 08 09h00 Une fusillade a lieu dans l’enceinte de la base militaire américaine de Fort Hood
du 19 04h30
au 21 02h30 Google rend public le code source de Chrome OS pour PC
du 27 18h00
au 29 05h00
du 28 22h30
au 30 23h30
du 29 17h00 monday, cyber : deals (0.84), pro (0.75)
au 30 23h30
du 10 01h00
au 12 03h00
du 04 17h00
au 06 05h30
du 15 09h00
au 17 23h30
du 25 10h00
au 26 10h00
du 19 21h30
au 21 16h00 Oprah Winfrey annonce la ﬁn de son talk-show en septembre 2011
du 07 11h30
au 09 05h00
du 11 03h30
au 13 08h30
du 18 14h00 whats : happening (0.76), twitter (0.73)
au 21 03h00
du 20 10h00
au 22 00h00
du 26 08h00
au 26 15h00

Les twittos partagent les offres commerciales high-tech du « Cyber Monday »
linkedin : synced (0.86), updates (0.84), status (0.83), twitter (0.71)
Linkedin permet à ses utilisateurs de synchroniser leurs statuts avec Twitter
yankees, series : win (0.84), won (0.84), fans (0.78), phillies (0.73), york (0.72)
Les Yankees, l’équipe de baseball de New York remporte la World Series face aux Philies
obama : chinese (0.75), barack (0.72), twitter (0.72), china (0.70)
Lors d’une visite en Chine, Barack Obama admet n’avoir jamais utilisé Twitter
holiday : shopping (0.72)
Les twittos réagissent par rapport au « Black Friday », un jour férié dédié au shopping
oprah, end : talk (0.81), show (0.79), 2011 (0.73), winfrey (0.71)

Twitter demande maintenant « What’s happening ? » et plus « What are you doing ? »
cern : lhc (0.86), beam (0.79)
Les faisceaux de particules circulent à nouveau dans l’accélérateur LHC du CERN
icom : lisbon (0.99), roundtable (0.98), national (0.88)
Tenue de la table ronde de l’ICOM à propos des marchés ﬁnanciers portugais

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

3.4. Expérimentations

base de clustering qui ont tendance à extraire des descriptions très longues.

Précision temporelle. MABED estime dynamiquement la période de temps du-
rant laquelle chaque évènement est discuté. Cela améliore la précision temporelle par
rapport aux méthodes existantes qui ﬁxe typiquement la durée des évènements à une
journée. Nous illustrons en quoi cela améliore la qualité des résultats avec l’exemple
suivant. Le 6ème évènement de la table 3.6 correspond aux utilisateurs de Twitter si-
gnalant la fusillade qui a eu lieu à la base militaire américaine de Fort Hood le 5
Novembre 2009 entre 13h34 et 13h44 CST 6 (i.e. 19h34 et 19h44 UTC). Le pic d’ac-
tivité engendré par cet évènement est détecté par MABED dès la tranche temporelle
couvrant la période 19h30-20h00 UTC du 5 novembre. MABED donne la description
suivante :

(i) du 05/11 19h30 au 08/11 9h00 (UTC) ; (ii) hood, fort ; (iii) ft (0,92), shooting

(0,83), news (0,78), army (0,75), forthood (0,73).

À la lecture de cette description, nous pouvons clairement comprendre (i) qu’il
s’est passé quelque chose aux alentours de 19h30 UTC le 5 novembre, (ii) à la base
de Fort Hood et (iii) qu’il s’agit d’une fusillade (i.e. shooting). Par contraste, α-MABED
ne détecte cet évènement que le 7 novembre, lorsque la couverture médiatique était
la plus importante.

Redondance. Certains évènements sont associés à plusieurs mots principaux. C’est
le cas des évènements #1 (Thanksgiving, turkey), 5 (HCR, bill, health, house, vote),
6 (Hood, Fort) et 8 (Tiger, Woods) entre autres. Ceci est dû aux fusions opérées par
le troisième composant de la méthode MABED pour éliminer les duplicata. La re-
dondance est également limitée grâce à l’estimation dynamique de la période du-
rant laquelle chaque évènement est discuté. Nous illustrons cela toujours à l’aide du
même exemple, à savoir l’évènement #6. La ﬁgure 3.12 (page 79) montre l’évolu-
tion de l’anomalie mesurée pour les mots « hood », « fort » et « shooting » entre le 5
et le 7 novembre 2009. Nous pouvons voir que l’anomalie est proche de 0 durant la
nuit, donnant une allure à « deux pics » à la courbe. Néanmoins, MABED détecte un
unique évènement discuté pendant plusieurs jours, plutôt que de reporter deux évène-
ments distincts, durant chacun une journée. L’importance de l’estimation dynamique
de la durée durant laquelle chaque évènement est discuté est renforcée par la ﬁgure
3.13 (page 80), qui montre la distribution de la durée des évènements détectés dans

6. http://en.wikipedia.org/wiki/Fort_Hood_shooting

78

Détecter les évènements

max

e
i
l
a
m
o
n
A

0
5 nov.

« hood »
« fort »

« shooting »

#6 (13h30)

6 nov.

Temps (CST)

7 nov.

FIGURE 3.12 – Anomalie mesurée pour les mots « hood », « fort » et « shooting » du 5
au 7 novembre à minuit (CST).

chaque corpus. Nous observons que certains évènements sont discutés pendant moins
de 12 heures (ce qui est le cas uniquement pour 3 évènements détectés à partir de

(cid:67)en), tandis que d’autres sont discutés pendant plus de 60 heures. Qui plus est, nous
notons que les durées des évènements détectés à partir de (cid:67) f r sont normalement dis-
tribuées – au sens du test de Shapiro et Wilk (1965), avec W = 0.92, une p-valeur de
0.54 et un seuil de signiﬁcation à 0.05 – et ont tendance à être plus longues que celles
des évènements détectés à partir de (cid:67)en. Autrement-dit, les évènements en lien avec

la campagne électorale ont tendance à animer les discussions plus longtemps. Cela
coïncide avec les résultats de l’étude empirique menée par Romero et al. (2011), qui
indiquent que les thématiques controversées et notamment les thématiques politiques
persistent plus longtemps sur Twitter que les autres.

3.5 Implémentation et visualisations

L’implémentation de la méthode MABED est disponible sur internet 7 et est éga-
lement incluse dans le logiciel SONDY, qui sera détaillé ultérieurement dans ce ma-
nuscrit. Pour permettre une exploration efﬁcace des évènements détectés, l’implé-

7. http ://mediamining.univ-lyon2.fr/people/guille/mabed.php

79

3.5. Implémentation et visualisations

(cid:67)en(cid:67) f r

<12

[12;24] ]24;36] ]36;48] ]48;60] >60
Durée des évènements (en heures)

’

s
t
n
e
m
e
n
è
v
é
d
e
g
a
t
n
e
c
r
u
o
P

0, 4

0, 3

0, 2

0, 1

0

FIGURE 3.13 – Distribution de la durée des évènements détectés par MABED.

mentation de MABED propose trois visualisations que nous décrivons ci-après. Nous
proposons une interface pour chacune des dimensions caractérisant les évènements :
temps, impact et thématique.

Temps. La ﬁgure 3.14 montre une capture d’écran de l’interface centrée-temps,
qui est une frise chronologique. Le ruban dans la partie inférieure permet la navi-
gation à travers le temps tandis que la partie supérieure donne des détails à propos
de l’évènement sélectionné. La description de chaque évènement est enrichie d’une
image et d’une URL. Ces deux éléments correspondent à la première image et à la
première URL renvoyées par le moteur de recherche Bing, en utilisant les descrip-
tions des évènements extraites à partir des messages par MABED comme requêtes. La
frise chronologique reprise par la ﬁgure 3.14 a été générée à partir des évènements

détectés dans (cid:67) f r.

Impact. La ﬁgure 3.15 montre une capture d’écran de l’interface centrée-impact.
Elle consiste en un graphique interactif qui permet de visualiser et comparer l’évo-
lution dans le temps de l’impact des évènements sur les utilisateurs du média social
étudié. Ce graphique a été généré à partir des cinq évènements au plus fort impact
détectés durant la journée du 30 mai 2014 par la version en ligne de MABED qui
analyse en permanence les tweets francophones mentionnant François Hollande.

80

Détecter les évènements

FIGURE 3.14 – Capture d’écran de l’interface centrée-temps. Elle se décompose en
deux parties : la partie inférieure permet la navigation à travers le temps tandis que
la partie supérieure donne des détails à propos des évènements.

FIGURE 3.15 – Capture d’écran de l’interface centrée-impact. Chaque évènement est
associé à une couleur.

81

3.6. Discussion

FIGURE 3.16 – Capture d’écran de l’interface centrée-thématique. Les nœuds gris cor-
respondent aux mots principaux. Leur diamètre est proportionnel à l’impact des évè-
nements.

Thématique. La ﬁgure 3.16 montre une capture d’écran de l’interface centrée-
thématique. Elle permet de naviguer dans le graphe des évènements construit par
le troisième composant de MABED, ce qui facilite l’identiﬁcation des évènements
proches. Pour simpliﬁer la lecture du graphe, les labels des nœuds ne s’afﬁchent que
sur demande. La ﬁgure 3.16 montre un extrait du graphe construit à partir du corpus

(cid:67) f r. Les nœuds bleutés en haut à gauche et à droite de la fenêtre correspondent res-

pectivement à François Hollande et Nicolas Sarkozy, i.e. les deux principaux candidats
à la présidence en 2012.

3.6 Discussion

Pour conclure ce chapitre, nous résumons dans un premier temps les travaux que
nous venons de présenter. Ensuite, nous présentons la principale piste de recherche
que nous comptons explorer dans nos futurs travaux.

82

Détecter les évènements

3.6.1 Résumé des travaux présentés

Dans ce chapitre nous avons décrit MABED, une nouvelle méthode efﬁcace re-
posant sur la mesure de l’anomalie dans la fréquence de création de mentions pour
détecter les évènements dans les médias sociaux. Contrairement aux méthodes de la
littérature, MABED prend en compte l’aspect social des ﬂux de messages à travers les
mentions que les utilisateurs insèrent dans leurs messages pour engager la discussion
avec d’autres. Par ailleurs, notre approche diffère des travaux existants en ce qu’elle
estime dynamiquement la période de temps durant laquelle chaque évènement est
discuté, plutôt que de supposer une durée ﬁxée et commune à tous les évènements
détectés. Les expérimentations que nous avons menées sur deux jeux de données col-
lectés sur Twitter ont démontré la pertinence de notre approche. Quantitativement
parlant, MABED a obtenu de meilleurs résultats que α-MABED – une variante qui
ignore la présence ou l’absence de mentions dans les messages – pour tous les tests
que nous avons menés, et surpasse également deux méthodes récentes tirées de la lit-
térature. Nous avons ainsi pu valider empiriquement – sur Twitter – notre hypothèse
de départ, à savoir que la prise en compte des mentions conduit à une détection plus
précise des évènements signiﬁcatifs. Les résultats obtenus suggèrent également que
la prise en compte des mentions accroît la robustesse de la détection en présence de
données très bruitées. Qualitativement parlant, nous avons montré que la distinction
entre mots principaux et mots liés augmente la lisibilité des descriptions des évène-
ments. Nous avons également mis en évidence l’intérêt des informations temporelles
fournies par MABED. D’une part, elles permettent de savoir précisément quand les
évènements se sont produits. D’autre part, l’estimation dynamique de la période tem-
porelle durant laquelle chaque évènement est discuté par les utilisateurs permet de
limiter la fragmentation et la duplication des évènements détectés.

Impact. Ces travaux ont notamment fait l’objet d’un article long, présenté à la

conférence internationale IEEE/ACM ASONAM en 2014.

3.6.2 Perspectives de travail

Partant de l’intuition que les communautés d’utilisateurs au sens social – c’est-
à-dire les communautés identiﬁables à partir de la structure du réseau social que
forment les utilisateurs d’un média social – sont similaires aux communautés d’utili-

83

3.6. Discussion

sateurs au sens thématique – c’est-à-dire les communautés identiﬁables à partir des
évènements à propos desquels les utilisateurs réagissent, une perspective de travail
consisterait à explorer la complémentarité entre la tâche de détection d’évènements à
partir des messages et la tâche de détection de communautés d’utilisateurs à partir de
la structure du réseau social. Par exemple, il serait intéressant d’exploiter la détection
d’évènements pour évaluer la pertinence des communautés détectées à partir de la
structure du réseau social, voire même pour améliorer la pertinence des communau-
tés détectées.

Pour justiﬁer l’intérêt de cette piste de recherche, nous présentons dans la suite
de cette section quelques résultats préliminaires. Ces résultats se basent d’une part

sur les évènements détectés à partir du corpus de messages (cid:67)en, et d’autre part, sur
Le corpus (cid:67)en – déjà utilisé dans la section précédente – contient 1 437 126 tweets

le réseau social formé par les liens d’abonnement entre les auteurs des messages.

publiés en novembre 2009 par 52 494 utilisateurs. Le réseau social interconnectant
ces utilisateurs comprend 5 793 961 liens d’abonnements, lesquels ont été collectés
ﬁn 2009 par Kwak et al. (2010).

Identiﬁcation des communautés à partir de la structure du réseau social. Pour
ce faire, nous utilisons la méthode de Louvain (Blondel et al., 2008), couramment
utilisée pour détecter les communautés dans les grands graphes et en particulier les
réseaux sociaux. Cette méthode repose sur une heuristique d’optimisation de l’indice
de modularité. Cet indice déﬁni par Newman (2006) mesure la différence entre la
densité d’une instance de graphe et la densité d’un graphe aléatoire possédant la
même distribution des degrés. La méthode de Louvain identiﬁe deux communautés :
c0, qui contient 25 625 membres et c1, qui contient 26 869 membres.
lisateurs en deux communautés nous amène à former deux sous-corpus, (cid:67)en(c0), qui
contient 479 899 messages, et(cid:67)en(c1), qui contient 932 699 messages. Ces corpus cor-

Détection des évènements à partir des messages. Le partitionnement des uti-

respondent respectivement aux messages publiés par les membres des communautés
c0 et c1. Nous utilisons la méthode MABED pour détecter dans chaque corpus les 10
évènements ayant eu le plus fort impact sur les membres de chaque communauté, ce
qui conduit à l’élaboration de deux listes d’évènements, L0 et L1, qui correspondent
respectivement aux évènements détectés à partir de (cid:67)en(c0) et de (cid:67)en(c1).

Analyse des évènements détectés par communauté. Nous associons tout d’abord

84

Détecter les évènements

manuellement chaque évènement détecté à une des catégories proposées par McMinn
et al. (2013) pour classiﬁer les évènements sur Twitter, à savoir :

— Science et technologie ;
— Justice, politique et scandales ;
— Business et économie ;
— Art, culture et divertissement ;
— Catastrophes et accidents ;
— Sport ;
— Conﬂits armés et attaques ;
— Divers.
Ensuite, du fait de la différence importante entre les volumes de messages conte-

nus dans (cid:67)en(c0) et (cid:67)en(c1), nous proposons non pas de comparer directement les

deux listes d’évènements sur la base de la magnitude d’impact des évènements par
catégorie, mais plutôt en fonction de la position des évènements dans chaque liste.
À cette ﬁn, nous déﬁnissons le poids d’un évènement comme une fonction afﬁne de

sa position dans la liste : 1− ((position− 1)× 0, 1). Ainsi le premier évènement a un

poids de 1, tandis que le dernier évènement de la liste a un poids de 0, 1. Enﬁn, pour
chaque catégorie, nous sommons les poids des évènements associés.

La ﬁgure 3.17.a (page 87) donne la distribution des poids des catégories des évè-
nements détectés à partir des messages par MABED en fonction des communautés
détectées avec la méthode de Louvain à partir de la structure du réseau social. Elle
révèle que les deux distributions sont très différentes. On observe par exemple que la
catégorie « Divers » a le poids le plus élevé pour la communauté c1, tandis qu’aucun
des évènements détectés pour la communauté c0 n’appartient à celle-ci. À l’inverse,
la catégorie « Science et technologie » a le poids le plus important pour c0, tandis
qu’elle est la seconde catégorie au poids le plus faible pour c1, après « Business et
économie », pour laquelle seule c0 a un poids non nul. Pour renforcer ce constat, nous
présentons sur la ﬁgure 3.17.b la distribution des poids des catégories des 10 évène-
ments au plus fort impact détectés à partir des messages par MABED dans le corpus

(cid:67)en et le corpus (cid:67)en(aléatoire). Ce dernier contient 725 806 tweets correspondant à
auteurs des messages dans le corpus (cid:67)en. Contrairement au cas précédent, on observe

tous les messages publiés par 26 000 auteurs sélectionnés aléatoirement parmi les

ici deux distributions similaires. Notamment, la catégorie au plus fort poids dans les

85

3.6. Discussion

deux corpus est « Art, culture et divertissement », tandis que les catégories « Business
et économie » et « Sport » ont des poids nuls pour les deux corpus.

Lien entre communautés au sens social et communautés au sens théma-

tique. La corrélation entre les distributions des poids obtenus à partir de (cid:67)en(c0) et
(cid:67)en(c1) mesurée selon le coefﬁcient de corrélation linéaire de Bravais-Pearson (Wilcox
et Muska, 2001) vaut −0, 36, tandis que le même coefﬁcient mesuré pour les distri-
butions obtenues à partir de (cid:67)en et (cid:67)en(aléatoire) vaut 0, 93. Cela signiﬁe, au sens

de ce coefﬁcient, que les centres d’intérêt des deux communautés détectées selon
la méthode de Louvain sont différents, voire opposés, tandis que les types d’évène-
ments détectés à partir d’un échantillon aléatoire sont très semblables à ceux détectés
pour l’ensemble de la population. Dans ce cas, les évènements détectés conﬁrment
la pertinence des communautés identiﬁées par la méthode de Louvain. Nous pou-
vons également imaginer la situation inverse, où les évènements détectés seraient
semblables pour deux ou plusieurs communautés, ce qui pourrait permettre de les fu-
sionner ensemble. D’une part, développer une mesure de qualité du partitionnement
d’un réseau social exploitant la détection d’évènements permettrait d’évaluer et de
comparer d’une nouvelle manière les méthodes de détection de communautés dans
les réseaux sociaux. D’autre part, cela pourrait permettre d’améliorer les résultats ob-
tenus avec des méthodes strictement basées sur la structure du réseau. Par exemple, il
serait possible d’intégrer cette mesure dans la méthode de détection de communautés
proposée par Newman (2006), qui procède à des dichotomies récursives du réseau en
optimisant la modularité, en vériﬁant que les deux communautés résultant d’une di-
chotomie ne soient pas trop semblables du point de vue des évènements qui suscitent
leur intérêt. Il serait également possible d’utiliser cette métrique pour déterminer le
paramètre optimal de résolution de la méthode de Louvain.

86

Détecter les évènements

(cid:67)en(c0)
(cid:67)en(c1)

(a) Évènements détectés dans les corpus (cid:67)en(c0) et (cid:67)en(c1).

(cid:67)en

(cid:67)en(aléatoire)

2.5

2

1.5

s
d
i
o
P

1

0.5

0

2.5

2

s
d
i
o
P

1.5

1

0.5

0

e
i
g
o
l
o
n
h
c
e
t

t
e

e
c
n
e
i
c
S

s
e
l
a
d
n
a
c
s

t
e

e
u
q
i
t
i
l
o
p

e
c
i
t
s
u
J

e
i
m
o
n
o
c
é

t
e

s
s
e
n
i
s
u
B

s
t
n
e
d
i
c
c
a

t
e

s
e
h
p
o
r
t
s
a
t
a
C

t
n
e
m
e
s
s
i
t
r
e
v
i
d
t
e

e
r
u
t
l
u
c

t
r
A

s
r
e
v
i
D

t
r
o
p
S

s
e
u
q
a
t
t
a

t
e

s
é
m
r
a

s
t
i
ﬂ
n
o
C

(b) Évènements détectés dans les corpus (cid:67)en et (cid:67)en(aléatoire).

Catégorie d’évènements

FIGURE 3.17 – Distribution du poids des catégories des évènements détectés par MA-

BED dans les corpus (cid:67)en(c0), (cid:67)en(c1), (cid:67)en et (cid:67)en(aléatoire)

87

Chapitre 4

Modéliser et prévoir la diffusion de l’in-
formation

Sommaire

4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
4.2 État de l’art
4.2.1 Modélisation n’exploitant pas la structure du réseau . . . . . . 93
4.2.2 Modélisation basée sur la structure du réseau . . . . . . . . . . 96
4.2.3 Synthèse de l’état de l’art . . . . . . . . . . . . . . . . . . . . . . . 100
4.3 Méthode proposée . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
4.3.1 Formulation du problème . . . . . . . . . . . . . . . . . . . . . . 102
4.3.2 Vue d’ensemble de la méthode proposée . . . . . . . . . . . . . 103
4.3.3 Description du modèle . . . . . . . . . . . . . . . . . . . . . . . . 105
4.3.4 Espace de représentation . . . . . . . . . . . . . . . . . . . . . . . 107
4.3.5 Estimation des paramètres du modèle . . . . . . . . . . . . . . . 109
4.4 Expérimentations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
4.4.1 Protocole expérimental . . . . . . . . . . . . . . . . . . . . . . . . 117
4.4.2 Évaluation de la procédure d’estimation des probabilités de

diffusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
4.4.3 Évaluation du modèle T-BASIC . . . . . . . . . . . . . . . . . . . 122
4.4.4 Analyse des facteurs impactant la diffusion de l’information . 127
4.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
4.5.1 Résumé des travaux présentés . . . . . . . . . . . . . . . . . . . . 131
4.5.2 Perspectives de travail . . . . . . . . . . . . . . . . . . . . . . . . . 132

89

4.1. Introduction

Dans le chapitre précédent, nous avons présenté une nouvelle méthode pour dé-
tecter les évènements faisant réagir les utilisateurs des médias sociaux, de manière
rétrospective. Dans ce chapitre, nous décrivons la deuxième contribution de cette
thèse, qui porte sur la modélisation et la prévision de la diffusion de l’information
au sein des médias sociaux, dans le but de comprendre et anticiper la réaction des
utilisateurs par rapport aux évènements.

4.1 Introduction

La méthode présentée dans le chapitre précédent permet de détecter a posteriori
les évènements ayant suscité l’intérêt des utilisateurs d’un média social. Néanmoins,
dans certains cas, il est utile de pouvoir anticiper la réaction que les utilisateurs au-
ront par rapport à un évènement, par exemple dans le but d’anticiper l’efﬁcacité d’une
campagne marketing virale. Dans d’autres cas, il peut être utile de prévoir l’évolu-
tion d’un phénomène de diffusion en cours, par exemple dans le but de combattre la
propagation d’informations erronées. La modélisation et la prévision du phénomène
de diffusion de l’information dans les médias sociaux sont des tâches qui suscitent
un fort intérêt de la part des chercheurs en fouille de données. De nombreuses mé-
thodes pour prévoir la diffusion de l’information ont été développées, qu’elles soient
inspirées de modèles épidémiologiques (Leskovec et al., 2007; Wang et al., 2012) ou
bien qu’elles soient des variantes de modèles initialement développés en marketing
(Galuba et al., 2010; Saito et al., 2010a; Motoda, 2011). Cependant, nous en savons
encore peu à propos des facteurs qui gouvernent le processus de diffusion au sein des
médias sociaux, tant au niveau social, temporel que thématique. Cela nous amène
donc à formuler les questions suivantes. D’abord, quels facteurs inﬂuent sur la diffu-
sion de l’information dans les médias sociaux ? Puis, comment prévoir la diffusion de
l’information à partir de ces facteurs ?

Nous sommes plus particulièrement intéressés par la prévision du volume d’uti-
lisateurs relayant une information spéciﬁque au sein d’un média social fondé sur un
réseau social explicite, décrivant quels utilisateurs sont exposés aux messages publiés
par quels utilisateurs. Ce réseau correspond par exemple, dans le cas de Twitter, au
graphe des abonnements.

Proposition et positionnement. Nous proposons un nouveau modèle probabi-

90

Modéliser et prévoir la diffusion de l’information

liste pour la diffusion de l’information dans les médias sociaux, ainsi qu’une procé-
dure pour estimer ses paramètres : T-BASIC (Time-Based ASynchronous Independent
Cascades). Ce modèle repose sur la structure de réseau sous-jacente aux médias so-
ciaux et deux paramètres pour chaque lien du réseau : la probabilité de diffusion de
l’information, et le délai de transmission entre les deux utilisateurs.

Contrairement aux modèles similaires également basés sur la structure de réseau,
qui reposent sur des probabilités de diffusion constantes pour chaque lien du réseau,
T-BASIC repose sur des probabilités dépendantes du temps, ce qui permet d’intégrer
la ﬂuctuation du niveau de réceptivité des utilisateurs en fonction du temps dans
la modélisation. La procédure que nous décrivons pour estimer les paramètres de T-
BASIC diffère aussi des approches existantes. Plutôt que d’estimer directement tous
les paramètres latents du modèles à partir de séquences d’activation, nous proposons
d’exprimer les paramètres en fonction de caractéristiques observables des utilisateurs.
Cela nous permet de réduire le coût de la phase d’estimation, puisqu’il ne s’agit plus
d’estimer directement tous les paramètres du modèle – dont le nombre est propor-
tionnel au nombre de liens dans le réseau étudié – mais d’estimer les paramètres de
deux fonctions, l’une modélisant la probabilité de diffusion et l’autre le délai de trans-
mission selon les caractéristiques des utilisateurs. Nous décrivons donc d’abord un
espace de représentation des utilisateurs, composés d’attributs sociaux, thématiques
et temporels, puis nous montrons comment inférer les paramètres de ces fonctions
à partir des caractéristiques mesurées et de séquences d’activation. Enﬁn, cette ap-
proche nous permet d’analyser l’effet des facteurs sociaux, thématiques et temporels
sur le processus de diffusion de l’information dans les médias sociaux.

Résultats. Nous menons une évaluation en deux temps par rapport à des mé-
thodes tirées de la littérature, avec des données collectées sur le média social Twitter.
D’abord, nous démontrons la validité de la démarche proposée pour estimer les pa-
ramètres de T-BASIC, puis nous évaluons les capacités prédictives du modèle T-BASIC
en comparant les séries temporelles réelles et prédites modélisant la diffusion de dif-
férents éléments d’information. Par ailleurs, nous menons une analyse des facteurs
inﬂuençant le phénomène de propagation de l’information à travers l’étude des para-
mètres de la fonction modélisant la probabilité de diffusion entre deux utilisateurs.
Cette analyse révèle que les caractéristiques des utilisateurs, tant sur le plan social,
que sur le plan thématique ou temporel, ont des effets importants sur la probabilité

91

4.2. État de l’art

de diffusion, dont la direction varie selon qu’elles soient mesurés par rapports aux uti-
lisateurs exerçant l’inﬂuence, ou ceux la subissant. Par ailleurs, cette analyse indique

que, pour une paire d’utilisateurs connectés (ux → u y), ce sont les caractéristiques

de l’utilisateur ux qui inﬂuent le plus sur la probabilité de diffusion, c’est-à-dire l’uti-
lisateur subissant l’inﬂuence de u y, plus que celles de l’utilisateur u y, qui relaie déjà
l’information.

Application. L’implémentation du modèle T-BASIC est publique et son code source

en Java est distribué librement 1.

Ce chapitre est organisé de la manière suivante. Dans la section 4.2 nous présen-
tons une synthèse de l’état de l’art, puis dans la section 4.3, nous décrivons formelle-
ment la méthode proposée. Ensuite, nous présentons les expérimentations que nous
avons menées dans la section 4.4. Enﬁn, nous concluons ce chapitre et discutons des
perspectives dans la section 4.5.

4.2 État de l’art

Les modèles pour la prévision de la diffusion de l’information dans les médias so-
ciaux reposent sur de nombreux travaux menés dans divers domaines. Ils s’inspirent
en particulier des travaux menés en épidémiologie – dans le but d’anticiper la pro-
pagation de maladies au sein d’une population – et en marketing – dans le but de
prédire l’adoption d’un produit ou d’une technologie parmi un groupe de consom-
mateurs. Du fait de la nature différente des problèmes abordés dans ces deux do-
maines, les modèles développés pour les traiter diffèrent de par les hypothèses sur
lesquelles ils se basent, et également de par la manière dont ils caractérisent la diffu-
sion. En effet, les modèles épidémiologiques classiques ne supposent pas l’existence
d’un réseau explicite interconnectant la population d’individus étudiée, ce qui est le
cas pour les modèles développés en marketing. Par ailleurs, les modèles épidémiolo-
giques se concentrent sur l’évolution temporelle du processus de diffusion, tandis que
les modèles développés en marketing s’intéressent plutôt à l’évolution structurelle de
la diffusion.

1. http://mediamining.univ-lyon2.fr/people/guille/tbasic.php

92

Modéliser et prévoir la diffusion de l’information

4.2.1 Modélisation n’exploitant pas la structure du réseau

Modèles classiques. Nous décrivons ici des modèles développés en épidémiolo-
gie, dits modèles compartimentaux, conçus pour modéliser la diffusion d’une maladie
au sein d’une population constante de N individus. Ils supposent d’une part que les
contacts entre les N individus se font aléatoirement, et d’autre part que les membres
se trouvent dans des états particuliers (dus à la diffusion), ce qui permet de les « com-
partimenter ». On parle de modélisation sous forme de « mélange homogène » puisque
ces modèles considèrent d’une part que les individus d’un même compartiment sont
connectés selon une structure régulière avec les individus des autres compartiments,
et d’autre part que les individus changent de compartiments de façon homogène. Les
modèles compartimentaux caractérisent le processus de diffusion à travers l’évolution
de la taille de chaque compartiment dans le temps, modélisée à l’aide d’équations dif-
férentielles. Ils se concentrent donc par nature sur l’aspect temporel de la diffusion.

Kermack et McKendrick (1927) décrivent le modèle le plus simple, SI, qui considère
deux états : « Susceptible » (S) et « Infected » (I). Les membres du réseau dans l’état
Susceptible peuvent contracter la maladie au contact des membres dans l’état Infected.
La seule transition possible, comme l’illustre la ﬁgure 4.1.a, se fait donc depuis l’état
Susceptible vers l’état Infected. Ce modèle suppose que tout individu dans le compar-
timent S a une probabilité constante β d’être infecté par un individu appartenant au
compartiment I. Soit S la taille du compartiment contenant les individus dans l’état
S et I la taille du compartiment regroupant les individus dans l’état I. On peut alors
exprimer les taux de changement de la taille des deux compartiments de la façon
suivante :

dS
d t

= − βSI
N

d I
d t

=

βSI
N

Comme on a N = S(t) + I(t), on peut ré-écrire la seconde équation comme suit :

d I
d t

= β I(1− I

N

)

Il apparaît que la taille du compartiment I croît selon un modèle de Verhulst, ce

93

4.2. État de l’art

(a) Le modèle SI.

(b) Le modèle SIR.

FIGURE 4.1 – Représentations graphiques des modèles épidémiologiques SI et SIR.

qui signiﬁe que I(t) suit une fonction logistique (Verhulst, 1845), jusqu’à ce que tous
les individus soient dans l’état I (Hethcote, 2000).

Kermack et McKendrick (1927) proposent d’enrichir ce modèle en ajoutant une
nouvelle transition depuis I vers R, un nouvel état nommé « Removed » en épidémio-
logie ou « Refractory » dans les travaux en rapport avec la diffusion de l’information.
La représentation graphique du modèle SIR est donnée par la ﬁgure 4.1.b. Les indi-
vidus appartenant au compartiment I rejoignent le compartiment R avec une proba-
bilité constante γ. Une fois dans cet état, les individus ne peuvent ni contracter, ni
transmettre la maladie. On modélise l’évolution de la diffusion comme suit :

dS(t)
d t

= − βS(t)I(t)

N

d I(t)
d t

=

βSI
N

− γI(t)

dR(t)
d t

= γI(t)

L’ajout du compartiment R modiﬁe notoirement le comportement du modèle,
puisque la quantité d’individus dans le compartiment I n’évolue plus nécessairement
de façon monotone, I(t) étant une fonction croissante dans le cas de SI. En effet,
comme le montre la ﬁgure 4.2, la courbe représentant l’évolution de la taille du com-
partiment I a une allure en cloche lorsque l’on choisit des valeurs de β et γ supérieures

94

Modéliser et prévoir la diffusion de l’information

FIGURE 4.2 – Allure typique des courbes de diffusion obtenues avec le modèle SIR.

à 0 pour le modèle SIR.

Modèles pour la diffusion de l’information dans les médias sociaux. Ces mo-
dèles compartimentaux, ou des variantes, sont couramment utilisés pour modéliser
puis prévoir la diffusion de l’information dans les médias sociaux (Saito et al., 2011;
Cheng et al., 2013), leurs paramètres étant estimés à l’aide de traces de diffusion pas-
sées. Leskovec et al. (2007) proposent par exemple de modéliser la diffusion de l’infor-
mation parmi les utilisateurs des médias sociaux à l’aide du modèle épidémiologique
SIS. Celui-ci est semblable au modèle SIR – I signiﬁant dans ce cas que l’utilisateur
a reçu l’information et participe à sa diffusion – sauf que les utilisateurs atteignant
l’état R retournent immédiatement dans l’état S. Cependant, cette modélisation im-
plique que l’on suppose que tous les individus soient aussi inﬂuents les uns que les
autres – ce degré d’inﬂuence commun à tous les individus étant proportionnel à la
valeur du paramètre β, ce qui n’est pas une hypothèse satisfaisante dans le cadre des
médias sociaux. Qui plus est, les auteurs ne proposent pas de méthode pour estimer
la valeur optimale de β mais la déterminent manuellement par essais successifs.

Face à ce problème, Yang et Leskovec (2010) proposent le Linear Inﬂuence Model
(LIM) qui se fonde sur l’estimation de l’inﬂuence de chacun des utilisateurs pour pré-
voir l’évolution du volume d’utilisateurs relayant l’information. LIM associe à chaque
utilisateur un paramètre, à savoir une fonction d’inﬂuence I, telle que la fonction Iux
associée à l’utilisateur ux exprime le volume d’utilisateurs qu’il inﬂuence au ﬁl du

95

4.2. État de l’art

FIGURE 4.3 – Illustration du fonctionnement du Linear Inﬂuence Model. Le volume de
messages publiés au ﬁl du temps est obtenu en sommant les fonctions d’inﬂuence des
utilisateurs initialement actifs : u1, u2 et u3.

temps après avoir relayé une information. Ainsi, comme nous l’illustrons à l’aide de
la ﬁgure 4.3, partant d’un ensemble d’utilisateurs initialement informés et connais-
sant le moment où ils publient un message à propos de cette information, LIM prévoit
l’évolution du volume d’utilisateurs relayant l’information en sommant les fonctions
d’inﬂuence des utilisateurs initiaux. Pour pouvoir estimer les fonctions d’inﬂuence, les
auteurs décrivent une formulation non paramétrique de LIM, où les fonctions sont
représentées par des vecteurs de longueur ﬁxe. Pour estimer les fonctions d’inﬂuence
des utilisateurs à l’aide de traces de diffusion passées (i.e. l’évolution du volume d’uti-
lisateurs relayant l’information et le moment où chaque utilisateur a relayé l’informa-
tion), ils formulent un problème d’optimisation de type « moindres carrés non négatif »
qui peut être résolu à l’aide de la méthode décrite par Coleman et Li (1996).

4.2.2 Modélisation basée sur la structure du réseau

Modèles classiques. Ces travaux, menés initialement dans le domaine du marke-
ting, modélisent le processus de diffusion au sein d’une population constante de N
individus interconnectés par un réseau statique décrit par un graphe orienté G. Ces
modèles supposent que l’information ne peut se propager que le long des liens de ce

96

Modéliser et prévoir la diffusion de l’information

réseau. Il existe deux manières de modéliser ce type de processus de diffusion, selon
que la modélisation soit :

— centrée sur les récepteurs – on parle alors de « modèle de seuil », tel que le

modèle de seuil linéaire développé par (Granovetter, 1978) ;

— ou bien centrée sur les émetteurs – on parle alors de « modèle de cascade »,
tel que le modèle des cascades indépendantes proposé par (Goldenberg et al.,
2001).

Dans les deux cas, on considère que chaque membre du réseau peut être soit
inactif, soit actif, un membre actif étant un individu ayant reçu l’information et par-
ticipant à sa propagation. Ces modèles caractérisent un processus de diffusion par
une séquence d’activation le long d’un axe temporel discret, puisqu’ils modélisent
la diffusion comme un processus itératif où les membres du réseau changent d’état
de façon monotone (i.e. les membres actifs ne peuvent pas redevenir inactifs) et syn-
chrone. Par conséquent, et contrairement aux modèles compartimentaux, ces modèles
se concentrent sur l’aspect structurel de la diffusion.

Les modèles de seuil sont fondés sur le principe selon lequel le passage de l’état in-
actif à l’état actif d’un membre du réseau dépend de l’inﬂuence exercée par ses voisins

actifs dans le réseau. Chaque arc (ux → u y) du graphe G est associé à un paramètre
wx y quantiﬁant le degré d’inﬂuence qu’exerce le membre u y sur ux, et chaque nœud
ux du graphe est associé à un seuil d’inﬂuence θux. Par ailleurs, à chaque nœud ux
du graphe G (dont l’ensemble des voisins sortants est noté Γ→
ux) est associée une fonc-
tion croissante gux déﬁnie sur (cid:80) (Γ→
) représentant l’ensemble des parties de
l’ensemble Γux, tel que gux((cid:59)) = 0. Étant donné un ensemble S de membres du réseau

ux

), (cid:80) (Γ→

ux

initialement actifs, le processus de diffusion se déroule itérativement, comme suit.
À une itération t, on évalue pour chaque nœud inactif ux la fonction gux pour l’en-
semble de ses voisins actifs. Si sa valeur excède le seuil θux, alors le nœud ux devient
actif à l’itération t + 1. Le processus s’achève lorsqu’aucune nouvelle activation n’est
possible. Granovetter (1978) propose le Linear Threshold Model (LT), qui, comme son
nom l’indique, déﬁnit la fonction g comme une somme linéaire des degrés d’inﬂuence

de tous les voisins actifs u y, c’est-à-dire : gux =(cid:80) wu y.
(ux → u y) la probabilité pux,u y que le membre ux inﬂuence son voisin inactif u y de

Les modèles de cascade requièrent, quant à eux, que l’on déﬁnisse pour chaque arc

sorte que celui-ci passe dans l’état actif. Étant donné un ensemble S de membres du

97

4.2. État de l’art

FIGURE 4.4 – Un processus de diffusion modélisé selon le Independent Cascades Model
(IC). À gauche : un extrait du réseau servant de support à la diffusion, annoté avec les
probabilités de diffusion pour chaque arc visible. À droite, le processus de diffusion
initié par les deux nœuds colorés en gris foncé.

réseau initialement actifs, le processus de diffusion se déroule itérativement, comme
suit. À une itération t, chaque nœud ux devenu actif à cette itération inﬂuence avec
une probabilité pux,u y chacun de ses voisins u y, qui deviennent actifs le cas échéant à
l’itération t +1. Le processus s’achève lorsqu’aucune nouvelle activation n’est possible.
Goldenberg et al. (2001) proposent le Independent Cascades Model (IC), qui suppose
que chaque nœud ux nouvellement actif inﬂuence indépendamment des autres cha-
cun de ses voisins inactifs.

La manière dont ces modèles sont déﬁnis, ainsi que le processus itératif sur le-
quel ils reposent, induisent des changements d’états synchrones. Comme le montre la
ﬁgure 4.4, qui illustre un processus de diffusion modélisé selon le modèle IC, la diffu-
sion ne se déroule pas le long d’un axe temporel concret, mais le long d’un axe tem-
porel simple qui correspond à une série d’étapes. Or, dans de nombreuses situations,
les processus de diffusion observés sont asynchrones. Par conséquent, ces modèles ne
peuvent pas reproduire les motifs temporels liés à des processus de diffusion réels
asynchrones, comme c’est le cas des processus de diffusion d’information à travers les
médias sociaux. Pour cette raison, Saito et al. (2010a) développent les modèles AsLT
et AsIC qui étendent respectivement les modèles LT et IC, en associant à chaque arc

98

Modéliser et prévoir la diffusion de l’information

(ux → u y) du graphe G un paramètre rux,u y quantiﬁant le délai d’activation. Ainsi,

dans le cas du modèle AsIC par exemple, un nœud ux inﬂuencé par u y à un instant t
est activé en t + rux,u y. Par conséquent, les changements d’état sont asynchrones et le
processus de diffusion modélisé le long d’un axe temporel concret.

Modèles pour la diffusion de l’information dans les médias sociaux. Saito et al.
(2009) proposent d’estimer les paramètres du modèle IC, à savoir les probabilités de
diffusion pour chaque lien du réseau, à partir de traces de diffusion passées dans un
média social, plus exactement des séquences d’activation des nœuds engendrées par
la diffusion de diverses informations. Pour ce faire, ils formulent la vraisemblance
qu’un ensemble donné de paramètres (i.e. l’ensemble des probabilités de diffusion
pour les liens du réseau) ait généré ces séquences d’activation, et décrivent une mé-
thode itérative de type EM (i.e. Espérance-Maximisation (Do et Batzoglou, 2008))
aﬁn d’identiﬁer l’ensemble de paramètres maximisant la vraisemblance. Galuba et al.
(2010) suggèrent d’employer le modèle LT pour modéliser la diffusion d’une infor-
mation (les auteurs considèrent uniquement la diffusion d’URL) à travers un média
social. Chaque lien du réseau est donc associé à un degré d’inﬂuence et chaque nœud
du réseau est associé à un seuil. Ce seuil est une fonction de deux variables latentes :
(i) la viralité de l’information et (ii) la probabilité que cet utilisateur relaie n’importe
quelle information. Pour déﬁnir les valeurs optimales des degrés d’inﬂuence et des
deux variables latentes caractérisant chaque seuil, les auteurs formulent un problème
d’optimisation à partir de séquences d’activation, qu’ils résolvent de manière itérative
selon la méthode du gradient (Snyman, 2005).

L’inconvénient des modèles IC et LT est que ceux-ci supposent des changements
d’états synchrones, ce qui fait qu’il n’est pas possible de les utiliser pour prévoir à long
terme l’évolution du volume d’utilisateurs relayant l’information à travers un média
social. Par exemple, Galuba et al. (2010) n’utilisent en pratique le modèle qu’ils pro-
posent que pour anticiper une étape dans le processus de diffusion. Autrement dit, ils
évaluent leur modèle pour la tâche qui consiste à, ayant observé le début d’un proces-
sus de diffusion, prédire quels utilisateurs parmi le voisinage direct de ceux déjà actifs
vont s’activer. Comme nous l’avons indiqué précédemment, des variantes asynchrones
– AsIC et AsLT – permettant de modéliser la diffusion le long d’un axe temporel concret
ont été développées. Similairement aux travaux cités précédemment, Motoda (2011)
décrit une méthode de type EM pour estimer les paramètres de ces variantes à partir

99

4.2. État de l’art

de séquences d’activation.

Il est également intéressant de mentionner le Diffusive Logistic Model (DLM) ré-
cemment proposé par Wang et al. (2012) qui – à l’instar des modèles épidémiolo-
giques que nous avons décrits dans la précédente section – modélise le processus de
diffusion à l’aide d’équations différentielles. Qui plus est, comme le modèle SI (Ker-
mack et McKendrick, 1927) décrit précédemment, il modélise l’évolution du volume
d’utilisateurs inﬂuencés selon une équation logistique, à la différence près que DLM
ne caractérise pas la diffusion uniquement dans le temps, mais également dans l’es-
pace. En effet – en supposant que le processus de diffusion est initié par un unique
utilisateur – le réseau peut alors être décrit de manière superﬁcielle en terme de dis-
tance entre l’utilisateur initial et les autres utilisateurs. Ainsi, DLM décrit l’évolution
du volume d’utilisateurs inﬂuencés dans le temps en fonction de leur distance (notée
x, i.e. le nombre de sauts) par rapport à la source selon l’équation suivante :

d I(t, x)

d t

= r I(x, t)(1− I(x, t)

K

)

où r et K sont des paramètres caractérisant respectivement la vitesse à laquelle
l’information se propage parmi les utilisateurs situés à une même distance de la
source, et la densité maximale d’utilisateurs pouvant être inﬂuencés à une distance
donnée.

4.2.3 Synthèse de l’état de l’art

La matrice présentée par la table 4.1 synthétise cet état de l’art en comparant les
méthodes existantes selon quatre critères : (i) la prise en compte de la structure du
réseau, (ii) la prise en compte du temps, (iii) la prise en compte de la spéciﬁcité de
l’information qui se diffuse et (iv) la manière dont l’inﬂuence entre les utilisateurs est
mesurée.

Réseau. Nous remarquons que seuls les modèles de seuil ou de cascade exploitent
totalement la structure du réseau, ce qui leur permet de modéliser précisément l’in-
ﬂuence entre chaque paire d’utilisateurs connectés. Prendre en compte la structure du
réseau est important, comme le démontre l’étude menée par Katona et al. (2011). Les
auteurs observent principalement deux effets liés au réseau : l’effet de degré, selon le-
quel un utilisateur connecté à beaucoup d’utilisateurs relayant une information a plus

100

Modéliser et prévoir la diffusion de l’information

TABLE 4.1 – Matrice de comparaison des modèles existants pour la prévision de la
diffusion de l’information dans les médias sociaux.

Prise en compte

du réseau

modèles
compartimentaux

LIM

IC-EM

LT-Gradient

AsIC-EM
AsLT-EM

–

–

(cid:216)

(cid:216)

(cid:216)

LDM

partielle

Aspects pris en compte pour la prévision :
temps

information

inﬂuence

(cid:216)

(cid:216)

–

–

(cid:216)

(cid:216)

–

–

–

(cid:216)

–

–

inﬂuence uniforme
à travers le réseau
inﬂuence globale

de chaque utilisateur

inﬂuence pour

chaque lien du réseau

inﬂuence pour

chaque lien du réseau

inﬂuence pour

chaque lien du réseau
inﬂuence globale de
l’utilisateur source

de chances de relayer l’information à son tour, et l’effet de clustering, selon lequel la
densité de connexions au sein d’un groupe d’utilisateurs relayant une information a
un effet positif important sur la probabilité que les utilisateurs connectés à ce groupe
relaient à leur tour l’information. Cependant, seules les variantes asynchrones de ces
modèles permettent d’intégrer le temps dans la prédiction.

Thématique. Nous remarquons par ailleurs que seule l’adaptation du modèle LT
proposée par Galuba et al. (2010) intègre une propriété de l’information, à savoir sa
viralité, dans le processus d’estimation des paramètres et de prévision. Or, comme le
montre l’étude menée sur Twitter par Romero et al. (2011) l’information se diffuse dif-
féremment selon les thématiques abordées. Par conséquent, il paraît crucial d’intégrer
l’aspect thématique dans la prévision de la diffusion de l’information.

Estimation des paramètres. Il ressort de cet état de l’art que les modèles existants
sont paramétrés par un grand nombre de variables latentes non observables. Dans le
cas des modèles de seuil, ce sont les degrés d’inﬂuence de tous les liens du réseau et
les seuils d’inﬂuence de tous les nœuds, tandis que dans le cas des modèles de cascade
de base, ce sont les probabilités de diffusion de tous les liens du réseau. Les approches
existantes estiment ces paramètres en formulant différents problèmes d’optimisation
qui exploitent un seul type de variable observable, à savoir des séquences d’activation,
ce qui pose deux problèmes. D’une part, plus le réseau étudié est grand et/ou dense,

101

4.3. Méthode proposée

plus la quantité de données nécessaires à l’estimation des paramètres du modèle est
grande aﬁn qu’elle soit satisfaisante, ce qui augmente d’autant plus le coût de l’esti-
mation des paramètres. D’autre part, en estimant directement les paramètres à partir
de séquences d’activation, on ne met pas en avant de facteurs observables permettant
d’interpréter clairement les mécanismes régissant la diffusion.

Dans la section suivante, nous décrivons la méthode que nous proposons pour

pallier aux limitations des méthodes existantes.

4.3 Méthode proposée

4.3.1 Formulation du problème

Entrée. Soit un ensemble U d’utilisateurs d’un média social et un corpus (cid:67) de

messages, reﬂétant l’activité de ces utilisateurs pendant une certaine période de temps
dans le passé. Le graphe orienté G = (U, E) modélise le réseau social interconnectant
ces utilisateurs, où E (⊂ U× U) est l’ensemble des liens connectant les utilisateurs, de
telle sorte qu’un lien (ux → u y) signiﬁe que l’utilisateur ux est connecté à l’utilisateur
un mot principal et un ensemble pondéré de mots liés. Soit un ensemble S ⊂ U

u y et est exposé au contenu publié par ce dernier. Soit une thématique T décrite par

d’utilisateurs étant les premiers à relayer l’information décrite par la thématique T .
La table 4.2 donne les notations utilisées dans le reste de ce chapitre.

Sortie. En supposant que l’information se propage par cascade d’information, l’ob-
jectif consiste à prévoir la série temporelle ˆsT modélisant l’évolution du volume d’utili-
sateurs inﬂuencés à propos de la thématique T , laquelle se propage à travers le graphe
G, en partant des utilisateurs de l’ensemble S. Supposer que la diffusion d’une théma-
tique T est due à une cascade d’information, revient à considérer un monde fermé,
où – hormis les utilisateurs de l’ensemble S – les utilisateurs ne sont inﬂuencés que
par leurs voisins dans le graphe G. Autrement dit, nous ne considérons pas de sources
d’inﬂuence externes.

102

Modéliser et prévoir la diffusion de l’information

TABLE 4.2 – Liste des notations utilisées dans le chapitre 4.

Notation Déﬁnition

Ensemble des utilisateurs
Un utilisateur appartenant à l’ensemble U

G = (U, E) Réseau social interconnectant les utilisateurs
Ensemble des liens orientés du réseau social
Ensemble des voisins sortants de ux dans le graphe G
Ensemble des voisins entrants de ux dans le graphe G
Corpus de messages publiés par les utilisateurs de l’ensemble U

ux

U
ux

ux

E
Γ→
Γ←
(cid:67)
(cid:67)ux
(cid:67) @
ux
Vux
M @
ux
N @
ux
T
t

Corpus des messages publiés par l’utilisateur ux

Corpus des messages publiés par l’utilisateur ux ((cid:67)ux ⊂ (cid:67) )
contenant au moins une mention ((cid:67) @
Vocabulaire des mots employés dans le corpus (cid:67)ux
Ensemble des utilisateurs mentionnés dans le corpus (cid:67) @
Nombre de messages dans le corpus (cid:67) mentionnant l’utilisateur ux

⊆ (cid:67)ux)

ux

ux

Une thématique décrite par un terme principal et un ensemble de mots liés
Variable modélisant le temps

4.3.2 Vue d’ensemble de la méthode proposée

La méthode que nous proposons, dont le déroulement est schématisé à la ﬁ-
gure 4.5, comporte deux volets. Tout d’abord, nous proposons un modèle probabi-
liste T-BASIC (Time-Based ASynchronous Independent Cascades) basé sur la structure
de réseau sous-jacente aux médias sociaux, et qui est une extension du modèle AsIC
proposé par (Saito et al., 2010a). Comme AsIC, il repose sur deux paramètres pour
chaque lien du réseau, la probabilité de diffusion entre deux utilisateurs et le délai
de transmission, et modélise le processus de diffusion comme des cascades indépen-
dantes asynchrones. Néanmoins, dans le cas de T-BASIC, les probabilités ne sont pas
constantes mais dépendante du temps, ce qui permet d’intégrer la ﬂuctuation du ni-
veau d’attention des utilisateurs au ﬁl du temps.

Ensuite, nous proposons une procédure pour estimer les paramètres du modèle
T-BASIC. Plutôt que d’estimer directement tous les paramètres latents du modèle,
dont le nombre est égal à deux fois le nombre de liens dans le réseau étudié, nous
proposons de les exprimer comme des fonctions de caractéristiques observables des

103

4.3. Méthode proposée

FIGURE 4.5 – Prévision de la diffusion d’une information à l’aide du modèle T-BASIC.

104

Modéliser et prévoir la diffusion de l’information

utilisateurs, selon trois aspects : social, thématique et temporel. Ainsi, le nombre de
paramètres à estimer n’est plus lié à la taille du réseau, mais au nombre de carac-
téristiques mesurées pour chaque paire d’utilisateurs, ce qui diminue grandement le
coût de la phase d’estimation. L’autre avantage de cette démarche est qu’elle permet
aisément d’analyser l’effet des caractéristiques des utilisateurs sur le phénomène de
diffusion de l’information.

4.3.3 Description du modèle

Le modèle T-BASIC est une extension du modèle AsIC (Saito et al., 2010a), lui-
même une extension du modèle IC (Goldenberg et al., 2001). T-BASIC modélise le
processus de diffusion le long d’un axe temporel concret, sous la forme de cascades

indépendantes, à travers le graphe orienté G = (U, E). Il y a pour chaque lien (ux →
u y) ∈ E deux paramètres :

— Une fonction réelle pux,u y(t) à valeur dans [0; 1] donnant la probabilité que

l’utilisateur u y inﬂuence ux à un instant t.

— Une valeur réelle µux,u y, telle que µux,u y > 0, correspondant au délai après
lequel l’utilisateur ux publie un message à propos de la thématique, une fois
qu’il a été inﬂuencé avec succès par l’utilisateur u y.

Le processus de diffusion, décrit par l’algorithme 2 (page 106), est initié à partir
d’un ensemble S d’utilisateurs activés. Chaque utilisateur u y nouvellement activé à
un instant t peut inﬂuencer chacun de ses voisins entrants ux inactifs avec une pro-
babilité pux,u y(t). Nous considérons que l’utilisateur ux est inﬂuencé avec succès par
u y si la probabilité pux,u y(t) est strictement supérieure à une valeur aléatoire réelle
comprise entre 0 et 1, tirée aléatoirement selon une loi uniforme. Les voisins qu’u y
a inﬂuencés deviennent actifs à leur tour à l’instant t + µux,u y. La condition d’arrêt
est la même que pour IC et AsIC, c’est-à-dire lorsqu’aucune nouvelle activation n’est
possible.

105

4.3. Méthode proposée

Algorithme 2 : Prévision de la diffusion avec le modèle T-BASIC.

Données : Un ensemble U d’utilisateurs, le graphe G = (U, E) décrivant le

réseau social

Paramètres : Une thématique T , les probabilités de diffusion pux,u y(t), les
délais de transmission µux,u y, un ensemble S d’utilisateurs initiant la diffusion
Résultat : Une série temporelle s caractérisant la diffusion
Initialiser l’horloge t;
Initialiser la table H utilisée pour mémoriser les couples (utilisateur, instant
d’activation);

pour chaque utilisateur u ∈ S faire

Ajouter un couple c = (u,t) à la table H;

ﬁn

tant que il existe un couple c = (u y, t y) ∈ H|t y (cid:190) t faire

Initialiser la variable compteur à 0;

pour chaque utilisateur u y ∈ H nouvellement activé à l’instant t, i.e. t y = t

faire

Incrémenter la variable compteur;

pour chaque utilisateur inactif ux ∈ Γ←

u y n’apparaissant pas dans H faire

Tirer une valeur aléatoire, v, selon une loi uniforme;
si pux,u y(t) > v alors

/* L’utilisateur uy a influencé ux
Ajouter une entrée (ux,t + µux,u y) à la table H;

*/

ﬁn

ﬁn

ﬁn

s(t) ← compteur;

Incrémenter l’horloge t;

ﬁn
retourner s;

106

Modéliser et prévoir la diffusion de l’information

4.3.4 Espace de représentation

L’estimation des paramètres du modèle T-BASIC repose sur les attributs numé-

riques que nous déﬁnissons ci-après. Ils caractérisent chaque paire d’utilisateurs (ux,u y)

connectés dans le réseau social par un lien (ux → u y) et couvrent trois aspects : social,
thématique et temporel. Ces attributs sont mesurés à partir du corpus de messages (cid:67)

qui reﬂète le comportement des utilisateurs dans le passé.

Aspect social. Ces attributs servent à quantiﬁer les interactions sociales ayant lieu
entre les utilisateurs. Les métriques utilisées pour les calculer et que nous décrivons
ci-après sont essentiellement basées sur les interactions explicites, lesquelles sont ma-
téralisées par les mentions présentes dans les messages. L’importance des mentions
dans le cadre de la détection d’évènements dans les médias sociaux a été soulignée
dans le chapitre précédent, et leur pouvoir prédictif dans le cadre de la diffusion
de l’information est également mis en avant dans l’étude menée par Yang et Counts
(2010).

— Activité (Ac) : cet attribut caractérise le degré d’activité de chaque utilisateur.
Il correspond au nombre moyen de messages publiés par heure, borné à 1. La
valeur de cet attribut est calculée ainsi :

 |(cid:67)ux|

ε

1

Ac(ux) =

si |(cid:67)ux| < ε,

sinon.

Ac(u y) =

si |(cid:67)u y| < ε,

sinon,

 |(cid:67)u y|

ε

1

où ε exprime la durée en heures de la période couverte par le corpus (cid:67) .

— Homogénéité sociale (Hs) : cet attribut porte sur la paire d’utilisateurs (ux,u y).
Son but est de quantiﬁer la similarité entre les ensembles d’utilisateurs avec
lesquels chacun des deux utilisateurs connectés interagissent, Mux et Mu y. Plus
cette similarité est grande, plus il est probable que les deux utilisateurs aient
des centres d’intérêt proches. La valeur de cet attribut est calculée selon la
métrique de Jaccard, qui correspond à la cardinalité de l’intersection des deux
ensembles, normalisée par la cardinalité de leur union :

|Mux ∩ Mu y|
|Mux ∪ Mu y|

Hs(ux, u y) =

107

4.3. Méthode proposée

— Rôle (Ro) : cet attribut vise à caractériser le rôle joué par chaque utilisateur
dans le processus de diffusion de l’information et, pour cela, nous proposons
de mesurer la proportion de messages publiés contenant au moins une men-
tion. Une proportion élevée indique que l’utilisateur a tendance à jouer un rôle
actif dans le processus de propagation de l’information, en ciblant activement
d’autres utilisateurs. Au contraire, une proportion faible indique un rôle plus
passif par rapport à la diffusion. Cette proportion est calculée comme suit :

 |(cid:67) @

|
|(cid:67)ux|

ux

0

Ro(ux) =

si |(cid:67)ux| > 0,

sinon.

Ro(u y) =

si |(cid:67)u y| > 0,

sinon.

 |(cid:67) @

|
|(cid:67)u y|

u y

0

— Mention (Me) : cet attribut binaire a pour but de signaler une interaction expli-

cite, i.e. mention, entre deux utilisateurs connectés (ux → u y). Autrement dit,
si ux mentionne u y dans l’un des messages contenus dans (cid:67)ux, Me(ux, u y)

vaut 1 et 0 autrement. L’attribut est calculé – symétriquement – pour chaque
utilisateur :

Me(ux, u y) =

1 si u y ∈ Mux,

0 sinon.

Me(u y, ux) =

1 si ux ∈ Mu y,

0 sinon.





— Taux de mention (Tm) : cet attribut est similaire au « mention rate » décrit par
Yang et Counts (2010). Sa valeur est proportionnelle à N @
ux, la quantité de mes-
sages mentionnant le pseudonyme de chaque utilisateur. C’est un indicateur
ﬁable de la popularité de chaque utilisateur et également un indicateur perti-
nent pour prédire la diffusion comme le montrent Yang et Counts (2010). Il est
calculé comme suit :

Tm(ux) = N @
|(cid:67)|

ux

Tm(u y) =

N @
|(cid:67)|
u y

Aspect thématique. Nous considérons également un attribut binaire (Th) captu-
rant le lien entre chaque utilisateur et la thématique dont on cherche à prévoir la
diffusion. Cet attribut signale si le mot principal, m, de la thématique T a déjà été
employé dans l’ensemble des messages publiés par chaque utilisateur, c’est-à-dire :

108

Modéliser et prévoir la diffusion de l’information



1 si m ∈ Vux,

0 sinon.

Th(ux, T) =



1 si m ∈ Vu y,

0 sinon.

Th(u y, T) =

Aspect temporel. Enﬁn, nous considérons aussi l’aspect temporel, et ce dans le
but de prendre en compte la ﬂuctuation de la réceptivité de chaque utilisateur au
cours de la journée. Nous modélisons la réceptivité (Re) d’un utilisateur au cours
de la journée de manière non paramétrique, à l’aide d’un vecteur non négatif de
longueur ﬁxe noté re. Chaque composante du vecteur représente le niveau d’attention
durant une période de la journée. Nous proposons ici de partitionner une journée en
6 périodes de 4 heures chacune et de déﬁnir la valeur de chaque composante comme
la proportion de messages que chaque utilisateur publie durant chaque période (i.e.
[0h; 4h[, [4h; 8h[, etc.). Autrement dit, la valeur de la composante de rei
ux donne la
probabilité qu’un message choisi aléatoirement dans (cid:67)ux ait été publié durant la ième

période d’une journée. À un instant t, tel que l’heure de la journée correspondante
soit h, on obtient donc la valeur de cet attribut ainsi :

(cid:48)
Re(ux, t) = ret
ux

Re(u y, t) = ret

(cid:48)
u y,

où l’on détermine la composante du vecteur correspondant à l’instant t ainsi :

(cid:48) = (cid:98)h
4

(cid:99),

t

c’est à dire le quotient de la division entière de l’heure h par 4.

Espace de représentation complet. Pour chaque lien du réseau (ux → u y), nous
déﬁnissons à un instant t le vecteur v t
ux,u y, pour une thématique T donnée, dont les
13 composantes correspondent aux valeurs des 13 mesures – toutes déﬁnies sur [0; 1]
– que nous venons de décrire. Pour illustrer cela, la table 4.3 donne une instanciation
possible du vecteur v t

ux,u y.

4.3.5 Estimation des paramètres du modèle

Plutôt que d’estimer directement tous les paramètres du modèle T-BASIC, i.e. les
probabilités de diffusion en fonction du temps et les délais de transmission pour tous
les liens du réseau, nous proposons de les exprimer en fonction des attributs de l’es-

109

4.3. Méthode proposée

TABLE 4.3 – Instanciation possible d’un vecteur v t

ux,u y.

Social

Ac
0,78
0,22

Hs

0,12

Ro Me
0,7
0
1
0,23

Tm
0,65
0,10

ux
u y

Th(T )

Thématique Temps
Re(t)
0,5
0,33

1
1

pace de représentation que nous venons de décrire. Ainsi, nous déﬁnissons la proba-
) des attributs
bilité de diffusion pux,u y(t) comme une fonction paramétrique f (v t
des utilisateurs ux et u y. Nous déﬁnissons le délais de transmission µux,u y comme
une fonction g(Ac(ux)) du degré d’activité de l’utilisateur ux. Dans la suite de cette
section, nous montrons comment estimer les paramètres de ces fonctions à partir d’un
jeu de données adapté.

ux,u y

Construction du jeu de données pour l’estimation des paramètres de f . Le jeu
de données servant à estimer les paramètres de la fonction f est un ensemble D de

n instances décrites par un vecteur vi = {vi1, vi2, ..., vi13} et une variable qualitative yi
à deux modalités, « diffusion » ou « non-diffusion », notée respectivement 1 et 0 pour
i=1, yi ∈ {1, 0}, où la modalité
simpliﬁer les écritures. Nous avons donc D = (vi, yi)n
yi = 1 signiﬁe que le vecteur vi correspond aux attributs d’une paire d’utilisateurs
(ux,u y) mesurés par rapport à un instant t et une thématique T , tel que l’utilisateur
u y a inﬂuencé ux à l’instant t à propos de la thématique T . Au contraire, la modalité
yi = 0 signiﬁe que l’information ne s’est pas diffusée entre les deux utilisateurs décrits
par le vecteur vi. La construction de ce jeu de données se base sur les deux éléments
suivants :
— Un corpus (cid:67) contenant l’intégralité des messages publiés par les utilisateurs de
l’ensemble U durant une période de temps donnée P(cid:67) . Les valeurs des attributs
composant les vecteurs vi sont mesurées à partir de ce corpus.

— Un ensemble A de séquences d’activation liées à la diffusion d’éléments d’in-
formation à travers le réseau social modélisé par le graphe G durant une pé-
riode de temps donnée PA. Une séquence d’activation a est liée à une théma-
tique T décrite par un terme principal et un ensemble pondéré de mots liés,
ainsi qu’un intervalle temporel I, c’est-à-dire une thématique saillante, telle
que nous l’avons déﬁnie dans le chapitre 3. Une séquence d’activation est une

110

Modéliser et prévoir la diffusion de l’information

séquence de couples (utilisateur, instant d’activation), qui indique quels utilisa-
teurs de l’ensemble U ont relayé l’information et à quel moment. Par exemple,
le couple (ux, t x) indique que l’utilisateur ux a publié un message à l’instant
t x.

Les périodes de temps P(cid:67) et PA sont choisies telles qu’elles soient disjointes et
que P(cid:67) soit antérieure à PA, de sorte que les attributs soient mesurés sur une période
précédent la période durant laquelle les séquences d’activation sont extraites.

L’objectif consiste, pour toutes les séquences d’activation, à identiﬁer les paires
d’utilisateurs (ux, u y) pour lesquelles – étant donnée l’hypothèse de monde fermé –
il apparaît que l’utilisateur ux a été inﬂuencé par u y à propos de la thématique T .
— Premièrement, il existe dans le graphe G un arc (ux → u y). Autrement dit, ux

est exposé aux messages publiés par l’utilisateur u y.

— Deuxièmement, les utilisateurs ux et u y apparaissent dans une même sé-

quence d’activation, liée à une thématique T ;

— Troisième condition, l’utilisateur ux a relayé l’information décrite par T après
u y, c’est-à-dire que t x > t y – t x et t y désignant les moments où respective-
ment ux et u y ont relayé l’information ;

— Enﬁn, parmi les utilisateurs appartenant à Γ→
de ux) seul u y satisfait la troisième condition.

ux (l’ensemble des voisins sortants

Pour chaque paire (ux, u y) vériﬁant ces quatre conditions, nous construisons une
ux,u y, 1). Les valeurs des attributs Re(ux) et Re(u y) – qui me-
instance positive : (v t y
surent la réceptivité des utilisateurs – sont mesurées en t y, c’est-à-dire au moment
de la journée où u y a relayé l’information. Les attributs Th(ux) et Th(u y) sont quant
à eux évalués pour le terme principal de la thématique T . Par ailleurs, pour chaque
paire (ux, u y) identiﬁée à partir de la séquence d’activation liée à la thématique T ,
nous choisissons un utilisateur uz, tel que :

— Premièrement, l’utilisateur uz n’a pas relayé l’information décrite par T , c’est-

— Deuxièmement, l’utilisateur uz est exposé aux messages publiés par u y, c’est-
u y, l’ensemble des voisins entrants de u y

à-dire qu’il n’apparaît pas dans la séquence d’activation ;
à-dire qu’il appartient à l’ensemble Γ←
dans G.

Ainsi, pour un utilisateur uz satisfaisant ces conditions, nous construisons une ins-
uz,u y, 0). Au ﬁnal, nous obtenons un jeu de données équilibré à

tance négative : (v t y

111

4.3. Méthode proposée

FIGURE 4.6 – Illustration du processus de construction du jeu de données d’entraîne-

ment. La structure représentée correspond au graphe G, un arc (ux → u y) signiﬁe

donc que l’utilisateur ux est exposé aux messages publiés par u y.

partir duquel nous pouvons estimer la fonction f . La ﬁgure 4.6 illustre ce processus :
les nœuds u1, u2 et u3 ont relayé l’information décrite par une thématique T respec-
tivement en t1, t2 et t3, tandis que les nœuds u4 et u5 n’ont pas relayé l’information.
Estimation des paramètres de f. Nous déﬁnissons la probabilité pux,u y(t) – la
probabilité que l’utilisateur u y inﬂuence ux à un instant t à propos d’une théma-
tique donnée – comme une fonction du vecteur d’attributs les décrivant, c’est-à-dire :
). Comme nous souhaitons que cette fonction soit interprétable et
pux,u y(t) = f (v t
nous permettent d’analyser l’impact des différents attributs sur la probabilité de dif-
w0 +(cid:80)13
fusion, nous proposons d’exprimer f comme une fonction monotone à valeurs dans
[0; 1] d’une combinaison linéaire des composantes du vecteur d’attributs v de la forme
j=1 w j vj. Pour simpliﬁer l’écriture de cette combinaison linéaire, nous modi-
ﬁons le vecteur v de sorte à avoir v0 = 1, ce qui nous permet de la ré-écrire comme
le produit scalaire w · v. La fonction f étant monotone, l’analyse du vecteur de coef-

ux,u y

ﬁcients w permet de quantiﬁer l’impact, négatif ou positif, des différents attributs sur
la probabilité de diffusion de l’information entre deux utilisateurs. Plusieurs formes
paramétriques pour la fonction f sont envisageables. Nous choisissons ici d’utiliser la

112

Modéliser et prévoir la diffusion de l’information

fonction sigmoïde et déﬁnissons donc f de la manière suivante :

f (v) = exp(w · v)
1 + exp(w · v).

Nous proposons d’estimer le vecteur de coefﬁcients w à l’aide du jeu de données
i=1 par maximisation de la vraisemblance. Étant donnée une instance de

D = (vi, yi)n
ce jeu de données, nous avons la relation suivante :

Par ailleurs, la probabilité qu’un vecteur vi soit associé à la modalité yi ∈ {0; 1}

s’écrit comme suit :

f (vi) = P(Y = 1|vi)


P(Y = 1|vi)
1− P(Y = 1|vi)

P(Y = yi|vi) =

si yi = 1,
si yi = 0.

Bénéﬁciant du fait que yi ∈ {0; 1}, nous pouvons écrire cette probabilité d’une

façon plus compacte :

P(Y = yi|vi) = P(Y = 1|vi) yi(1− P(Y = 1|vi))1− yi

Ainsi, en supposant l’indépendance entre les instances du jeu de données D, nous

mesurons la vraisemblance du vecteur w pour le jeu de données D (i.e. P(D|w))

comme suit :

L(D, w) =

=

=

n(cid:89)
n(cid:89)
n(cid:89)

i=1

P(Y = yi|vi)
P(Y = 1|vi) yi(1− P(Y = 1|vi))1− yi
 exp(w · vi)
1 + exp(w · vi)

 yi

1− exp(w · vi)
1 + exp(w · vi)

1− yi

i=1

i=1

Estimer le vecteur w s’apparente donc au problème d’optimisation qui consiste à

113

4.3. Méthode proposée

maximiser la vraisemblance du jeu de données D :

w = argmax

w

L(D, w)

ce qui se fait en dérivant L(D, w) par rapport à w. Or, le logarithme naturel étant
une fonction strictement croissante, maximiser L(D, w) équivaut par conséquent à
maximiser la log-vraisemblance, (cid:96)(D, w) – dont la dérivation est plus simple. Nous
avons donc le problème d’optimisation suivant, équivalent au précédent :

Nous exprimons la log-vraisemblance comme suit :

w

(cid:96)(D, w)

w = argmax
 yi
1− exp(w · vi)


1 + exp(w · vi)
+ (1− yi) ln

1− yi
1− exp(w · vi)
1 + exp(w · vi)

 n(cid:89)

n(cid:88)

i=1

 exp(w · vi)
 exp(w · vi)
1 + exp(w · vi)
1 + exp(w · vi)

yi ln

=

i=1

(cid:96)(D, w) = ln

1− yi

.

En exploitant le fait que :

on obtient alors :

1

=

P(Y = 0|vi) = 1− P(Y = 1|vi) = 1− exp w · vi
1 + exp w · vi

1 + exp(w · vi)
n(cid:88)
 yi(w · vi)− yi ln(1 + exp(w · vi))− (1− yi) ln(1 + exp(w · vi))
n(cid:88)
( yi(w · vi)− ln(1 + exp(w · vi))

i=1

(cid:96)(D, w) =

=

i=1

114

Modéliser et prévoir la diffusion de l’information

Enﬁn, nous exprimons la dérivée partielle de (cid:96)(D, w) par rapport à w comme suit :

∂ (cid:96)(D, w)

∂ wk

=

=

=

∂
∂ wk

n(cid:88)
n(cid:88)

i=1

i=1

1

i=1

n(cid:88)
( yi(w · vi)− ln(1 + exp(w · vi))


yi vik −
1 + exp(w · vi) vik exp(w · vi)

yi − exp(w · vi)
1 + exp(w · vi)






yi − exp(w · vi)
1 + exp(w · vi)

= 0

vik

n(cid:88)

i=1

vik

Or, il apparaît que l’équation :

n’est pas linéaire en w, ce qui ne permet pas de trouver une solution analytique.
Néanmoins, il est possible d’appliquer la méthode numérique itérative de Newton-
Raphson telle qu’elle est décrite par McCullagh et Nelder (1989) pour résoudre ce
système d’équations non-linéaires. Cela nécessite, en plus du vecteur gradient déﬁni
par les dérivées partielles premières, de déﬁnir la matrice Hessienne qui est constituée
des dérivées partielles secondes :

∂ 2(cid:96)(D, w)
∂ wk∂ wl





vik

=

n(cid:88)
= − n(cid:88)
= − n(cid:88)

i=1

i=1

i=1

yi − exp(w · vi)
∂
 exp(w · vi)

1 + exp(w · vi)
∂ wl
∂
1 + exp(w · vi)
∂ wl
exp(w · vi)
(1 + exp(w · vi))2

vikvil

vik

La méthode de Newton-Raphson repose sur la relation suivante entre les solutions
à l’itération p et p + 1, l’inverse de la matrice Hessienne étant la matrice de variance-
covariance des coefﬁcients (McCullagh et Nelder, 1989) :

w p+1 = w p − ∂ 2(cid:96)(D, w)

∂ w∂ w(cid:48)

−1 × ∂ (cid:96)(D, w)

∂ w

115

4.3. Méthode proposée

FIGURE 4.7 – Représentation du classiﬁeur linéaire construit à partir de la fonction f .

Si l’on se donne un seuil (e.g. 0,5), cette démarche ayant pour but l’estimation des
paramètres de la fonction f à partir du jeu de données D conduit à la construction
d’un classiﬁeur binaire, équivalent à un perceptron simple utilisant comme fonction
d’activation la fonction sigmoïde ou un classiﬁeur par régression logistique. Ainsi,
lorsque f (v) est supérieure au seuil, le classiﬁeur prédit la modalité 1, et 0 sinon. La
ﬁgure 4.7 schématise le classiﬁeur construit, avec à gauche les entrées et à droite la
sortie.

Construction du jeu de données et estimation des paramètres de g. Nous dé-
ﬁnissons le délai de transmission µux,u y, exprimé en heures, comme une fonction du
degré d’activité de l’utilisateur ux, Ac(ux), c’est-à-dire : µux,u y = g(Ac(ux)). Plus
particulièrement, nous déﬁnissons g de la façon suivante, de sorte que le délai de
transmission minimum soit d’une heure :



g(Ac(ux)) =

w0 + w1Ac(ux)
1

si w0 + w1Ac(ux) > 1,
sinon.

Pour estimer les paramètres w0 et w1, nous construisons un jeu de données dérivé
. Ce jeu de données est constitué de m = n/2 instances décrites par deux

(cid:48)

de D, noté D
variables (xi, zi), selon le principe suivant :

— Pour chaque instance (vi, yi) de D décrite par un vecteur vi = vux,u y et telle
que yi = 1, nous créons l’instance suivante (Ac(ux), δux,u y), où δux,u y est le
délai de transmission entre u y et ux exprimé en heures. Autrement dit, pour
chaque paire d’utilisateurs (ux, u y) identiﬁée lors de la construction du jeu de
données D et entre lesquels l’information s’est diffusée (cf. ﬁgure 4.6), nous

116

Modéliser et prévoir la diffusion de l’information

(cid:48)

créons une instance de D
degré d’activité de l’utilisateur ux qui a été inﬂuencé par u y.

Nous proposons d’estimer les paramètres w = {w0, w1} selon la méthode des

décrite par le délai de transmission observé, et le

moindres carrés (Cornillon et Matzner-Løber, 2007), i.e. en minimisant la somme des
carrés des différences entre les délais de transmission observés et les délais prédits, le
critère des moindres carrés étant le suivant :

w = argmin

w0,w1

m(cid:88)
(zi − w0 − w1xi)2

i=1

4.4 Expérimentations

Dans cette section, nous présentons la synthèse des résultats obtenus lors de
l’étude expérimentale que nous avons menée avec des données issues de Twitter. Pre-
mièrement, nous examinons la validité de l’approche que nous proposons pour esti-
mer les paramètres de T-BASIC, plus précisément la procédure proposée pour estimer
les paramètres de la fonction f . Ensuite, nous évaluons la capacité du modèle T-BASIC
à prévoir la dynamique temporelle du processus de diffusion, puis pour conclure cette
section, nous analysons l’effet des caractéristiques des utilisateurs sur la diffusion de
l’information.

4.4.1 Protocole expérimental

Corpus. Les résultats expérimentaux que nous présentons dans cette section re-
posent sur trois corpus de messages collectés par Yang et Leskovec (2011). Chacun
représente l’intégralité des tweets écrits en anglais, publiés par 52 494 utilisateurs
américains durant une période d’un mois :

— Le corpus (cid:67)1 : 1 399 840 tweets publiés du 01/10/2009 au 31/11/2009 ;
— (cid:67)2 : 1 437 126 tweets publiés du 01/11/2009 au 30/11/2009 ;
— (cid:67)3 : 1 141 740 tweets publiés du 01/12/2009 au 31/12/2009.

Structure du réseau. Le réseau d’abonnements qui interconnectent ces 52 494
utilisateurs forme un graphe orienté comprenant 5 793 961 liens, collecté ﬁn 2009
par Kwak et al. (2010). Le graphe a un diamètre de 8, tandis que la longueur moyenne
du chemin entre deux nœuds est de 2,55.

117

4.4. Expérimentations

FIGURE 4.8 – Visualisation sous forme de graphe des instances de D ayant pour moda-
lité yi = 1 extraites à partir d’un même évènement.

118

Modéliser et prévoir la diffusion de l’information

données d’apprentissage D et D

(cid:48)

Construction des jeux de données d’apprentissage. Les instances des jeux de
sont décrites par les attributs des utilisateurs mesu-

rés à l’aide du corpus (cid:67)1. Nous utilisons la méthode MABED – que nous avons décrite
dans le chapitre 3 – pour détecter des évènements dans le corpus (cid:67)2, à partir des-

quels nous extrayons les séquences d’activation nécessaires à la construction des jeux
de données d’apprentissages. Plus spéciﬁquement, nous appliquons la méthode MA-
BED avec comme paramètres k = 30, p = 3, θ = 0, 75 et σ = 0, 5, ce qui génère
une liste L de 30 évènements décrits chacun par au plus 3 mots liés. Pour chaque
évènement e ∈ L, décrit par un terme principal, un ensemble de mots liés et un in-

tervalle temporel, nous extrayons une séquence d’activation en identiﬁant les auteurs
des messages publiés durant cet intervalle et contenant le terme principal et les mots
liés à l’évènement. Ceci nous permet, en ordonnant les auteurs des messages selon
les dates de publication, de construire la séquence d’activation liée à cet évènement.
Pour chacune des séquences d’activation ainsi extraite, nous appliquons le processus
que nous avons décrit dans la section précédente aﬁn de générer les instances de D
et D
, avec néanmoins une contrainte supplémentaire. Aﬁn que les instances iden-
tiﬁées soient signiﬁcatives, nous ne considérons que les utilisateurs ayant publié au

moins 6 messages dans chacun des corpus (cid:67)1 – le corpus à partir duquel les attributs
caractérisant les utilisateurs sont mesurés – et (cid:67)2 – le corpus à partir duquel les sé-

(cid:48)

quences d’activation sont extraites. Nous obtenons ainsi le jeu de données D équilibré
comportant 28980 instances et le jeu de données D
comportant 14490 instances. La
ﬁgure 4.8 montre par exemple le dessin d’un graphe où les liens correspondent aux

instances de D, (vi, yi)| yi = 1, extraites à partir d’un des évènements détectés dans
(cid:67)2. Cette visualisation permet de voir comment l’information s’est diffusée spatiale-

(cid:48)

ment à partir du premier utilisateur dans la séquence d’activation, coloré en orange
sur ce dessin.

Construction des jeux de données de test. Aﬁn d’évaluer la procédure proposée
pour estimer les probabilités de diffusion sur lesquelles repose T-BASIC, nous construi-
sons un jeu de données de test noté Dtest et comportant 24722 instances, selon le
même principe que celui employé pour construire D, sauf que les attributs des utilisa-
teurs sont mesurés à partir de (cid:67)2 et que les 30 séquences d’activation sont extraites
à partir de (cid:67)3. Aﬁn d’évaluer la capacité de T-BASIC à prévoir la diffusion de l’infor-

mation, plus précisément sa capacité à prévoir l’évolution du nombre d’utilisateurs

119

4.4. Expérimentations

relayant une information donnée au ﬁl du temps, nous construisons un ensemble (cid:83)
de 30 séries temporelles de référence. Chaque série temporelle sT ∈ (cid:83) est construite

à partir de la séquence d’activation liée de l’un des évènements, et est donc liée à une
thématique T décrite par un terme principal et un ensemble de mots, et un intervalle
temporel I. Les séries temporelles ont une résolution de 4 heures, c’est-à-dire que
sT(0) donne le nombre d’utilisateurs activés durant les 4 premières heures de l’inter-
valle I, sT(1) donne le nombre d’utilisateurs activés entre 4 et 8 heures après le début
de l’évènement, etc.

4.4.2 Évaluation de la procédure d’estimation des probabilités de

diffusion

Dans cette section nous nous intéressons plus particulièrement à l’efﬁcacité de la
procédure que nous proposons pour estimer les probabilités de diffusion – la probabi-
lité pux,u y(t) étant donnée par la fonction f (v t
), dont les paramètres sont estimés à
partir du jeu de données D. Or, comme nous l’avons déjà mentionné, en se dotant d’un
seuil θ, cette procédure s’apparente à la construction d’un classiﬁeur linéaire binaire.
Par conséquent, nous pouvons évaluer la qualité de cette modélisation en évaluant ses
performances pour la tâche qui consiste à classiﬁer les instances du jeu de données
Dtest. Pour une instance vi de Dtest, la modalité prédite est alors donnée par la fonction
h(vi) suivante :

ux,u y



h(vi) =

1 si f (vi) > θ,
0 sinon.

Méthodes comparées. Nous considérons deux classiﬁeurs couramment utilisés en
apprentissage automatique, que nous entraînons avec le jeu de données D : le sépa-
rateur à vaste marge proposé par Cortes et Vapnik (1995) et le classiﬁeur bayésien
naïf, décrit par John et Langley (1995). Un séparateur à vaste marge (SVM) est un
classiﬁeur binaire qui cherche à identiﬁer l’hyperplan séparant les instances avec la
plus vaste marge. Plus spéciﬁquement, nous considérons le SVM à marge souple ca-
pable de traiter les cas non linéairement séparables, qui cherche donc à maximiser
la marge tout en minimisant les erreurs de classiﬁcation. Un classiﬁeur bayésien naïf
est un classiﬁeur simple qui repose sur l’hypothèse selon laquelle les attributs vi j sont
deux à deux indépendants conditionnellement à la valeur yi.

120

Modéliser et prévoir la diffusion de l’information

Choix des paramètres. Nous considérons quatre conﬁgurations pour le SVM :
(i) un SVM utilisant un noyau linéaire (SVM-l), un SVM utilisant un noyau gaussien
(SVM-g), deux SVM utilisant pour l’un un noyau polynomial (SVM-p2) de degré 2 et
l’autre un noyau polynomial de degré 3 (SVM-p3). Le paramètre élémentaire de tout
SVM est la valeur γ qui déﬁnit l’inﬂuence accordé à chaque instance du jeu de données
d’entraînement. Un SVM à marge souple requiert aussi la déﬁnition d’un paramètre
C > 0 caractérisant le compromis entre maximisation de la marge et minimisation des
erreurs de classiﬁcation. Pour chacune de ces quatre conﬁgurations, nous appliquons
la démarche « grid-search », c’est-à-dire une recherche exhaustive, en utilisant la vali-
−4 et 104 aﬁn d’identiﬁer le
dation croisée sur D, pour des valeurs comprises entre 10
couple (γ, C) donnant les meilleurs résultats. Enﬁn, nous ﬁxons θ = 1
2, ce qui signiﬁe
que le classiﬁeur h basé sur la fonction f prédit la modalité la plus probable.

Métriques d’évaluation. Puisque nous sommes intéressés par la capacité à pré-
voir la diffusion de l’information entre des paires d’utilisateurs connectés, nous me-
surons donc la précision par rapport à la modalité y = 1, comme le rapport entre le
nombre d’instances de Dtest correctement associées par le classiﬁeur à la modalité 1 et
le nombre total d’instances associées par le classiﬁeur à la modalité 1, c’est-à-dire :
P = nombre d’instances correctement associées par le classiﬁeur à la modalité y = 1

nombre d’instances associées par le classiﬁeur à la modalité y = 1

De même, nous déﬁnissons le rappel par rapport à la modalité y = 1 comme
le rapport entre le nombre d’instances correctement associées par le classiﬁeur à la
modalité 1 et le nombre total d’instances associées à la modalité 1 dans Dtest, c’est-à-
dire :
R = nombre d’instances correctement associées par le classiﬁeur à la modalité y = 1

nombre d’instances réellement associées à la modalité y = 1

Enﬁn, nous combinons précision et rappel en calculant la F-mesure, déﬁnie comme

la moyenne harmonique de ces deux métriques :

F = 2PR
P + R

La table 4.4 reporte la précision, le rappel et la F-mesure obtenus par chaque

121

4.4. Expérimentations

TABLE 4.4 – Performances des six classiﬁeurs sur le jeu de données Dtest.

Métrique
Précision
Rappel
F-mesure

(h | f,θ ) SVM-l SVM-g SVM-p2 SVM-p3 Bayésien naïf

0,700
0,822
0,756

0,712
0,799
0,753

0,712
0,798
0,753

0,721
0,755
0,738

0,688
0,692
0,690

0,708
0,777
0,741

classiﬁeur sur le jeu de données Dtest.

Validité de la procédure d’estimation des paramètres de la fonction f. La lec-
ture de la table 4.4 révèle que tous les classiﬁeurs obtiennent des performances sa-
tisfaisantes sur le jeu de données équilibré Dtest, la F-mesure la plus faible (0, 690)
étant obtenue par le SVM à noyau polynomial de degré 3. Nous observons que la
meilleure précision est obtenue par le SVM à noyau polynomial de degré 2, tandis
que le meilleur rappel et la meilleure F-mesure sont atteints avec le classiﬁeur h basé
sur le couple ( f , θ). La consistance des résultats obtenus en classiﬁcation montre le
pouvoir prédictif des attributs retenus pour constituer l’espace de représentation des
utilisateurs, i.e. les vecteurs à partir desquels les probabilités de diffusion du modèle
T-BASIC sont estimées. Globalement, nous observons que le rappel est supérieur à la
précision. Ceci suggère l’existence d’un motif spéciﬁque lié aux caractéristiques des
utilisateurs et qui se retrouve très souvent lorsque l’information se diffuse d’un uti-
lisateur vers un autre. Néanmoins, le fait que la précision soit inférieure au rappel
indique que la présence de ce motif n’induit pas systématiquement la diffusion de
l’information.

4.4.3 Évaluation du modèle T-BASIC

Dans cette section, nous évaluons la capacité de T-BASIC à prévoir la diffusion

de l’information en nous basant sur l’ensemble (cid:83) contenant 30 séries temporelles
de référence, extraites à partir du corpus (cid:67)3. Chaque série temporelle sT ∈ (cid:83) décrit

l’évolution réelle du volume d’utilisateurs relayant une thématique T décrite par un
mot principal et un ensemble de mots liés, durant un intervalle temporel I. Il s’agit
donc de prévoir la série temporelle ˆsT représentant l’évolution du volume d’utilisa-
teurs inﬂuencés à propos de la thématique T durant l’intervalle temporel I.

Méthodes comparées. Nous considérons la même méthode de référence que celle

122

Modéliser et prévoir la diffusion de l’information

utilisée par Yang et Leskovec (2010) pour évaluer le Linear Inﬂuence Model, à savoir le
One-time-lag Predictor (OTL). Cette méthode simple consiste à prédire ˆsT(t) comme
étant égal à sT(t − 1). Nous considérons d’autre part une variante de T-BASIC, αT-
BASIC, telle que les probabilités de diffusion soient constantes, i.e. indépendantes du
(cid:48)),
temps. Pour αT-BASIC nous déﬁnissons donc pux,u y comme la moyenne de pux,u y(t
mesurée en chacune des 6 tranches temporelles considérées pour l’attribut Re, c’est-
à-dire :

pux,u y(t

(cid:48))

, où t’ ∈ {0h,4h,8h,12h,16h,20h}

pux,u y =(cid:88)

t(cid:48)

6

Métriques d’évaluation. Comme Yang et Leskovec (2010), nous mesurons la dif-
férence entre le volume réel d’utilisateurs inﬂuencés à propos d’une thématique T et
le volume prédit, ˆsT(t), à un instant t :

ET(t) = sT(t)− ˆsT(t)

à partir de laquelle nous déﬁnissons l’erreur relative globale par rapport au vo-

lume :

Evolume =

(cid:80)
(cid:80)

T,t ET(t)2
T,t sT(t)2

des jeux de données D et D

Choix des paramètres. Les paramètres des fonctions f et g sont estimés à l’aide

(pour rappel, les attributs sont mesurés à partir de (cid:67)1
et les séquences d’activation sont extraites à partir de (cid:67)2). Pour la prédiction, les at-
tributs des utilisateurs sont à nouveau mesurés à partir du corpus (cid:67)2. La prédiction

(cid:48)

est réalisée avec un pas d’une heure, autrement dit, T-BASIC et αT-BASIC prédisent le
nombre d’utilisateurs nouvellement activés par heure. Pour chacune des thématiques
T dont on cherche à prévoir la diffusion, l’ensemble S d’utilisateurs initiant le pro-
cessus de diffusion est déﬁni comme l’ensemble réel des utilisateurs activés durant la
première heure suivant le début de l’évènement décrit par T (i.e. la prévision débute
après 1 heure d’observation). Aﬁn de lisser les prédictions et d’obtenir une résolution
identique à celle des séries temporelles de référence, 4 heures, le volume prédit par
T-BASIC et αT-BASIC est cumulé par tranches successives de 4 heures.

Analyse des performances de T-BASIC. La table 4.5 donne l’erreur mesurée par
rapport au volume pour chacune des trois méthodes. Nous comparons les perfor-

123

4.4. Expérimentations

24/12 12h 24/12 20h

25/12 4h
Temps (UTC)

25/12 12h 25/12 20h

(a) Exemple pour le premier cas.

8/12 18h

9/12 02h

9/12 10h
Temps (UTC)

9/12 18h

9/12 22h

(b) Exemple pour le deuxième cas.

s
é
c
n
e
u
ﬂ
n

i

s
r
u
e
t
a
s
i
l
i
t

U

s
é
c
n
e
u
ﬂ
n

i

s
r
u
e
t
a
s
i
l
i
t

U

s
é
c
n
e
u
ﬂ
n

i

s
r
u
e
t
a
s
i
l
i
t

U

25/12 18h 26/12 02h 26/12 10h 26/12 18h 26/12 22h

Temps (UTC)

(c) Exemple pour le troisième cas.

FIGURE 4.9 – Séries temporelles réelles et prédites représentant l’évolution du volume
d’utilisateurs inﬂuencés, pour trois processus de diffusion représentatifs. Les points
symbolisés par des carrés correspondent aux séries temporelles réelles, tandis que
ceux symbolisés par des cercle correspondent aux séries temporelles prédites.

124

Modéliser et prévoir la diffusion de l’information

TABLE 4.5 – Erreur mesurée pour les trois méthodes.

Métrique One-time-lag Predictor T-BASIC αT-BASIC

Evolume

0,522

0,412

0,447

mances de T-BASIC et αT-BASIC par rapport à celles du One-time-lag Predictor en me-
surant le taux de réduction de l’erreur par rapport au volume, r, de la façon suivante :

rT-BASIC = Evolume(T-BASIC)− Evolume(OTL)
rαT-BASIC = Evolume(αT-BASIC)− Evolume(OTL)

Evolume(OTL)

Evolume(OTL)

Le modèle T-BASIC réduit l’erreur relative globale sur le volume de 21,2% par
rapport au One-time-lag Predictor, tandis que la variante αT-BASIC ne réduit l’erreur
relative globale sur le volume que de 14,3%. Ceci suggère que la dynamique globale
du phénomène de diffusion ne dépend pas uniquement de la topologie du réseau
social à travers lequel l’information se propage, mais dépend également de la ﬂuctua-
tion du niveau de réceptivité de chaque utilisateur. L’analyse approfondie des résultats
indique que les performances de T-BASIC semblent dépendre du type d’évènement lié
au phénomène de diffusion à prévoir. En examinant par quels liens du réseau l’infor-
mation s’est propagée et les probabilités de diffusion associées, nous identiﬁons trois
cas. T-BASIC obtient les meilleurs résultats dans deux de ces cas :

— Le premier cas correspond à des évènements rapidement relayés par une part
importante des utilisateurs. La diffusion étant initiée par un ensemble S conte-
nant beaucoup d’utilisateurs répartis à travers le réseau, l’information se pro-
page rapidement en passant même par des liens associés à des probabilités de
diffusion faibles, grâce à l’effet de degré déjà observé par (Katona et al., 2011).
L’allure des séries temporelles réelles et prédites présente dans ce cas un seul
pic d’activation important dès le début de la diffusion. La ﬁgure 4.9.a illustre
l’un de ces cas, basé sur un évènement détecté le 24 décembre 2009, décrit par
le mot principal « xmas », et les mots liés « merry » et « hope ». Cet évènement
est créé par les utilisateurs qui souhaitent un joyeux noël à leurs abonnés.

125

4.4. Expérimentations

— Le second cas correspond à des évènements relayés par une faible part des
utilisateurs. La diffusion est alors initiée par un ensemble S contenant peu
d’utilisateurs et l’information se propage principalement via des liens associés
à des probabilités de diffusion élevées. L’allure des séries temporelles réelles
et prédites présente dans ce cas des pics d’activation successifs de moins en
moins importants, dus à la ﬂuctuation du niveau d’attention des utilisateurs et
aux délais de transmission. La ﬁgure 4.9.b illustre l’un de ces cas, basé sur un
évènement détecté le 8 décembre 2009, décrit par le mot principal « chrome »,
et les mots liés « google », « mac » et « beta ». Cet évènement fait référence à
la sortie en version beta du navigateur Chrome développé par Google pour le
système d’exploitation Mac OS.

Dans un troisième cas, nous constatons que T-BASIC obtient de moins bonnes per-
formances car les séries temporelles réelles présentent une allure irrégulière. Dans ce
cas, il semble que des sources d’inﬂuence externes suscitent de nombreuses activation
à travers le réseau tout au long de l’évènement. Puisque T-BASIC modélise la diffusion
en se basant uniquement sur l’inﬂuence entre utilisateurs, l’allure de la série tempo-
relle prédite ne correspond donc pas à celle observée. La ﬁgure 4.9.c illustre l’un de ces
cas, basé sur un évènement détecté le 25 décembre 2009, décrit par le mot principal
« ﬂight », et les mots liés « northwest » et « passenger ». Cet évènement fait référence
à l’attentat terroriste manqué d’un passager d’un vol de la compagnie aérienne North-
west le jour de noël. L’article Wikipédia 2 consacré à cette attaque manquée témoigne
de l’intense activité médiatique engendrée par cet évènement, puisqu’il fait référence
à plus de 170 articles de presse, principalement publiés entre le 25 décembre et le 31
décembre 2009.

Ayant montré la validité de la procédure d’estimation des paramètres de T-BASIC
puis la capacité du modèle à prévoir la diffusion de l’information, nous analysons,
dans la section suivante, comment et dans quelle mesure les caractéristiques des uti-
lisateurs affectent le processus de diffusion de l’information.

2. Lien vers l’article : http://en.wikipedia.org/wiki/Northwest_Airlines_Flight_253

126

Modéliser et prévoir la diffusion de l’information

4.4.4 Analyse des facteurs impactant la diffusion de l’information

Dans cette section, nous étudions les paramètres de la fonction f (v t

) qui mo-
délise la probabilité que l’utilisateur u y inﬂuence ux à un instant t à propos d’une
thématique donnée T , pux,u y(t). Aﬁn de quantiﬁer l’effet des facteurs sociaux, thé-
matiques et temporels, nous mesurons les rapports de cotes – « odds-ratio » selon la
terminologie anglophone (Mosteller, 1968) – déﬁnis ici comme le rapport de la cote
de l’évènement y = 1 étant donné un vecteur v1 avec celle de l’évènement y = 1 étant
donné un vecteur v2.

ux,u y

Tout d’abord, la cote de l’évènement y = 1 sachant v, i.e. la diffusion d’une infor-

mation entre une paire d’utilisateurs décrits par le vecteur v, est mesurée ainsi :

odds(v) = P(Y = 1|v)
1− P(Y = 1|v)

= f (v)
1− f (v)

Le rapport de cotes pour deux vecteurs d’attributs v1 et v2 est alors mesuré comme

suit :

odds-ratio(v1, v2) = odds(v1)
odds(v2)

Cette mesure indique donc combien de fois il y a « plus de chances » d’avoir y = 1
au lieu d’avoir y = 0 lorsque l’on a v1 au lieu de v2. Aﬁn d’examiner l’impact de chaque
attribut indépendamment des autres, nous mesurons les rapports de cotes pour des
couples de vecteurs différant en une seule composante. Si nous nous intéressons par
exemple à l’effet de l’attribut binaire Th(ux) sur la probabilité pux,u y(t) – la probabilité
que l’utilisateur u y inﬂuence ux à un instant t à propos d’une thématique T , nous
notons :

Th(ux) = 1
Th(ux) = 0

= odds-ratio(v1, v2)

où les valeurs des composantes des vecteurs v1 et v2 sont les mêmes exceptées
celles correspondant à Th(ux), qui vaut 1 pour v1 et 0 pour v2. Par déﬁnition, un
rapport de cotes est toujours supérieur ou égal à 0, i.e. odds-ratio(v1, v2) ∈ [0; +∞[,

et s’interprète dans le cas présent de cette façon :

— Lorsque odds-ratio(v1, v2) vaut 1, l’attribut qui varie entre v1 et v2 n’a aucun

effet sur la probabilité de diffusion ;

— Lorsque le rapport est supérieur à 1, la valeur de l’attribut considéré a un effet

127

4.4. Expérimentations

attribut variant pour ux
attribut variant pour u y

Tm=1
Tm=0

Ro=1
Ro=0

Th(T)=1
Th(T)=0

Re(t)=1
Re(t)=0

Ac=1
Ac=0

−2

10

−1

10

100

101

102

FIGURE 4.10 – Rapports de cotes pour différents attributs, mesurés par rapport aux
utilisateurs ux et u y. La direction des barres traduit la direction de la relation entre
chaque attribut et la probabilité de diffusion : vers la gauche, l’effet est négatif, vers
la droite, l’effet est positif.

positif sur la probabilité de diffusion ;

— Au contraire, lorsque odds-ratio(v1, v2) est inférieur à 1, l’attribut étudié à un

effet négatif sur la probabilité de diffusion.

La ﬁgure 4.10 donne les rapports de cotes en fonction de pux,u y(t) pour différents
attributs, mesurés par rapport à ux et u y. Comme les attributs sont soit numériques
à valeurs dans [0; 1] ou binaires avec pour valeur 1 ou 0, nous mesurons les rapports
de cotes pour un attribut valant soit 1 soit 0. Bénéﬁciant du fait que le logarithme
naturel est monotone et qu’il s’annule en 1, les rapports de cotes sont présentés selon
une échelle logarithme, ce qui facilite la lecture du diagramme en faisant ressortir
clairement la direction de la relation entre chaque attribut et la probabilité de diffu-
sion.

Impact des attributs sociaux. Il apparaît logiquement que le facteur ayant la plus
forte inﬂuence positive sur la probabilité de diffusion est le degré d’activité de l’uti-
lisateur ux. Plus cet utilisateur est actif, i.e. a posté beaucoup de messages dans le
passé, plus la probabilité de diffusion est importante. Au contraire, nous constatons
qu’un important degré d’activité pour l’utilisateur u y a plutôt tendance à avoir un

128

Modéliser et prévoir la diffusion de l’information

effet négatif sur pux,u y(t). Cela peut potentiellement s’expliquer par la dilution de l’in-
ﬂuence de l’utilisateur proportionnellement au volume de messages qu’il produit. En
effet, en publiant beaucoup de messages l’utilisateur contribue à diminuer le temps
durant lequel ses voisins seront exposés à chacun de ses messages. Par conséquent,
même si une activité importante peut contribuer à l’inﬂuence globale d’un utilisa-
teur, cela a tendance à diminuer son inﬂuence localement liée à chaque message. Par
ailleurs, nous constatons que les taux de mention des deux utilisateurs ux et u y –
que l’on peut voir comme un indicateur de popularité ou d’autorité – ont un effet
important mais opposé. Nous constatons que plus un utilisateur est populaire ou jouit
d’une autorité importante, moins il est inﬂuençable alors que dans le même temps
cela renforce l’inﬂuence qu’il exerce sur ses voisins. Par ailleurs, la ﬁgure 4.10 révèle
que l’attribut Ro(ux) qui caractérise le rôle de l’utilisateur ux (qui lorsqu’il est proche
de 1 indique que ux participe de manière active au processus de diffusion en dirigeant
l’information vers les utilisateurs qu’il cible directement) a un effet négatif important
sur la probabilité de diffusion. Cela signiﬁe qu’un utilisateur passif est plus suscep-
tible d’être inﬂuencé qu’un utilisateur plus actif par rapport au processus de diffusion.
Enﬁn, nous constatons que la valeur de l’attribut H(ux, u y) – qui mesure la similarité
entre les deux ensembles d’utilisateurs avec lesquels ux et u y interagissent – a un ef-
fet positif sur la probabilité de diffusion, ce qui suggère que l’attribut H évalue d’une
façon pertinente la similarité entre les centres d’intérêt des deux utilisateurs.

Impact des attributs thématiques. Nous observons que l’attribut thématique
Th(ux, T) – qui vaut 1 si ux a publié par le passé un message contenant le mot prin-
cipal de la thématique T et 0 sinon – a un effet positif sur la probabilité de diffusion,
puisque nous mesurons un rapport de cotes entre Th(ux, T) = 1 et Th(ux, T) = 0 de
5, 05, tandis que pour Th(u y, T) = 1 et Th(u y, T) = 0 le rapport de cotes mesuré n’est
que de 1, 26. Cela indique qu’un utilisateur ayant déjà abordé une thématique proche
de T par le passé est plus susceptible d’être inﬂuencé à son propos, peu importe que
l’utilisateur qui exerce l’inﬂuence soit familier avec ou non.

Impact des attributs temporels. Enﬁn, nous constatons que la transmission d’in-
formation de l’utilisateur u y vers ux est d’autant plus probable que la valeur de l’at-
tribut Re(t) est élevée, le rapport de cotes mesuré entre Re(ux, t) = 0 et Re(ux, t) = 1
valant 5,99. Autrement dit, il est plus probable que l’utilisateur ux soit inﬂuencé par
un message publié à un instant t s’il est habituellement actif durant la période de la

129

4.4. Expérimentations

1 : diffusion

0 : non-diffusion

s
e
c
n
a
t
s
n

i
’

d
e
r
b
m
o
N

0

0.2

0.4

Re(ux)

0.6

0.8

FIGURE 4.11 – Distribution des valeurs de Re(ux) en fonction de la modalité prise par
yi (1 : diffusion, 0 : non-diffusion).

journée associé à t. Pour souligner l’effet de la ﬂuctuation du niveau de réceptivité sur
le phénomène de diffusion de l’information, nous montrons avec la ﬁgure 4.11 la dis-
tribution des valeurs de l’attribut Re(ux) dans D en fonction de la modalité prise par
yi, 0 ou 1. Bien que les deux distributions aient des allures similaires pour Re(ux) > 0,
il apparaît un clair déséquilibre pour Re(ux) = 0 puisqu’il y a environ 4,1 fois plus
d’instances pour lesquelles yi vaut 0 que 1.

Globalement, l’étude des facteurs impactant la diffusion de l’information nous
apprend deux choses. Premièrement, les trois aspects considérés sont importants,

TABLE 4.6 – Effet des caractéristiques des utilisateurs sur la probabilité de diffusion :
la couleur orange traduit un effet positif tandis que la couleur grise traduit un effet
négatif, l’intensité de la couleur traduisant l’importance de l’effet mesurée selon le
log-odds-ratio.

Social

Ac Hs Ro Me Tm

Thématique Temps
Re(t)

Th(T )

ux
u y

130

Modéliser et prévoir la diffusion de l’information

puisque l’activité sociale, la familiarité avec la thématique à diffuser ainsi que la ﬂuc-
tuation du niveau d’attention agissent signiﬁcativement sur la probabilité de diffusion.
Deuxièmement, nous observons que la probabilité de diffusion est plus fortement liée
aux caractéristiques de l’utilisateur subissant l’inﬂuence que de celles de l’utilisateur
qui l’exerce. Pour illustrer ce constat, la table 4.6 présente un tableau structurant les
caractéristiques d’une paire quelconque d’utilisateurs, et dont les cellules sont colo-
rées selon leur impact sur la probabilité de diffusion. La couleur orange symbolise un
effet positif et le gris un effet négatif, plus ou moins important selon l’intensité. La pre-
mière ligne de cette table liste l’effet des attributs de l’utilisateur subissant l’inﬂuence
et ressort nettement par rapport à la deuxième ligne qui se rapporte à l’utilisateur
exerçant l’inﬂuence.

4.5 Discussion

Pour conclure ce chapitre, nous résumons dans un premier temps les travaux que
nous venons de présenter. Ensuite, nous discutons des pistes de recherche ouvertes
par ces travaux.

4.5.1 Résumé des travaux présentés

Dans ce chapitre, nous avons décrit T-BASIC, une modélisation probabiliste de la
diffusion de l’information dans les médias sociaux basée sur la structure de réseau
interconnectant les utilisateurs, ainsi qu’une procédure pour estimer les paramètres
latents du modèle (i.e. la probabilité de diffusion et le délai de transmission pour
chaque lien). Ce modèle se distingue des approches existantes basées sur la struc-
ture du réseau, de par le fait que la probabilité de diffusion en chaque lien n’est pas
constante mais dépendante du temps. La procédure d’estimation des paramètres dif-
fère également des procédures existantes, de par le fait qu’elle ne vise pas à estimer
directement tous les paramètres latents à partir de séquences d’activation. Nous pro-
posons plutôt d’exprimer la probabilité de diffusion en chaque lien du réseau comme
une fonction f de caractéristiques observables des utilisateurs, et le délai de trans-
mission comme une fonction g de leurs caractéristiques. Ainsi, estimer les paramètres
du modèle T-BASIC consiste à estimer les paramètres déﬁnissant les fonctions f et g

131

4.5. Discussion

à partir de séquences d’activation et de mesures des caractéristiques des utilisateurs,
ce qui a pour effet de diminuer le coût d’estimation global. Les expérimentations que
nous avons menées avec des données collectées sur Twitter ont démontré la perti-
nence de notre proposition. Dans un premier temps, nous avons examiné la validité
de la procédure d’estimation des paramètres et avons montré son efﬁcacité. Dans un
second temps, nous avons évalué les capacités prédictives du modèle T-BASIC. Il ap-
paraît notamment que ses performances sont supérieures à celles d’αT-BASIC, une
variante où les probabilités de diffusion en chaque lien du réseau sont indépendantes
du temps. Nous avons pu ainsi conﬁrmer l’effet de la ﬂuctuation du niveau de récep-
tivité des utilisateurs au ﬁl du temps sur le phénomène de diffusion de l’information.
Enﬁn, l’analyse des paramètres de la fonction f nous a permis d’étudier l’impact des
caractéristiques des utilisateurs sur la probabilité de diffusion. Nous avons montré
que leurs caractéristiques, tant sur le plan social, que sur le plan thématique ou tem-
porel, affectent le phénomène de diffusion de l’information. Par ailleurs, nous avons
observé que la probabilité de diffusion est plus fortement liée aux caractéristiques de
l’utilisateur inﬂuencé, qu’à celles de l’utilisateur exerçant l’inﬂuence.

Publication. Ces travaux ont notamment fait l’objet d’un article long présenté à
l’atelier international MSND organisé conjointement avec la conférence internationale
WWW en 2012.

4.5.2 Perspectives de travail

Les travaux entamés à propos de la modélisation et la prévision de la diffusion de

l’information ouvrent diverses perspectives de travail.

Notamment, nous identiﬁons diverses pistes pour l’amélioration de T-BASIC. Dans
les travaux présentés, nous supposons implicitement que la structure du réseau ser-
vant de support au processus de diffusion reste constante dans le temps. Néanmoins,
il semblerait pertinent de considérer ce réseau comme dynamique. En effet, en se pro-
pageant, l’information peut susciter la création de nouveaux liens à travers le réseau,
et de nouveaux utilisateurs peuvent également intégrer le réseau, ce qui peut inﬂuer
sur la dynamique de diffusion. Nous supposons également qu’une thématique don-
née se diffuse indépendamment des autres thématiques relayées en parallèle par les
utilisateurs. Toutefois, comme le montre l’étude menée par Myers et Leskovec (2012),

132

Modéliser et prévoir la diffusion de l’information

les différents processus de diffusion simultanés peuvent interagir entre eux et entrer
en compétition, ce qui affecte les probabilités de diffusion entre les utilisateurs. Plus
précisément, ils observent que la diffusion d’une thématique très populaire peut fa-
voriser la propagation de thématiques proches moins populaires, tandis qu’elle limite
encore plus la diffusion des thématiques très différentes et peu populaires. Enﬁn,
une autre piste pour améliorer T-BASIC consisterait à relâcher l’hypothèse de monde
fermé, puisque dans certains cas, des sources d’inﬂuence externes au réseau peuvent
jouer un rôle prépondérant dans la diffusion de l’information. Myers et al. (2012) s’in-
téressent d’ailleurs à cette question et mènent une étude sur Twitter, qui révèle qu’en
moyenne 71% des utilisateurs sont activés du fait de l’inﬂuence interne au réseau, les
29% d’activations restantes pouvant être attribuées à une inﬂuence externe.

Par ailleurs, nous identiﬁons une piste de recherche intéressante concernant le
problème de maximisation de la diffusion de l’information. En effet, en analysant
l’impact des caractéristiques des utilisateurs sur les probabilités de diffusion, nous
observons que ce sont les caractéristiques des utilisateurs subissant l’inﬂuence qui
affectent le plus le phénomène de diffusion. Partant de ce constat, il serait possible de
traiter le problème de maximisation de la diffusion en recherchant les utilisateurs les
plus susceptibles d’être inﬂuencés, tandis que les méthodes existantes (Kempe, 2003;
Even-Dar et Shapira, 2007; Saito et al., 2010b) recherchent les utilisateurs le plus
susceptible d’exercer une inﬂuence importante.

133

Chapitre 5

Un logiciel libre pour la détection d’évè-
nements et l’analyse de l’inﬂuence dans
les médias sociaux

Sommaire

5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
5.2 État de l’art
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139

5.2.1 Détection d’évènements et analyse de l’inﬂuence dans les mé-

dias sociaux . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139

5.2.2 Logiciels pour la fouille et l’analyse de données issues des

médias sociaux . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
5.2.3 Synthèse de l’état de l’art . . . . . . . . . . . . . . . . . . . . . . . 147
5.3 Logiciel proposé . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148

5.3.1 But du logiciel, publics visés et architecture générale . . . . . . 149
5.3.2 Service de manipulation des données . . . . . . . . . . . . . . . 152
5.3.3 Service de détection d’évènements . . . . . . . . . . . . . . . . . 154
5.3.4 Service d’analyse du réseau social
. . . . . . . . . . . . . . . . . 157
. . . . . . . . . . . . . . . 160
5.3.5 Service d’import d’algorithmes et API
5.4 Exemples de scénarios d’utilisation . . . . . . . . . . . . . . . . . . . 161

. . . . . . . . . . . . . . . . . . . . 161
5.4.1 Utilisation par un non-expert
5.4.2 Utilisation par un chercheur du domaine . . . . . . . . . . . . . 165
5.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170

135

5.1. Introduction

Dans ce chapitre, nous présentons la troisième contribution apportée par ces tra-
vaux de thèse. Il s’agit d’un logiciel libre que nous développons pour la fouille et
l’analyse des données issues des médias sociaux, autant destiné aux chercheurs du
domaine qu’à des non-experts.

5.1 Introduction

Les précédents chapitres ont permis de montrer la richesse de la recherche menée
à propos des médias sociaux et de l’analyse du phénomène de diffusion de l’informa-
tion. L’abondance de la recherche sur cette question traduit directement l’intérêt que
portent de nombreux acteurs de la société aux médias sociaux et à leur analyse. Parmi
ces acteurs, on retrouve par exemple les entreprises qui cherchent à analyser les ré-
actions des consommateurs à propos de leurs produits et à promouvoir ces derniers.
Parmi ces acteurs se trouvent également les services de renseignement, qui cherchent
à identiﬁer des discussions suspectes et des personnes potentiellement dangereuses,
mais aussi les services météorologiques, qui cherchent à détecter des phénomènes na-
turels dangereux et difﬁcilement prévisibles, à partir d’éléments relayés par le grand
public. Les journalistes cherchent également à tirer parti des médias sociaux pour dé-
tecter les évènements marquants, identiﬁer des personnes à interroger à leur propos,
et démarrer leurs investigations. Quel que soit le cas d’application, le processus d’ana-
lyse sous-jacent reste le même : partant d’un sujet d’étude, i.e. des données collectées
à partir d’un média social, on cherche dans un premier temps à identiﬁer les prin-
cipaux évènements animant les discussions, puis dans un second temps on cherche
à identiﬁer des personnes inﬂuentes par rapport à ces évènements aﬁn de prendre
des décisions et éventuellement agir. Dans le cas d’une entreprise, le sujet d’étude
peut être un ensemble de messages contenant le nom de la marque associée, et les
évènements peuvent correspondre aux réactions suscitées par la sortie de nouveaux
produits de la marque par exemple. En identiﬁant les personnes inﬂuentes par rapport
aux divers produits, l’entreprise peut alors mettre en place des campagnes de promo-
tion virales, ciblées et plus efﬁcaces. Dans le cas des services de renseignement, le
sujet d’étude peut être un ensemble de messages contenant des mots-clés spéciﬁques,
ou bien un ensemble d’utilisateurs suspects ainsi que les messages qu’ils ont publiés.
Les évènements peuvent correspondre à des menaces ou crises susceptibles d’affecter

136

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

un état, et les personnes inﬂuentes au sein des sous-réseaux liés à ces évènements
peuvent alors correspondre à des criminels ou des leaders d’opinion dangereux. En-
ﬁn, dans le cas d’une analyse journalistique des médias sociaux, le sujet d’étude peut
être l’ensemble des messages publiés sur un média social, ou bien un sous-ensemble
plus spéciﬁque, e.g. lié à la politique, le sport ou la technologie. Les évènements dé-
tectés peuvent alors faire l’objet d’une investigation journalistique, laquelle peut être
menée en commençant par interroger les personnes inﬂuentes par rapport à chaque
évènement, etc.

Étant donné le grand volume de données produites par les médias sociaux, pour
que leur analyse soit efﬁcace et utile, celle-ci doit reposer sur des techniques de fouille
de données adaptées. De nombreuses méthodes ont été récemment proposées pour
détecter les évènements (Shamma et al., 2011; Weng et Lee, 2011; Lau et al., 2012;
Yuheng et al., 2012; Li et al., 2012; Parikh et Karlapalem, 2013; Benhardus et Ka-
lita, 2013; Guille et Favre, 2014a) et analyser l’inﬂuence (Page et al., 1998; Kitsak
et al., 2010; Brown et Feng, 2011; Dugué et Perez, 2014) à partir de données pro-
duites par les médias sociaux. Néanmoins, les chercheurs développant ces méthodes
ne partagent pas systématiquement leurs implémentations et lorsque c’est le cas, elles
sont programmées dans des langages différents, requièrent des formatages de don-
nées différents, etc. Ceci constitue un problème fondamental dans la mesure où – en
particulier dans le domaine de la recherche en informatique – la reproductibilité est
essentielle pour quantiﬁer les progrès réalisés, mais aussi parce que cela ne favorise
pas la réutilisation de ces méthodes par des non-experts (e.g. journalistes, enquêteurs,
analystes médias) pour analyser les données dont ils disposent. Nous constatons par
ailleurs que les méthodes pour l’analyse de l’inﬂuence au sein des médias sociaux,
via l’analyse de la structure des réseaux sociaux sous-jacents, sont souvent utilisées
pour étudier de grandes populations d’utilisateurs sans considération pour les théma-
tiques à propos desquelles l’inﬂuence s’exerce (Bi et al., 2014). Les utilisateurs des
médias sociaux réagissant notamment par rapport aux évènements, une manière de
rafﬁner l’analyse de l’inﬂuence consiste justement à la mesurer par rapport aux évène-
ments. Face à ces constatations, nous sommes donc amenés à formuler les questions
suivantes. D’abord, comment favoriser le partage et la réutilisation par les chercheurs
des implémentations des méthodes développées pour la détection d’évènements et l’ana-
lyse de l’inﬂuence ? Ensuite, comment permettre aux non-experts d’explorer facilement

137

5.1. Introduction

les données dont ils disposent ? Bien qu’il existe des logiciels libres développés dans le
milieu académique pour analyser l’inﬂuence dans les réseaux (Auber, 2004; Bastian
et al., 2009), il n’existe pas encore de logiciel libre dédié à la détection d’évènements.
Il existe par ailleurs des logiciels commerciaux – dont les plus connus sont SAP So-
cial Media Analytics 1, NetBase 2 et BrandWatch Analytics 3 – permettant une certaine
analyse des évènements et de l’inﬂuence dans les médias sociaux. Néanmoins, ils
n’apportent pas de solutions tangibles puisque ce sont des logiciels propriétaires qui
ne dévoilent pas les algorithmes sur lesquels ils reposent, ce qui est problématique
lorsqu’il s’agit d’interpréter les résultats qu’ils produisent et évaluer leur validité.

Proposition et positionnement. Nous proposons SONDY (SOcial Network DYna-
mics), qui est – à notre connaissance – le premier outil libre et extensible pour la
détection d’évènements et l’analyse de l’inﬂuence à partir de données collectées sur
les médias sociaux. Ces données consiste d’une part en un corpus de messages et
d’autre part en la structure du réseau social interconnectant leurs auteurs. SONDY est
un logiciel écrit en langage Java, qui inclut des outils de visualisation et implémente
plusieurs méthodes de la littérature pour la détection d’évènements et l’analyse de
l’inﬂuence, ainsi que des fonctionnalités avancées de préparation des données.

Contrairement aux logiciels académiques existants qui se concentrent soit sur
l’analyse des messages soit sur l’analyse du réseau, SONDY permet d’analyser ces deux
types de données conjointement, en permettant l’analyse de l’inﬂuence par rapport
aux évènements. L’application est conçue de telle sorte qu’il soit simple d’y ajouter de
nouveaux algorithmes en utilisant l’interface de programmation qu’elle fournit. Elle
peut par ailleurs être utilisée comme bibliothèque au sein de tout programme, ce qui
permet d’automatiser son fonctionnement. Son interface graphique simpliﬁe son uti-
lisation par des non-experts qui peuvent alors explorer les données dont ils disposent
à l’aide de méthodes et visualisations adaptées, sans avoir de connaissances poussées
en informatique.

Scénarios d’utilisation. Nous illustrons, à l’aide d’exemples et de captures d’écran,
des scénarios d’utilisation montrant comment SONDY peut être utilisé, que ce soit par
un non-expert (tel qu’un journaliste recherchant des informations et des personnes
à interroger à propos d’évènements spéciﬁques, ou bien un analyste média souhai-

1. SAP Social Media Analytics : http://www.news-sap.com/power-social-media-analytics/
2. NetBase : http://www.netbase.com
3. BrandWatch Analytics : http://www.brandwatch.com/brandwatch-analytics/

138

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

tant analyser l’image d’une marque sur un média social) ou par un chercheur du
domaine (par exemple pour comparer les types d’évènements détectés par différentes
méthodes).

Ce chapitre est organisé de la manière suivante. Dans la section 5.2, nous synthéti-
sons l’état de l’art, d’une part en matière de méthodes pour la détection d’évènements
et pour l’analyse de l’inﬂuence dans les médias sociaux, et d’autre part en matière
de solutions logicielles. Dans la section 5.3 nous présentons le logiciel SONDY et les
fonctionnalités qu’il offre. Dans la section 5.4 nous illustrons le potentiel du logiciel
à travers plusieurs scénarios d’utilisation. Enﬁn, nous concluons ce chapitre avec la
discussion présentée à la section 5.5.

5.2 État de l’art

Dans cette section, nous présentons l’état de l’art à propos, d’une part, des mé-
thodes pour la détection d’évènements et l’identiﬁcation d’utilisateurs inﬂuents dans
les médias sociaux, et d’autre part à propos des solutions logicielles pour ces tâches.

5.2.1 Détection d’évènements et analyse de l’inﬂuence dans les

médias sociaux

Détection d’évènements. Un état de l’art détaillé à propos des méthodes pour
la détection d’évènements dans les médias sociaux est présenté dans la section 3.2
du chapitre 3 de ce manuscrit de thèse. Pour résumer, nous pouvons dire que les
méthodes existantes mettent en œuvre différentes approches se concentrant sur le
contenu textuel des messages pour détecter les évènements : pondération statistique
des termes (Shamma et al., 2011; Benhardus et Kalita, 2013), modélisations proba-
bilistes des thématiques latentes (Lau et al., 2012; Yuheng et al., 2012), ou encore
clustering (Weng et Lee, 2011; Li et al., 2012; Parikh et Karlapalem, 2013). Comme
nous l’avons montré dans le chapitre 3, il est aussi possible d’exploiter le contenu
hyper-textuel des messages pour améliorer la détection automatique des évènements
(Guille et Favre, 2014a).

En complément de cet état de l’art, il est également intéressant de citer les tra-
vaux de Rong et Qing (2012) sur l’analyse visuelle des évènements. Ils proposent de

139

5.2. État de l’art

FIGURE 5.1 – Comportement typique de l’indicateur MACD.

visualiser leur dynamique à l’aide de courbes MACD (Moving Average Convergence
Divergence) calculées à partir de la fréquence des termes liés aux évènements. Ils dé-
veloppent à cette ﬁn une version de l’indicateur MACD adaptée à cette tâche. Cet
indicateur, originellement développé pour analyser la dynamique des cours de bourse
par Appel (2005), met en valeur les tendances d’une série temporelle en mesurant
la différence entre une moyenne exponentielle mobile à long terme et une moyenne
exponentielle mobile à plus court terme. Ainsi, les changements de signe de l’indica-
teur traduisent des inversions de tendance. Constatant que la fréquence des termes
peut être volatile, les auteurs proposent d’employer des moyennes mobiles simples
et non pas exponentielles, lesquelles sont susceptibles d’ampliﬁer cette volatilité du
fait du poids important qu’elles donnent aux tranches temporelles récentes. La ﬁgure
5.1 illustre l’oscillation de l’indicateur MACD en fonction de deux moyennes mobiles.
L’oscillation est facilement interprétable visuellement, suivant deux règles : (i) quand
la valeur de l’indicateur MACD devient positive, l’intérêt des utilisateurs pour l’évène-
ment grandit et (ii) quand la valeur de l’indicateur MACD devient négative, l’intérêt
pour l’évènement s’atténue.

Analyse de l’inﬂuence. Il existe de nombreuses méthodes pour l’analyse de l’in-
ﬂuence, i.e. autorité, des utilisateurs des médias sociaux. Celle-ci exploite la struc-
ture du réseau social interconnectant les utilisateurs, partant du postulat qu’un lien

140

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

(ux → u y) est assimilable à un vote de l’utilisateur ux en faveur de l’autorité de

l’utilisateur u y.

Page et al. (1998) montrent que le degré entrant – dans le cas d’un réseau so-
cial, le degré entrant d’un utilisateur caractérise le nombre d’utilisateurs exposés aux
messages qu’il publie – n’est pas une mesure sufﬁsante pour déterminer la distribu-
tion de l’inﬂuence au sein d’un réseau et propose une mesure nommée PageRank. En
plus du degré entrant, elle considère deux autres facteurs. D’abord, un lien (i.e. vote)
provenant d’un utilisateur inﬂuent est plus signiﬁcatif qu’un lien provenant d’un utili-
sateur peu inﬂuent. Ensuite, plus le degré sortant d’un utilisateur est grand, moins les
liens sortants correspondants sont signiﬁcatifs. La mesure d’autorité relative µux d’un
utilisateur ux combine ces trois facteurs et est formulée ainsi :

µux = (cid:88)

(u y→ux)

µu y
ds(u y)

Exprimer la mesure d’autorité pour tous les membres du réseau revient à écrire

un système d’équations linéaires, que l’on peut exprimer sous forme matricielle :

µT A = µT

où le vecteur noté µ représente les mesures d’autorité relatives pour tous les
membres, et A est une matrice carrée telle que Ax y vaut 1/dux, i.e. l’inverse du de-
gré sortant d’ux, s’il existe un lien (ux → u y) et 0 sinon. Or, par déﬁnition, µT A = µT
équivaut à (A− I)µT = 0 et le vecteur µ correspond alors à un vecteur propre de

la matrice A. Dans le cas des grands réseaux où calculer la solution exacte est trop
coûteux, la méthode de la puissance itérée est couramment employée (Wills, 2006)
pour estimer la valeur propre de plus grand module et le vecteur propre associé. Par
ailleurs, la matrice A étant stochastique puisque ses colonnes somment à 1, elle peut
être interprétée comme la matrice des transitions d’une marche aléatoire. Dans ce cas,
la mesure PageRank d’un utilisateur modélise la probabilité qu’une personne navigant
aléatoirement à travers la structure du réseau social visite le proﬁl de cet utilisateur.
La convergence de la méthode de la puissance itérée pouvant être lente dans cer-
tains cas, Kitsak et al. (2010) suggèrent d’utiliser la décomposition en k-enveloppes,
pour mesurer plus simplement l’autorité relative des membres d’un réseau social.

141

5.2. État de l’art

Cette méthode affecte à chaque membre du réseau une mesure d’autorité relative,
sur la base des deux facteurs suivants : (i) l’autorité d’un utilisateur dépend de son
degré entrant et (ii) un lien (i.e. vote) entrant est d’autant plus signiﬁcatif qu’il pro-
vient d’un utilisateur inﬂuent, i.e. ayant lui-même un degré entrant important. Une
k-enveloppe est déﬁnie par Seidman (1983) comme un sous-réseau connexe et maxi-
mal, dont les membres ont un degré entrant supérieur ou égal à k. Les membres du
réseau appartenant à l’enveloppe k peuvent être identiﬁés selon la simple méthode
suivante. D’abord, les membres du réseau dont le degré entrant est inférieur à k sont
retirés, ainsi que leurs liens entrants et sortants. Puis, les membres dont le degré en-
trant est inférieur à k suite à cette opération sont également retirés ainsi que leurs
liens, et ainsi de suite jusqu’à ce qu’il n’y ait plus de membres dont le degré entrant
est inférieur à k. L’ensemble d’utilisateurs restants, s’il n’est pas vide, correspond alors
aux utilisateurs dont le score relatif d’inﬂuence vaut k. Cette méthode est appliquée
itérativement à partir de k = 0 jusqu’à ce que le cœur du réseau ait été atteint, c’est
à dire l’enveloppe non-vide au plus grand k. La ﬁgure 5.2 montre la décomposition
d’un réseau ﬁctif en 3 itérations (Kitsak et al., 2010). Lors de la première itération,
les membres de couleur sombre sont retirés, puis les membres colorés en gris sont
retirés lors de l’itération suivante. Le cœur du réseau est atteint à la troisième ité-
ration et est composé des membres colorés en blanc. La décomposition complète en

k-enveloppes d’un réseau peut être calculée avec une complexité temporelle en (cid:79) (|E|)

selon l’algorithme décrit par Batagelj et Zaversnik (2011).

Brown et Feng (2011) constatent que la distribution du nombre d’utilisateurs en
fonction de l’enveloppe à laquelle ils appartiennent a généralement une allure en
longue traîne (de l’anglais « long-tail distribution »). Par exemple, ils observent à par-
tir de données collectées sur Twitter que la majorité des utilisateurs ont des valeurs
de k faibles, avec un pic à k = 4 et le reste des utilisateurs se répartissant dans
plusieurs milliers d’enveloppes non-vides, ce qui rend difﬁcile l’analyse des résultats.
Pour remédier à ce problème, ils proposent une version modiﬁée de la méthode, en
ce qu’elle adopte une échelle logarithme. C’est-à-dire qu’une enveloppe k rassemble

les membres du réseau ayant un degré entrant supérieur ou égal à 2k − 1 au lieu de

k.

La majorité des méthodes existantes, telles que celles que nous venons de décrire,
visent à caractériser l’inﬂuence, ou l’autorité – c’est-à-dire l’inﬂuence a priori positive

142

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

FIGURE 5.2 – Décomposition en trois enveloppes d’un réseau comportant 11 membres.

– des membres d’un média social sur la diffusion de l’information. Toutefois, Dugué
et al. (2014) observent que certains utilisateurs des médias sociaux cherchent à maxi-
miser leur inﬂuence de manière artiﬁcielle, selon une stratégie « capitalisme social ».
Ces utilisateurs – appelés capitalistes sociaux – se connectent à un grand nombre
d’utilisateurs pratiquant également cette stratégie, sans égard au contenu publié par
ces derniers et dans l’espoir qu’ils établissent un lien réciproque. Ils gagnent ainsi en
visibilité au sein des réseaux sociaux en augmentant leur degré, au détriment de la
qualité du contenu échangé. Cette pratique remet en cause le postulat à la base des
méthodes que nous avons décrites précédemment, à savoir que les liens entre utili-
sateurs sont assimilables à des votes pouvant être exploités comme des marqueurs
d’inﬂuence. Les capitalistes sociaux ayant des degrés importants et se connectant ma-
joritairement à d’autres capitalistes, leur inﬂuence au sens de la décomposition en
k-enveloppes ou en log-k-enveloppes est importante également. La méthode Page-
Rank semble néanmoins plus robuste face à ce genre de comportement, puisque la
signiﬁcativité des liens est pondérée par le degré sortant des utilisateurs à leur ori-
gine, ce qui modère l’impact des capitalistes sociaux. Dugué et Perez (2014) proposent

143

5.2. État de l’art

une méthode simple permettant d’identiﬁer les utilisateurs susceptibles de pratiquer
le capitalisme social. Elle repose sur la mesure du taux de recouvrement entre les voi-
sinages entrants et sortants des utilisateurs ainsi que le rapport entre la taille de leurs
voisinages entrants et sortants.

5.2.2 Logiciels pour la fouille et l’analyse de données issues des

médias sociaux

Dans cette sous-section, nous synthétisons l’état de l’art concernant les solutions
logicielles, que nous divisons en deux catégories, selon qu’elles soient développées
dans l’industrie ou dans le milieu académique.

Logiciels développés dans l’industrie. De nombreux logiciels pour l’analyse des
médias sociaux sont développés dans l’industrie, dont trois des plus populaires sont :
SAP Social Media Analytics 4, NetBase 5 et BrandWatch Analytics 6. Ces logiciels sont
orientés marketing et sont conçus pour permettre aux entreprises, à partir de don-
nées qu’elles ont ciblées, de détecter les évènements qui animent les discussions à
propos de leur(s) marque(s) et identiﬁer les utilisateurs inﬂuents à leur sujet. Ces lo-
giciels souffrent principalement de deux limitations. Premièrement, les trois logiciels
mentionnés sont payants et propriétaires, ce qui signiﬁe que le code source n’est pas
public. Cela ajouté à l’absence de communication à propos des algorithmes mis en
œuvre pour traiter les données leur donne un aspect « boîte noire » qui se révèle pro-
blématique lorsqu’il s’agit d’interpréter les résultats qu’ils produisent et d’évaluer leur
validité. Deuxièmement, les interfaces qu’ils proposent ne sont pas toujours adaptées
aux résultats à visualiser.

À titre d’exemple et comme on peut le voir sur la ﬁgure 5.3.a, le logiciel SAP
Social Media Analytics décrit les thématiques animant les discussions à l’aide d’un
nuage de mots, lesquels sont dessinés avec plusieurs couleurs et tailles de police. Cette
représentation faisant appel à beaucoup de dimensions ne permet pas d’identiﬁer
aisément les différentes thématiques. La ﬁgure 5.3.b montre quant à elle l’interface
dédiée à l’identiﬁcation d’utilisateurs inﬂuents proposée par le logiciel BrandWatch
Analytics, qui consiste en une distribution et ne permet pas de visualiser la structure

4. SAP Social Media Analytics : http://www.news-sap.com/power-social-media-analytics/
5. NetBase : http://www.netbase.com
6. BrandWatch Analytics : http://www.brandwatch.com/brandwatch-analytics/

144

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

(a) Interface dédiée à l’analyse des messages, montrant des résultats à propos de la

marque American Airlines.

(b) Interface dédiée à l’analyse de l’inﬂuence des utilisateurs, montrant des résultats

pour la marque Ben & Jerry’s.

FIGURE 5.3 – Interfaces des logiciels SAP Social Media Analytics (a) et BrandWatch
Analytics (b).

145

5.2. État de l’art

FIGURE 5.4 – Interface utilisateur du logiciel Gephi pour la fouille de graphe.

du réseau social. Or, la notion de réseau est essentielle à la notion d’inﬂuence, puisque
c’est précisément parce que les utilisateurs des médias sociaux font partie d’un réseau
qu’ils peuvent subir ou exercer de l’inﬂuence. Par conséquent, sa visualisation est un
élément essentiel lors de l’analyse de l’inﬂuence.

Logiciels développés dans le milieu académique. Il n’existe à notre connais-
sance aucun logiciel développé dans le milieu académique permettant d’étudier à la
fois les évènements et l’inﬂuence à partir de données collectées sur les médias sociaux.
Il existe néanmoins plusieurs prototypes non-libres pour la détection d’évènements –
qui implémentent chacun leur propre algorithme et ne sont pas conçus pour en in-
tégrer d’autres – tels que Eddi (Bernstein et al., 2010), TwitInfo (Marcus et al., 2011)
ou bien encore KeySEE (Lee et al., 2013). Ces prototypes sont spéciﬁquement conçus
pour Twitter et collectent directement les messages qu’ils analysent. Par ailleurs, les
algorithmes que mettent en œuvre ces prototypes pour analyser les données sont peu
ou pas décrits. Pour l’analyse de l’inﬂuence, il existe plusieurs logiciels libres émanant
du milieu académique tels que Gephi écrit en Java (Bastian et al., 2009), dont l’inter-
face utilisateur est illustrée par la ﬁgure 5.4, ou encore Tulip (Auber, 2004) et SNAP 7

7. Page ofﬁcielle du logiciel SNAP : http://snap.stanford.edu

146

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

TABLE 5.1 – Matrice synthétisant les fonctionnalités des logiciels développés dans le
milieu académique pour les tâches de détection d’évènements et l’analyse de l’in-
ﬂuence.

Détection d’évènements

Analyse de l’inﬂuence

Algorithmes

Visualisations

Algorithmes

Visualisations

Eddi

ad hoc

TwitInfo

ad hoc

KeySEE

ad hoc

frise chronologique,

nuage de mots
frise chronologie,

courbe de fréquence
frise chronologique,
courbe de fréquence,

nuage de mots

–

–

–

–

–

–

Gephi

Tulip

SNAP

–

–

–

–

–

–

Page Rank,

HITS,

réseau coloré,

réseau interactif,

intermédiarité

distribution de l’inﬂuence

Page Rank,

k-cores,

réseau coloré,

réseau interactif,

intermédiarité

distribution de l’inﬂuence

Page Rank,

HITS,
k-cores,

intermédiarité

export vers

Microsoft Excel

écrits en C++. La table 5.1 liste les fonctionnalités des logiciels que nous venons de
citer.

5.2.3 Synthèse de l’état de l’art

Dans cette section, nous synthétisons brièvement l’état de l’art.
Logiciels développés dans l’industrie. Tout d’abord, nous constatons que les
logiciels développés dans le milieu industriel sont très spécialisés et ne permettent
pas d’analyser conjointement évènements et inﬂuence dans les médias sociaux. Par
ailleurs, les visualisations qu’ils proposent ne sont pas toujours nécessairement adap-
tées, ce qui ne facilite pas l’analyse de leurs résultats, d’autant plus que nous ignorons
les algorithmes mis en œuvre pour parvenir à ces résultats.

Logiciels développés dans le milieu académique. La table 5.1 liste les fonction-
nalités des logiciels développés dans le milieu universitaire pour la détection d’évène-
ments et l’analyse de l’inﬂuence dans les médias sociaux. D’une part, nous constatons

147

5.3. Logiciel proposé

que les auteurs de méthodes pour la détection d’évènements partagent rarement leurs
implémentations. Par exemple, parmi les 10 méthodes couvertes par l’état de l’art
(section 5.2.1), seule l’implémentation d’On-line LDA et de MABED (la méthode que
nous proposons) sont partagées par les auteurs 8. Qui plus est, les logiciels dédiés à la
détection d’évènements se concentrent principalement sur l’aspect visualisation et im-
plémentent des algorithmes peu ou pas décrits. De plus, ils collectent directement les
données qu’ils analysent et ne permettent pas l’import de jeux de données statiques.
Par conséquent, il semble important de favoriser le partage des implémentations des
méthodes de détection d’évènements. D’autre part, il existe une riche offre de logiciels
pour l’analyse de l’inﬂuence, qui implémentent des méthodes décrites dans la littéra-
ture et permettent d’importer manuellement les données à explorer. Or, l’inﬂuence
n’étant pas universelle, il semble important de la mesurer dans un contexte précis,
e.g. par rapport à un évènement discuté par les utilisateurs.

Face à ces constats, nous présentons dans la section suivante le logiciel libre et
extensible que nous proposons pour analyser conjointement évènements et inﬂuence
dans les médias sociaux.

5.3 Logiciel proposé

Nous proposons le logiciel SONDY, qui inclut des outils de visualisation et im-
plémente des algorithmes de l’état de l’art pour la fouille et l’analyse des données
issues des médias sociaux, et est doté d’une interface graphique facilitant l’accès à
ses fonctionnalités. Ce logiciel est disponible librement et gratuitement sous la li-
cence GNU GPLv3 9. Il est téléchargeable à l’adresse suivante : http://mediamining.
univ-lyon2.fr/sondy, et son code source est accessible par SVN à l’adresse qui suit :
http://mediamining.univ-lyon2.fr/websvn. SONDY est développé en Java, ce qui per-
met de l’exécuter sur la majorité des systèmes d’exploitation, et ce avec de bonnes
performances (Taboada et al., 2013). La popularité de ce langage favorise par ailleurs
sa réutilisation et son interopérabilité, puisque plusieurs groupes de recherche ont
aussi fait le choix de ce langage, comme le « Stanford Natural Language Processing

8. Nous avons contacté les auteurs des autres méthodes par e-mail aﬁn de leur demander s’ils
pouvaient partager leur implémentation mais avons reçu des réponses négatives voire aucune réponse.
9. Les termes de la licence sont consultables à l’adresse : https://www.gnu.org/licenses/quick-

guide-gplv3.fr.html.

148

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

Group 10 » ou le « Machine Learning Group at the University of Waikato 11 » entre
autres.

5.3.1 But du logiciel, publics visés et architecture générale

But du logiciel. Le logiciel SONDY a pour but de permettre la détection d’évène-
ments et l’identiﬁcation d’utilisateurs inﬂuents à partir de données issues d’un média
social. Pour cela, il offre quatre services, dotés d’interfaces graphiques adaptées :

— Le service de manipulation (i.e. import et préparation) des données (cf. ﬁgure

5.5.a) ;

— Le service de détection et de visualisation des évènements (cf. ﬁgure 5.5.b) ;
— Le service d’analyse et de visualisation du réseau social (cf. ﬁgure 5.5.c) ;
— Le service d’import de nouveaux algorithmes (cf. ﬁgure 5.5.d) ;
La ﬁgure 5.6 montre où ces services se positionnent par rapport au processus
typique de fouille de données. Le service de manipulation des données se place na-
turellement au niveau des premières phases du processus, tandis que les trois autres
services interviennent lors des dernières phases du processus qui conduisent à l’ex-
traction de connaissances.

Publics visés. Le logiciel est destiné à être utilisé par des non-experts – e.g. jour-
nalistes, analystes médias, enquêteurs – grâce à son interface graphique claire. Il est
également destiné à être utilisé par les chercheurs du domaine, puisque le logiciel
est conçu pour permettre l’ajout de nouveaux algorithmes de façon simple, et peut
également être utilisé comme bibliothèque au sein d’un autre programme Java.

Architecture. L’application traite deux types de données à la fois : les données dé-
crivant un ensemble de messages publiés par les utilisateurs d’un média social, et les
données décrivant la structure du réseau social interconnectant ces utilisateurs. La ﬁ-
gure 5.7 décrit l’architecture logicielle de SONDY, i.e. les entrées/sorties des différents
services et la manière dont ils communiquent entre eux. On observe notamment que
le service de manipulation des données fait le pont entre données brutes et données
manipulées par les services de détection d’évènements et d’analyse du réseau social.

10. Liste des logiciels développés par le Stanford NLP Group : http://nlp.stanford.edu/software/
index.shtml
11. Liste des logiciels développés par le Waikato Machine Learning Group : http://www.cs.waikato.
ac.nz/ml/weka/index.html

149

5.3. Logiciel proposé

FIGURE 5.5 – Interfaces du logiciel SONDY : manipulation des données (a), détection
et visualisation des évènements (b), analyse et visualisation du réseau social (c) et
import de nouveaux algorithmes (d).

150

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

FIGURE 5.6 – Positionnement des services du logiciel SONDY dans le processus typique
de fouille de données.

FIGURE 5.7 – Architecture du logiciel SONDY.

151

5.3. Logiciel proposé

Dans les prochaines sections, nous décrivons en détail le rôle et le fonctionnement

de chaque service.

5.3.2 Service de manipulation des données

Ce service gère une collection de jeux de données (cf. ﬁgure 5.8.a). Un jeu de
données correspond à un ensemble de messages publiés sur un média social ainsi que
le réseau interconnectant les auteurs.

Import de données. L’import d’un nouveau jeu de données est réalisé à partir de
deux ﬁchiers CSV (cf. ﬁgure 5.8.b et b’). L’un comporte trois colonnes – auteur, date,
texte – qui permettent de représenter les messages. L’autre comporte deux colonnes
qui permettent de modéliser à l’aide d’un graphe orienté le réseau interconnectant
les auteurs des messages. Lors de l’import, une copie de référence du jeu de données
est sauvegardée dans une base de données indexée et gérée par SONDY. La version
actuelle du logiciel utilise un serveur de base de données libre, MySQL 12.

Pré-traitement des données. Une fois un jeu de données importé, l’ensemble de
messages correspondant peut être préparé aﬁn d’optimiser son analyse par les algo-
rithmes de détections d’évènements. Les pré-traitements implémentés dans SONDY
sont les suivants (cf. ﬁgure 5.8.c) :

— Partitionnement : discrétise l’axe temporel en partitionnant les messages dans

des tranches temporelles d’une durée égale à celle entrée en paramètre.

— Tokenization : déﬁnit la manière dont le contenu de chaque message est dé-

coupé – en unigrammes, bigrammes ou trigrammes.

— Racinisation : supprime les préﬁxes et sufﬁxes des mots pour ne conserver que

leur racine – disponible pour l’anglais et le français.

— Lemmatisation : transforme les différentes ﬂexions des mots en leur lemme –

disponible pour l’anglais.

Lorsqu’un pré-traitement est appliqué à un jeu de données, l’ensemble de mes-
sages qui en résulte est sauvegardé et indexé à l’aide de la bibliothèque libre Lucene 13,
ceci dans le but de permettre l’analyse efﬁcace de grands corpus de messages. L’utilisa-
teur du logiciel peut par la suite choisir la « préparation » à utiliser à l’aide d’une liste
déroulante (ﬁgure 5.8.d). Les préparations sont nommées selon le modèle : pas de
12. Le serveur MySQL est téléchargeable à l’adresse : http://www.mysql.com.
13. La bibliothèque Lucene est téléchargeable à l’adresse : http://lucene.apache.org.

152

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

FIGURE 5.8 – La principale fenêtre correspond à l’interface du service de manipulation
des données. Les deux petites fenêtres (b’) montrent des extraits de ﬁchiers CSV (à
gauche, les messages, à droite le réseau social) pouvant être importés par SONDY.

153

5.3. Logiciel proposé

discrétisation (durée en minutes) – mode de racinisation (standard, anglais, français)
– lemmatisation (activée : lem1, sinon lem0) – tokenization (1gram, 2gram, 3gram).
Filtrage des données. Lorsqu’une préparation a été sélectionnée, il est possible de
ﬁltrer les données avant de les traiter à l’aide du service de détection d’évènements.
Les ﬁltrages disponibles sont les suivants (cf. ﬁgure 5.8.e) :

— Sélection d’une période de temps : limite la détection d’évènements à une partie

du ﬂux de message.

— Suppression des mots vides : retire du vocabulaire employé dans les messages
les mots d’une des listes de mots vides intégrées à SONDY (anglais, chinois,
français, et mot vides spéciﬁques à Twitter), ou une liste fournie par l’utilisa-
teur.

5.3.3 Service de détection d’évènements

Ce service permet de conﬁgurer et d’appliquer des algorithmes pour la détection
automatique d’évènements (cf. ﬁgure 5.9.a) à partir d’un jeu de données préparé et
éventuellement ﬁltré avec le service de manipulation des données, puis d’explorer les
résultats.

Algorithmes implémentés pour la détection d’évènements. Plusieurs méthodes

récentes tirées de la littérature sont implémentées, à savoir :

— Peaky Topics : une méthode de pondération statistique des termes pour détecter

les évènements très localisés dans le temps (Shamma et al., 2011).

— Persistent Conversations : une méthode de pondération statistique des termes
pour détecter les évènements suscitant l’intérêt des utilisateurs pendant une
période de temps prolongée (Shamma et al., 2011).

— Trending Score : une méthode de pondération statistique des termes plus spé-

ciﬁquement adaptée aux N-grammes (Benhardus et Kalita, 2013).

— EDCoW : une méthode de classiﬁcation non supervisée des termes fondée sur
la mesure de la similarité temporelle entre termes à l’aide de la théorie des
ondelettes (Weng et Lee, 2011).

— ET : une méthode de clustering des termes par classiﬁcation hiérarchique as-
cendante fondée sur la mesure de la similarité temporelle et sémantique entre
termes (Parikh et Karlapalem, 2013).

154

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

FIGURE 5.9 – La principale fenêtre correspond au cœur de l’interface du service de dé-
tection d’évènements. Les deux autres fenêtres correspondent respectivement, de haut
en bas, à la fenêtre pour l’exploration des messages (d’) et à la frise chronologique
des évènements détectés (f’).

155

5.3. Logiciel proposé

— MABED : une méthode basée sur la mesure de l’anomalie dans la fréquence de

création de mentions (Guille et Favre, 2014a).

— Pont vers On-line LDA : exporte le jeu de données préparé et ﬁltré dans un
format compatible avec l’implémentation en python fournie par les auteurs
(Lau et al., 2012).

Pour permettre une exploration efﬁcace des évènements détectés par ces algo-
rithmes, SONDY offre plusieurs visualisations. Ces visualisations, qui couvrent les trois
dimensions des évènements – à savoir l’impact, la thématique et le temps – sont dé-
crites ci-après.

Liste des évènements détectés. Lorsqu’un algorithme est appliqué, les évène-
ments détectés sont listés par magnitude d’impact – au sens de l’algorithme utilisé –
décroissante dans une table (cf. ﬁgure 5.9.b). Chaque évènement est décrit dans cette
table par une thématique (un ou plusieurs termes selon l’algorithme utilisé) et un
intervalle temporel. Le contenu de la table peut être ﬁltré selon les thématiques, en
entrant une expression régulière dans le champ de recherche au-dessus de celle-ci (cf.
ﬁgure 5.9.b).

Courbe de fréquence. Lorsqu’un évènement est sélectionné, sa fréquence d’appa-
rition dans les messages est donnée par le graphique à gauche de la table (cf. ﬁgure
5.9.c) et l’intervalle temporel identiﬁé par l’algorithme est automatiquement grisé.
Pour faciliter la lecture de la courbe de fréquence, SONDY propose de calculer l’in-
dicateur MACD (Rong et Qing, 2012), qui fait ressortir les irrégularités de la courbe
de fréquence. Cet indicateur peut également aider l’utilisateur à rafﬁner l’intervalle
temporel associé aux évènements détectés par certains algorithmes dont la précision
temporelle est faible. Pour ce faire, la zone grisée peut être librement modiﬁée, dé-
placée, etc.

Exploration des messages liés aux évènements. Un clic droit dans la zone grisée
(cf. ﬁgure 5.9.d) permet d’extraire les messages liés à la thématique de l’évènement
et publiés durant l’intervalle temporel sélectionné. La fenêtre pour l’exploration des
messages (5.9.d’) permet également d’extraire les termes les plus fréquents dans cet
ensemble de messages, ce qui est particulièrement utile lorsque l’algorithme utilisé
identiﬁe des descriptions d’évènements sémantiquement faibles.

Frise chronologique des évènements. Enﬁn, il est possible de générer (cf. ﬁgure
5.9.f’) une frise chronologique reprenant les évènements détectés. Le survol avec la

156

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

souris des points sur l’axe temporel fait apparaître la description de l’évènement cor-
respondant.

5.3.4 Service d’analyse du réseau social

Ce service permet de conﬁgurer et d’appliquer des algorithmes pour l’analyse du
réseau social des auteurs des messages liés à l’évènement sélectionné dans le service
de détection des évènements (i.e. un évènement sélectionné dans la table et l’inter-
valle temporel correspondant à la zone grisée sur le graphique donnant la courbe de
fréquence).

Algorithmes implémentés pour l’analyse du réseau social. Plusieurs méthodes
récentes tirées de la littérature sont implémentées (cf. ﬁgure 5.10.a). Leur but est
d’associer un rang – dont la signiﬁcation dépend de la méthode utilisée – à chaque
membre du réseau social analysé :

— Page Rank : estime le rang de chaque membre du réseau en fonction de son
inﬂuence, mesurée comme la probabilité qu’un surfeur aléatoire visite le nœud
correspondant (Page et al., 1998).

— K-shell decomposition : partitionne de manière récursive les membres du réseau
en enveloppes, l’enveloppe à laquelle chaque membre appartient correspon-
dant à son rang (Batagelj et Zaversnik, 2011).

— Log K-shell decomposition : partitionne les membres de façon similaire à la dé-
composition en k-enveloppes, mais avec une échelle logarithmique pour fa-
voriser la lecture des rangs dans les grands réseaux sociaux (Brown et Feng,
2011).

— Centralité d’intermédiarité : mesure pour chaque nœud le nombre de chemins
les plus courts passant par celui-ci, depuis tous les nœuds, vers tous les autres
(Freeman, 1977).

— Social capitalist identiﬁcation : estime le rang de chaque membre du réseau
en fonction de sa propension au capitalisme social, mesuré comme le taux
de recouvrement entre les ensembles d’arcs entrants et sortants pour chaque
membre du réseau (Dugué et Perez, 2014).

Distribution des rangs des utilisateurs. Lorsqu’un algorithme est appliqué, la
distribution du nombre de membres du réseau en fonction des rangs déterminés par

157

5.3. Logiciel proposé

FIGURE 5.10 – L’interface principale du service d’analyse du réseau permet de naviguer
dans le réseau social coloré et de consulter la distribution des rangs identiﬁés par les
algorithmes. La seconde fenêtre (d’) permet de naviguer parmi les messages publiés
par les utilisateurs membres du réseau.

158

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

FIGURE 5.11 – Quatre des étapes d’une séquence d’activation capturées à partir de
l’interface du service d’analyse du réseau social.

cet algorithme est donnée dans la partie droite de l’interface principale (cf. ﬁgure
5.10.b). Au-dessus de cette distribution est présenté un gradient allant du bleu au
rouge en passant par le jaune, qui correspond à la répartition des couleurs associées
aux nœuds du réseau en fonction de leur rang.

Navigation dans le réseau social coloré. Le réseau social analysé est dessiné
et coloré selon le gradient et les rangs déterminés par l’algorithme utilisé (cf. ﬁ-
gure 5.10.c). Les nœuds du réseau sont positionnés selon l’algorithme de type force-
directed proposé par Fruchterman et Reingold (1991) et le rendu du graphe est ob-
tenu grâce à la bibliothèque GraphStream développée par l’université du Havre (Dutot
et al., 2007). La visualisation est interactive et permet à la fois de déplacer le graphe
dans le plan de dessin et d’ajuster la distance entre la caméra et le plan.

Exploration des messages publiés par les utilisateurs. Un clic droit sur un
nœud du réseau social révèle l’identiﬁant de l’utilisateur qu’il représente et permet
d’explorer tous les messages qu’il a publiés (cf. ﬁgure 5.10.d’). Cette interface permet
également d’extraire les termes les plus fréquents dans les messages de l’utilisateur
sélectionné, ce qui peut aider par exemple à déterminer ses centres d’intérêt.

Visualisation de la séquence d’activation. Parmi la liste des méthodes dispo-
nibles (cf. ﬁgure 5.10.a) se trouve une méthode permettant de rejouer la séquence
d’activation liée à un évènement, en afﬁchant les nœuds un à un. La ﬁgure 5.11
montre quelques étapes de la séquence d’activation engendrée par la diffusion d’une

159

5.3. Logiciel proposé

information à travers le réseau des utilisateurs ayant réagi à son sujet. Une fois tous
les nœuds activés, le réseau est coloré en fonction du rang de chaque utilisateur dans
la séquence d’activation.

5.3.5 Service d’import d’algorithmes et API

SONDY fournit une API qui permet le développement de nouveaux algorithmes
compatibles, qui peuvent être importés dynamiquement à l’aide du service d’import
d’algorithmes – grâce à un système de plug-ins – sous la forme d’une classe Java ou
d’un JAR si plusieurs ressources sont nécessaires à l’exécution de l’algorithme importé.
L’API fournit un ensemble de méthodes pour la manipulation des données ainsi
que des interfaces pour concevoir les classes implémentant les algorithmes de détec-
tion d’évènements ou d’analyse du réseau social.

Fonctions pour la manipulation des données. L’API donne accès aux fonctions
de bases pour manipuler le ﬂux de messages : extraction du vocabulaire des termes,
fréquence des termes, cooccurrences entre termes, etc. Elle fournit également les fonc-
tions de base pour manipuler la structure du réseau social : liste des nœuds et des
liens, identiﬁcation du voisinage entrant ou sortant d’un nœud.

Interface pour les algorithmes de détection d’évènements. Les nouveaux al-
gorithmes doivent implémenter une interface de l’API. Cette interface permet la dé-
ﬁnition des paramètres de l’algorithme de sorte qu’ils s’intègrent automatiquement
à l’interface utilisateur du logiciel SONDY. Elle déﬁnit par ailleurs une structure gé-
nérique permettant de stocker les évènements détectés par tout algorithme, qui est
reprise automatiquement dans l’interface utilisateur pour différents usages (e.g. listes
des évènements, frise chronologique, courbe de fréquence).

Interface pour les algorithmes d’analyse du réseau social. Similairement, l’API
fournit une interface à implémenter pour développer de nouveaux algorithmes d’ana-
lyse du réseau social, qui gère l’intégration des paramètres et des résultats. Elle permet
aussi de maintenir la cohérence avec le service de détection d’évènements, en four-
nissant la structure du sous-graphe correspondant aux utilisateurs ayant publié des
messages en lien avec l’évènement sélectionné.

160

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

5.4 Exemples de scénarios d’utilisation

Dans cette section, nous présentons deux scénarios d’utilisation illustrant les ca-
pacités du logiciel SONDY. Le premier scénario décrit comment un non-expert peut
utiliser le logiciel pour analyser les données dont il dispose. Le deuxième scénario
montre comment un chercheur peut utiliser SONDY pour réutiliser et comparer les
méthodes développées dans le domaine.

5.4.1 Utilisation par un non-expert

On se propose ici d’étudier le média social Twitter du point de vue d’un non-
expert. Plus particulièrement, on se propose de s’intéresser à la société Google et à
ses produits.

Sélection et préparation des données. Pour ce scénario, nous utilisons un jeu
de données Twitter. Il représente l’intégralité des 1 437 126 de tweets publiés du
1 au 31 novembre 2009 par 52 494 utilisateurs nord-américains interconnectés par
5 793 961 liens d’abonnement (cf. ﬁgure 5.12.a). À l’aide du service de manipulation
des données, nous partitionnons le ﬂux de messages en tranches de 30 minutes et
nous découpons et indexons le contenu des messages en unigrammes. Avant de passer
à l’étape de détection des évènements, nous ﬁltrons le vocabulaire du ﬂux de messages
avec deux listes de mots vides inclus dans SONDY, à savoir la liste des mots communs
dans la langue anglaise et la liste des mots vides propres à Twitter (e.g. RT, via).

Détection et analyse des évènements. Nous utilisons d’abord la méthode MABED
pour détecter les 80 évènements ayant eu le plus d’impact auprès de ces utilisateurs
en novembre 2009, puis nous ﬁltrons la liste obtenue par le mot clé « Google » (cf.
ﬁgure 5.12.b). Il reste 8 évènements dont on peut visualiser la répartition temporelle
en générant la frise chronologique (cf. ﬁgure 5.12.c). L’évènement qui a le plus fait
réagir les utilisateurs est la publication par Google du code source du projet Chrome
OS, qui a retenu leur attention les 19 et 20 novembre. On observe que les évènements
# 6 – la sortie d’un langage de programmation, Go, conçu par Google – et # 8 –
l’annonce de l’accès wiﬁ offert dans les aéroports américains par Google – ont suscité
l’intérêt des utilisateurs du 10 au 11 novembre (cf. ﬁgure 5.13.a et b). La lecture des
messages liés à ces évènements permet d’en approfondir la compréhension (cf. ﬁgure

161

5.4. Exemples de scénarios d’utilisation

(a) Sélection du jeu de données.

(b) Liste des évènements concernant Google détectés avec la méthode MABED et

fréquence de l’évènement # 1.

(c) Frise chronologique des évènements.

FIGURE 5.12 – Exploration des évènements en lien avec Google : (a) sélection des
données à analyser, (b) détection des évènements et (c) frise chronologique.

162

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

(a) Fréquence de l’évènement # 6.

(b) Fréquence de l’évènement # 8.

(c) Messages liés à l’évènement # 6

(d) Messages liés à l’évènement # 8

FIGURE 5.13 – Exploration des évènements en lien avec Google : (a,b) courbes de
fréquence des évènements et (c,d) messages liés.

5.13.c et d). Un journaliste spécialisé préparant un dossier sur ce nouveau langage
trouvera des ressources intéressantes en lisant les messages liés à l’évènement # 6 où
ﬁgurent entre autres un lien vers un podcast présentant le langage, un lien vers la
page ofﬁcielle du projet, et diverses réactions d’utilisateurs. La lecture des messages
liés à l’évènement # 8 peut par exemple permettre au département marketing de
Google de se faire une idée de la réaction suscitée auprès des utilisateurs de Twitter
à propos de cette annonce.

Analyse de l’inﬂuence. Ayant sélectionné le premier évènement de la liste (cf.
ﬁgure 5.12.b), nous décidons d’étudier l’inﬂuence au sein du réseau social formé par
les connexions entre les auteurs des messages liés. Nous appliquons la méthode de
décomposition en k-enveloppes (cf. ﬁgure 5.14.a). Il apparaît qu’un utilisateur oc-
cupe une place centrale dans ce réseau puisque la distribution indique que tous les
membres du réseau ont une valeur de k inférieure à 17 sauf celui-ci dont la valeur
k est 28. La visualisation du réseau permet d’identiﬁer aisément le sommet coloré
en rouge qui lui correspond. Cela nous permet de consulter l’ensemble des messages

163

5.4. Exemples de scénarios d’utilisation

(a) Mesure et visualisation de l’inﬂuence selon la méthode de décomposition en

k-enveloppes au sein du réseau des auteurs.

(b) Ensemble des messages de l’utilisateur sélectionné, i.e. le plus inﬂuent au sens de

la méthode appliquée.

FIGURE 5.14 – Identiﬁcation d’utilisateurs inﬂuents à propos de l’évènement le plus
marquant concernant Google.

164

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

FIGURE 5.15 – Fenêtre de log retraçant les opérations effectuées.

qu’il a publiés (cf. ﬁgure 5.14.b). L’extraction des termes les plus fréquents dans ses
messages révèle que cet utilisateur s’intéresse aux produits Google. Cet utilisateur
semble donc jouer un rôle important dans le processus de diffusion de l’information
liée à Google et pourrait donc être interrogé prioritairement par un journaliste s’in-
téressant à cet évènement. Par ailleurs, le département marketing de Google pourrait
par exemple établir un contact avec cet utilisateur aﬁn qu’il bénéﬁcie d’informations
exclusives qu’il pourrait ensuite relayer à moindre coût sur Twitter.

5.4.2 Utilisation par un chercheur du domaine

Dans ce scénario, nous nous intéressons à la comparaison des résultats obtenus en

variant les méthodes employées et les préparations des données.

Comparaison des méthodes de détection d’évènements. La fenêtre de log (cf.
ﬁgure 5.15) permet de suivre le nombre d’évènements détectés par chacune des mé-
thodes appliquées ainsi que leur temps de calcul.

La ﬁgure 5.16 (a,b,c,d) montre la liste des évènements détectés par les méthodes
Peaky Topics, Trending Score, Persistent Conversations et EDCoW à partir d’un même jeu
de données. Les différentes listes révèlent que ces méthodes décrivent les évènements
de diverses manières tant du point de vue sémantique (unigramme, N-gramme, en-
semble d’unigrammes) que du point de vue temporel (une tranche temporelle, une
séquence de tranches temporelles). Par ailleurs, les courbes de fréquences des évène-
ments sélectionnés révèlent que ces méthodes détectent des motifs différents (plus ou
moins saillants). Nous pouvons voir en quoi les différentes préparations impactent les
résultats du point de vue de la redondance, de la précision sémantique ou temporelle.
La ﬁgure 5.17 montre les évènements détectés par une même méthode pour diffé-
rentes préparations – du point de vue du vocabulaire et de la discrétisation temporelle

165

5.4. Exemples de scénarios d’utilisation

(a) Peaky Topics.

(b) Trending Score.

(c) Persistent Conversations.

(d) EDCoW.

FIGURE 5.16 – Détection d’évènements avec différents algorithmes : Peaky Topics, Tren-
ding Score, Persistent Conversations et EDCoW.

166

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

– d’un même jeu de données. Cela permet d’évaluer la sensibilité de l’algorithme par
rapport à la préparation des données, en analysant la variation des résultats, tant en
terme de redondance que de précision sémantique ou temporelle.

Comparaison des méthodes d’analyse de l’inﬂuence. La ﬁgure 5.18 montre les
résultats obtenus avec les méthodes Page Rank, k-shell decomposition et log-k-shell
decomposition pour un même réseau social. Lorsque plusieurs algorithmes sont appli-
qués successivement au même réseau, SONDY mémorise la position des nœuds, ce
qui facilite la comparaison entre les résultats produits par les différentes méthodes.

Utilisation comme bibliothèque. Le programme 1 ci-après (page 170) montre
comment il est possible d’utiliser SONDY comme bibliothèque pour automatiser une
série d’expérimentations. Dans cet exemple, un jeu de données est chargé, préparé
puis une méthode de détection d’évènements est appliquée avec différents paramé-
trages et les résultats obtenus sont écrits sur le disque. Pour réaliser un benchmark
plus complet, il serait par exemple possible d’ajouter une boucle à ce programme
aﬁn de charger différents jeux de données et appliquer différents pré-traitements, et
également charger d’autres algorithmes.

167

5.4. Exemples de scénarios d’utilisation

(a) Unigrammes, 30 minutes.

(b) Bigrammes, 30 minutes.

(c) Unigrammes, 300 minutes.

(d) Bigrammes, 600 minutes.

FIGURE 5.17 – Résultats obtenus par la méthode Trending Score pour différentes prépa-
rations d’un même jeu de données. L’intervalle temporel est déﬁni en jours, le début
de la période couverte par le jeu de données étant associé au jour 0.

168

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

(a) Page Rank.

(b) k-shell decomposition

(c) log k-shell decomposition

FIGURE 5.18 – Analyse de l’inﬂuence au sein d’un réseau social à l’aide de différentes
méthodes.

169

5.5. Discussion

Programme 1 : Utilisation de SONDY comme bibliothèque dans un programme Java.
import fr.ericlab.sondy.*;
import org.apache.commons.io.FileUtils;

public class Programme {

public static void main(String[] args) {

AppVariables state;
DataManipulation dataManipulation;
// import d’un jeu de donnees
dataManipulation.importDataset("messages.csv","network.csv",

"Nom","Description optionnelle",state);

// preparation du jeu de donnees
dataManipulation.prepareStream(60,"English",false,state);
// chargement de la methode MABED
EventDetectionAlgorithm mabed = (EventDetectionAlgorithm)

Class.forName("MABED").newInstance(state);

for(double i = 0.2; i <= 1; i += 0.1){

// variation du parametre sigma de la methode
mabed.sigma = i;
mabed.k = 40;
mabed.theta = 0.7
mabed.p = 10;
mabed.apply();
EventDetectionResults results = mabed.getResults();
// ecriture des resultats
FileUtils.write("chemin",results);

}

}

}

5.5 Discussion

Dans ce chapitre nous avons décrit SONDY, un logiciel libre et extensible pour la
détection d’évènements et l’analyse de l’inﬂuence à partir de données générées par les
médias sociaux. Il vient combler un manque dans l’offre de logiciels développés dans
le milieu académique, qui se concentrent sur l’une ou l’autre de ces tâches. Le logiciel
SONDY permet, à l’aide des méthodes tirées de la littérature qu’il implémente et des
outils de visualisations qu’il offre, la détection et l’exploration d’évènements à partir
des messages publiés par un ensemble d’utilisateurs, puis, pour chaque évènement,

170

Un logiciel libre pour la détection d’évènements et l’analyse de l’inﬂuence dans les
médias sociaux

l’analyse de l’inﬂuence à partir de la structure du réseau social correspondant au sous-
ensemble d’utilisateurs ayant réagi. Nous avons montré comment, à l’aide de plusieurs
scénarios et de données réelles, un non-expert ou un chercheur du domaine peut uti-
liser SONDY. De plus, les capacités d’extensibilité du logiciel ont été démontrées par
l’implémentation d’un algorithme de détection d’évènements (EDCoW) avec l’API de
SONDY par des étudiants de master de l’université Lumière Lyon 2, ainsi qu’un algo-
rithme d’analyse de l’inﬂuence (Social capitalists identiﬁcation) par des chercheurs de
l’université d’Orléans. Une des principales perspectives de travail consiste à amélio-
rer la documentation (tant pratique que technique) du logiciel, aﬁn de faciliter son
utilisation.

Impact. Ces travaux ont notamment fait l’objet d’un article dans la session dé-
monstration et d’une présentation interactive à la conférence internationale ACM SIG-
MOD en 2013. Le logiciel SONDY a été téléchargé plus de 700 fois entre mars 2013
et octobre 2014.

171

Chapitre 6

Conclusion

Pour conclure ce manuscrit de thèse, nous résumons tout d’abord les travaux que
nous avons présentés, puis nous terminons en synthétisant les principales perspectives
de recherche ouvertes par ces travaux.

6.1 Résumé de la thèse

Dans cette thèse, nous nous sommes intéressés à la diffusion de l’information dans
les médias sociaux, et avons apporté des solutions à certaines des problématiques
majeures liées à ce phénomène.

Premièrement, nous avons proposé MABED (Mention-Anomaly-Based Event De-
tection), une méthode statistique pour détecter automatiquement les évènements im-
portants qui suscitent l’intérêt des utilisateurs des médias sociaux à partir du ﬂux de
messages qu’ils publient, dont l’originalité est d’exploiter la fréquence des interactions
sociales entre utilisateurs, en plus du contenu textuel des messages. La méthode MA-
BED diffère par ailleurs des méthodes existantes en ce qu’elle estime dynamiquement
la durée de chaque évènement, plutôt que de supposer une durée commune et ﬁxée
à l’avance pour tous les évènements. Deuxièmement, nous avons proposé T-BASIC
(Time-Based ASynchronous Independent Cascades), un modèle probabiliste basé sur
la structure de réseau sous-jacente aux médias sociaux pour prévoir la diffusion de
l’information, plus précisément l’évolution du volume d’utilisateurs relayant une in-
formation donnée au ﬁl du temps. Contrairement aux modèles similaires également
basés sur la structure du réseau, la probabilité qu’une information donnée se diffuse
entre deux utilisateurs n’est pas constante mais dépendante du temps. Nous avons
décrit une procédure pour l’inférence des paramètres latents du modèle (probabilité

173

6.1. Résumé de la thèse

de diffusion et délai de transmission pour chaque lien du réseau), dont l’originalité est
de formuler les paramètres comme des fonctions de caractéristiques observables des
utilisateurs. Troisièmement, nous avons proposé SONDY (SOcial Network DYnamics),
un logiciel libre implémentant des méthodes tirées de la littérature pour la fouille et
l’analyse des données issues des médias sociaux. Le logiciel manipule deux types de
données : les messages publiés par les utilisateurs, et la structure du réseau social
interconnectant ces derniers. Contrairement aux logiciels académiques existants qui
se concentrent soit sur l’analyse des messages, soit sur l’analyse du réseau, SONDY
permet d’analyser ces deux types de données conjointement en permettant l’analyse
de l’inﬂuence par rapport aux évènements détectés. Utilisé comme logiciel autonome,
SONDY offre une interface utilisateur avancée et des visualisations adaptées. Utilisé
comme blibliothèque, il permet d’intégrer facilement les méthodes implémentées dans
d’autres programmes.

Les expérimentations que nous avons menées à l’aide de divers jeux de données
collectés sur le média social Twitter (plusieurs millions de messages publiés par des
utilisateurs interconnectés par plusieurs millions de liens) ont démontré la pertinence
de nos propositions et ont mis en lumière des propriétés qui nous aident à mieux
comprendre les mécanismes régissant la diffusion de l’information. Premièrement, en
comparant les performances de MABED avec celles de méthodes récentes tirées de la
littérature, nous avons montré que la prise en compte des interactions sociales entre
utilisateurs conduit à une détection plus précise des évènements importants, avec une
robustesse accrue en présence de contenu bruité. Nous avons également montré que
MABED facilite l’interprétation des évènements détectés en fournissant des descrip-
tions claires et précises, tant sur le plan sémantique que temporel. Deuxièmement,
nous avons montré la validité de la procédure proposée pour estimer les probabilités
de diffusion sur lesquelles repose le modèle T-BASIC, en illustrant le pouvoir prédictif
des caractéristiques des utilisateurs que nous avons sélectionnées et en comparant
les performances de la méthode d’estimation proposée avec celles de méthodes tirées
de la littérature. Nous avons également montré l’intérêt d’avoir des probabilités non
constantes, ce qui permet de prendre en compte dans T-BASIC la ﬂuctuation du niveau
de réceptivité des utilisateurs des médias sociaux au ﬁl du temps. Enﬁn, nous avons
montré comment, et dans quelle mesure, les caractéristiques sociales, thématiques
et temporelles des utilisateurs affectent la diffusion de l’information. Troisièmement,

174

Conclusion

nous avons illustré à l’aide de divers scénarios d’utilisation l’utilité du logiciel SONDY,
autant pour des non-experts que pour des chercheurs du domaine. Nous avons par
exemple montré comment une société peut utiliser SONDY pour détecter les évène-
ments la concernant et qui font réagir les utilisateurs des médias sociaux, et ensuite
identiﬁer des personnes inﬂuentes, lesquelles pourraient potentiellement participer à
des campagnes marketing virales. Nous avons aussi montré comment un chercheur
du domaine peut utiliser SONDY comme logiciel autonome pour expérimenter des
méthodes de la littérature, mais aussi comment il peut concevoir un programme uti-
lisant l’interface de programmation de SONDY, aﬁn par exemple d’automatiser une
série d’expérimentations.

6.2 Perspectives de travail

Les travaux de recherche entamés durant cette thèse et présentés dans ce manus-

crit ouvrent plusieurs perspectives de recherche intéressantes.

En conclusion du chapitre 3 nous avons présenté une perspective de travail, ap-
puyée par des premières expérimentations prometteuses menées à l’aide de MABED
et de la méthode de Louvain (Blondel et al., 2008), reposant sur l’intuition selon la-
quelle les communautés d’utilisateurs au sens social – c’est-à-dire les communautés
identiﬁables à partir de la structure du réseau social que forment les utilisateurs d’un
média social – sont similaires aux communautés d’utilisateurs au sens thématique –
c’est-à-dire les communautés identiﬁables à partir des évènements à propos desquels
les utilisateurs réagissent. En particulier, il semblerait intéressant de pouvoir exploiter
la détection d’évènements pour évaluer la pertinence des communautés identiﬁées à
partir de la structure du réseau social, voire même pour améliorer la pertinence des
communautés identiﬁées.

Concernant la modélisation et la prédiction de la diffusion de l’information, nous
avons mentionné en conclusion du chapitre 4 plusieurs pistes pour améliorer nos tra-
vaux. L’une d’elles consisterait à supposer que le réseau servant de support à la propa-
gation de l’information puisse évoluer, plutôt que le considérer statique comme c’est
le cas avec T-BASIC. Aussi, plutôt que de modéliser la diffusion de chaque thématique
indépendamment des autres, il serait intéressant de modéliser ces processus simulta-
nément, aﬁn de pouvoir prendre en compte les interactions entre thématiques. Enﬁn,

175

6.2. Perspectives de travail

puisque dans certains cas l’inﬂuence externe à un média social peut jouer un rôle
prépondérant dans le processus de diffusion, il pourrait être intéressant de relâcher
l’hypothèse de monde fermé sur laquelle se fonde notre approche.

Au-delà de ces perspectives directes, nous envisageons plusieurs autres directions
pour nos futurs travaux. Par exemple, il serait intéressant d’expérimenter les algo-
rithmes et modèles que nous avons proposés avec des données collectées à partir de
médias sociaux autres que Twitter – chose que nous n’avons malheureusement pu
faire jusqu’à présent du fait de la difﬁculté à obtenir des données à partir d’autres ser-
vices. Cela nous permettrait d’étudier la généricité pratique de nos propositions, mais
aussi de savoir dans quelle mesure les propriétés du phénomène de diffusion de l’in-
formation mis en avant par nos travaux sont indépendantes ou non du média social
étudié. Il serait par ailleurs intéressant de considérer la problématique de la détection
de sentiments et d’opinion. Cela pourrait par exemple nous permettre d’améliorer
la description des évènements détectés avec MABED à partir des médias sociaux, et
pourrait également faire l’objet d’un service supplémentaire dans le logiciel SONDY.

176

Bibliographie

Aggarwal, C. C. (2011), Social Network Data Analytics, Springer.

Aiello, L. M., A. Barrat, R. Schifanella, C. Cattuto, B. Markines, et F. Menczer (2012),
Friendship prediction and homophily in social media, ACM Trans. Web, 6(2), 137–
170.

Aiello, L. M., G. Petkos, C. Martin, D. Corney, S. Papadopoulos, R. Skraba, A. Goker,
Y. Kompatsiaris, et A. Jaimes (2013), Sensing trending topics in twitter, IEEE Trans.
Multimedia, 15(6), 1–15.

AlSumait, L., D. Barbará, et C. Domeniconi (2008), On-line lda : Adaptive topic mo-
dels for mining text streams with applications to topic detection and tracking, in
ICDM ’08, pp. 3–12.

Anagnostopoulos, A., R. Kumar, et M. Mahdian (2008), Inﬂuence and correlation in

social networks, in KDD ’08, pp. 7–15.

Appel, G. (2005), Technical analysis power tools for active investors, Financial Times

Prentice Hall, pp. 166–167.

Auber, D. (2004), Tulip – a huge graph visualization framework, in Graph Drawing

Software, pp. 105–126, Springer.

Backstrom, L., et J. Leskovec (2011), Supervised random walks : Predicting and re-

commending links in social networks, in WSDM ’11, pp. 635–644.

Bakshy, E., I. Rosenn, C. Marlow, et L. A. Adamic (2012), The role of social networks

in information diffusion, in WWW ’12, pp. 519–528.

Banerjee, A. V. (1992), A simple model of herd behavior, The Quarterly Journal of

Economics, 107(3), 797–817.

Bastian, M., S. Heyman, et M. Jacomy (2009), Gephi : An open source software for

exploring and manipulating networks, in ICWSM, pp. 361–362.

Batagelj, V., et M. Zaversnik (2011), Fast algorithms for determining (generalized)
core groups in social networks, Advances in Data Analysis and Classiﬁcation, 5(2),
129–145.

177

Bibliographie

Benhardus, J., et J. Kalita (2013), Streaming trend detection in twitter, IJWBC, 9(1),

122–139.

Bentley, J. (1984), Programming pearls : algorithm design techniques, CACM, 27(9),

865–873.

Bernstein, M. S., B. Suh, L. Hong, J. Chen, S. Kairam, et E. H. Chi (2010), Eddi :
Interactive topic-based browsing of social status streams, in UIST ’10, pp. 303–312.

Bi, B., Y. Tian, Y. Sismanis, A. Balmin, et J. Cho (2014), Scalable topic-speciﬁc in-
ﬂuence analysis on microblogs, in WSDM ’14, pp. 513–522, doi :10.1145/2556195.
2556229.

Blei, D., A. Ng, et M. Jordan (2003), Latent dirichlet allocation, JMLR, 3, 993–1022.

Blondel, V., J.-L. Guillaume, R. Lambiotte, et E. Lefebvre (2008), Fast unfolding of
communities in large networks, Journal of Statistical Mechanics : Theory and Expe-
riment, P10008, 1–12.

Bouillot, F., P. Nhat Hai, N. Béchet, S. Bringay, D. Ienco, S. Matwin, P. Poncelet,
M. Roche, et M. Teisseire (2012), How to extract relevant knowledge from tweets ?,
in ISIP ’12, pp. 111–120.

Boyd, D. (2006), Friends, friendsters, and myspace top 8 : Writing community into

being on social network sites, First Monday, 11(12).

Boyd, D., et N. Ellison (2007), Social network sites : Deﬁnition, history, and scholar-

ship, Journal of Computer-Mediated Communication, 13(1), 210–230.

Brockmann, D., L. Hufnagel, et T. Geisel (2006), The scaling laws of human travel,

Nature, 439(7075), 462–465.

Brown, P., et J. Feng (2011), Measuring user inﬂuence on twitter using modiﬁed k-

shell decomposition, in ICWSM ’11 Workshops.

Can, F., T. Özyer, et F. Polat (2014), State of the Art Applications of Social Network

Analysis, Springer.

Casella, G., et E. I. George (1992), Explaining the gibbs sampler, The American Statis-

tician, 46(3), 167–174.

Cheng, J.-J., Y. Liu, B. Shen, et W.-G. Yuan (2013), An epidemic model of rumor

diffusion in online social networks, The European Physical Journal B, 86(1).

178

Bibliographie

Coleman, T. F., et Y. Li (1996), A reﬂective newton method for minimizing a quadratic
