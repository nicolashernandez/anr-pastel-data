http://ethesis.inp-toulouse.fr/archive/00002506/01/navarro.pdf

En vue de l'obtention du

DOCTORAT DE L'UNIVERSITÉ DE TOULOUSE

Institut National Polytechnique de Toulouse (INP Toulouse)

Délivré par :

Discipline ou spécialité :

Intelligence Artificielle

Présentée et soutenue par :

M. EMMANUEL NAVARRO
le lundi 4 novembre 2013

Titre :

METROLOGIE DES GRAPHES DE TERRAIN, APPLICATION A LA

CONSTRUCTION DE RESSOURCES LEXICALES ET A LA

RECHERCHE D'INFORMATION.

Ecole doctorale :

Mathématiques, Informatique, Télécommunications de Toulouse (MITT)

Institut de Recherche en Informatique de Toulouse (I.R.I.T.)

Unité de recherche :

Directeur(s) de Thèse :

M. HENRI PRADE
M. BRUNO GAUME

Rapporteurs :

Mme CLEMENCE MAGNIEN, UNIVERSITE PIERRE ET MARIE CURIE

M. PIERRE FRAIGNIAUD, UNIVERSITE PARIS 7

M. PIERRE ZWEIGENBAUM, LIMSI

Membre(s) du jury :

M. ERIC GAUSSIER, UNIVERSITE GRENOBLE 1, Président
M. BRUNO GAUME, UNIVERSITE TOULOUSE 3, Membre

M. FRÉDÉRIC AMBLARD, UNIVERSITE TOULOUSE 1, Membre

M. HENRI PRADE, UNIVERSITE TOULOUSE 3, Membre

1
2
2
2

2

3

Résumé

Les graphes construits à partir de données réelles d’origines variées (les graphes
de synonymie, les réseaux sociaux, les graphes d’interaction de protéines ou encore
les graphes issus d’internet, . . .) comportent des propriétés communes non triviales.
Cela a été découvert il y a maintenant près de quinze ans. Ces graphes sont appe-
lés complex networks ou graphes de terrain. Depuis ce nouvel éclairage, il a émergé
un nombre considérable de travaux qui tendent à former une nouvelle discipline.
On parle de « science des réseaux » (« network science ») ou encore de l’étude des
grands graphes de terrain. Bien sûr cette discipline hérite de nombreuses théories
et résultats plus anciens provenant en particulier de la théorie des graphes, des sta-
tistiques ou encore de l’analyse de données. Toutefois, l’objet d’étude et la prise en
compte de ces propriétés particulières font que cette discipline se distingue de plus
en plus clairement de ses « parents ». Les retombées de l’étude des grands graphes
de terrain concernent potentiellement un nombre important de domaines. Il faut ce-
pendant remarquer que les applications se développent plus fortement dans certaines
disciplines. L’étude des réseaux sociaux est, aujourd’hui, le champ d’application pri-
vilégié des travaux sur les graphes de terrain. À l’inverse, par exemple, les travaux
en traitement automatique du langage sont beaucoup moins nombreux à intégrer les
résultats de l’analyse des graphes de terrain 1.

C’est dans ce cadre général que s’inscrit cette thèse. Entre théories et applica-
tions, nous développons et étudions des outils d’analyse de graphe de terrain, et les
utilisons ensuite pour résoudre des problèmes de traitement automatique du langage
naturel et de recherche d’information. Après un chapitre d’introduction qui présente
le contexte scientiﬁque de l’étude des graphes de terrain et déﬁnit un certain nombre
de notions, ce rapport s’organise en deux parties. Nous nous intéressons tout d’abord
aux mesures de proximité pouvant être déﬁnies entre les sommets d’un graphe, puis
dans une seconde partie aux méthodes de clustering de graphe, et en particulier de
graphe biparti.

Une première partie traite donc des mesures de proximité (ou mesures de simi-
larité) entre sommets d’un graphe. Nous dressons tout d’abord un état de l’art de
1. Précisons qu’il existe tout de même de tels travaux, en témoigne le workshop annuel Text-

Graphs (http://www.textgraphs.org/).

4

ces mesures tant locales que globales (soit ne dépendant que d’un voisinage immé-
diat des sommets, soit utilisant la topologie complète du graphe). Nous proposons
ensuite une nouvelle méthode, la conﬂuence, basée sur des marches aléatoires en
temps courts. Cette méthode vériﬁe plusieurs caractéristiques proﬁtables, en parti-
culier elle se montre indépendante de la densité du graphe. Enﬁn nous proposons une
importante comparaison expérimentale de ces mesures de proximité. Cette compa-
raison permet de mieux comprendre les diﬀérences et ressemblances entre toutes ces
mesures, et cela permet de mettre en avant diﬀérentes propriétés les caractérisant.
Nous utilisons ensuite ces mesures de proximité entre sommets pour nous pencher
sur un autre problème de l’étude des graphes de terrain : mesurer une similarité entre
deux graphes. Nous nous concentrons sur les méthodes de comparaison de graphes
partageant le même ensemble de sommets. Les méthodes classiques, par exemple la
distance d’édition, en considérant les graphes comme des ensembles d’arêtes indé-
pendantes, ne mesurent pas, ou mal, la similarité de structure que peuvent présenter
deux graphes. Nous proposons une généralisation de la distance d’édition classique
entre graphes qui se montre plus robuste et répond au problème. Cette distance
d’édition est mise en application pour comparer et fusionner des graphes de syno-
nymie. Nous montrons notamment que ces ressources peuvent avoir une structure
similaire malgré le désaccord apparent qui existe entre leurs ensembles d’arêtes.
Cette dernière conclusion a des implications linguistiques quant à la déﬁnition de la
notion de synonymie.

Nous développons ensuite une application des mesures de proximité entre som-
mets d’un graphe pour construire un système d’aide à la création de ressources
lexicales. L’idée est de pouvoir proposer des candidats synonymes à un contribu-
teur à partir de l’ensemble des relations déjà établies dans la ressource. Une preuve
de concept de ce système a été développée sur le dictionnaire collaboratif, le Wik-
tionnaire. En eﬀet, nous présentons une étude de cette ressource qui montre que le
nombre d’entrées progresse plus rapidement que le nombre de relations (en particu-
lier de synonymies). Cela indique que les contributions consistent plus facilement en
des ajouts d’entrées (et déﬁnitions) qu’en des ajouts de relations (en particulier de
synonymie). Un système d’assistance à l’ajout de relations répond donc manifeste-
ment à un besoin.

Dans une seconde partie nous nous intéressons au problème de clustering de
graphe et en particulier de graphe biparti. Nous établissons, dans un premier temps,
un parallèle entre l’analyse formelle de concepts et le clustering de graphe biparti. Ce
parallèle apporte un éclairage pertinent pour comprendre les diﬀérentes méthodes
de clustering de graphe. Il permet aussi de nous pencher sur le cas particulier où l’on
souhaite partitionner les objets d’un graphe biparti sans imposer de partitionnement
correspondant sur les propriétés. Nous proposons et évaluons une méthode simple
qui répond à ce problème.

5

Nous présentons ensuite Kodex, un système de classiﬁcation automatique des ré-
sultats d’une recherche. Ce système est une application en recherche d’information
des méthodes de clustering de graphe biparti vues précédemment. Une architecture
logicielle modulable a été mise en place, elle est présentée dans ses grandes lignes.
Elle permet à la fois de construire des scripts pour lancer des campagnes d’éva-
luation, et de façonner facilement des applications « ﬁnales ». Nous avons évalué
Kodex sur une collection d’un peu plus de deux millions de pages web. Cette éva-
luation montre les avantages de l’approche et permet en outre de mieux comprendre
certaines diﬀérences entre méthodes de clustering. Aussi deux applications de dé-
monstration de Kodex sont disponibles en ligne. Elles fonctionnent en utilisant les
APIs de recherche du Guardian et de DBLP. Nous les présentons brièvement.

6

7

Remerciements

Je clôture avec émotion la rédaction de ce rapport par cette traditionnelle et
indispensable section de remerciements. Ce n’est pas la plus facile des parties à
rédiger tant il me tient à cœur de saluer, avec des mots justes, les nombreuses
personnes qui ont parcouru avec moi l’un ou l’autre des chemins qui ont relié mes
premiers travaux de recherche à ce présent rapport.

Merci tout d’abord à Bruno et à Henri. Merci pour m’avoir fait conﬁance, pour
m’avoir toujours encouragé. Ce fut une chance incroyable d’avoir deux directeurs de
thèse si complémentaires.

Merci à Pierre Fraigniaud, Clémence Magnien et Pierre Zweigenbaum d’avoir
accepté d’être rapporteurs de mon travail. Ce n’est pas une tâche aisée que de lire
et évaluer les 230 pages qui suivent. Merci pour vos riches retours. De même, merci
à Frédéric Amblard et à Eric Gaussier d’avoir accepté d’être membre du jury.

J’ai eu l’opportunité durant ma thèse de proﬁter de nombreuses collaborations
avec plusieurs chercheurs, étudiants et ingénieurs. Sans ce travail en équipe, beau-
coup des travaux présentés dans ce rapport n’auraient été possibles.

De toutes ces collaborations, la plus longue et la plus enrichissante aura été celle
avec Yannick, collègue et ami précieux. C’est toujours un plaisir de coder, débattre,
réﬂéchir, puis refactorer ensemble ! Merci pour tous ces captivants moments partagés
avec toi. Je suis sûr que nous allons continuer à faire équipe et que nos milles idées
vont nous mener à de belles réalisations.

Merci aussi à Pierre pour nos enrichissantes discussions entre Paris, Toulouse,
Taiwan et Beytan, par IRC ou IRL (mais jamais vraiment AFK). Je ne doute pas que
nos collaborations vont s’intensiﬁer : la segmentation et le clustering ont beaucoup
à faire sans supervision.

Merci à Benoît, en particulier pour le travail sur la comparaison de graphes.
Merci à Franck sans qui le travail sur Wiktionary n’aurait pas été possible. Salut et
merci à Yann (alors on se le fait ce SLAM v2 ?). Merci à Karen et Guillaume pour
leur précieuse collaboration sur l’évaluation de Kodex.

Merci enﬁn à tous les collègues de l’ENSEEIHT, de l’IRIT ou de CLLE-ERSS. Il
est diﬃcile de citer toutes les personnes auxquelles je peux penser, et ce l’est encore
plus pour celles que j’oublie. Mais s’il est si plaisant de travailler dans ces lieux,
votre bonne humeur et « vos humours » y sont certainement pour beaucoup.

8

Ce rapport doit aussi beaucoup à nombre de proches et d’amis sans qui ces

années de travail n’auraient pas été si agréables.

Merci à mes parents pour leurs encouragements et leur soutien tout au long de
mes études. De la prépa jusqu’à cette thèse, en passant par l’école d’ingénieur... la
route est longue.

Merci en particulier à mon frère, Étienne, pour tous ses retours (très) pertinents,

et je lui souhaite plein de courage pour la rédaction qu’il entame à son tour.

Merci à Choucas et Agnès pour les (plus que) précieuses relectures de dernières

minutes.

Une immense embrassade de mercis à toute l’équipe de colocataires : Nico, Nata,
Coralie et Seb. Merci d’avoir toujours supporté le Manu tant absorbé dans cette
rédaction. Cette embrassade vaut aussi pour Natcho, Yannick, Jojo et Vincent.

Et enﬁn un immense merci à Sarah, la plus belle des (sages-)femmes avec qui
nous vivons cette année « deux » fabuleuses grossesses, dont les accouchements vont
se suivre de près. Merci de m’avoir soutenu au quotidien tout en faisant grandir
« notre petite crevette » dans ton ventre. Nous nous construisons de merveilleux
souvenirs, mais le plus heureux est que ces aboutissements ne sont que le début de
nouvelles aventures. Merci d’être là.

Table des matières

Résumé

Remerciements

Table des matières

9

3

7

9

1 Introduction

13
1.1 De la théorie des graphes à l’étude des graphes de terrain . . . . . . . 13
1.2 Notions de théorie des graphes, notations . . . . . . . . . . . . . . . . 16
1.2.1 Graphes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
1.2.2 Graphes dirigés, graphes pondérés et graphes bipartis . . . . . 17
1.2.3 Représentations matricielles usuelles
. . . . . . . . . . . . . . 18
1.2.4 Graphes complets, cliques et bicliques . . . . . . . . . . . . . . 20
1.2.5 Chemins et composantes connexes . . . . . . . . . . . . . . . . 21
1.2.6 Graphes aléatoires et graphes réguliers . . . . . . . . . . . . . 21
1.2.7 Propriétés structurelles . . . . . . . . . . . . . . . . . . . . . . 22
1.3 Propriétés non triviales communes . . . . . . . . . . . . . . . . . . . . 27
1.3.1 Le phénomène de petit monde . . . . . . . . . . . . . . . . . . 27
1.3.2 Distribution des degrés en loi de puissance . . . . . . . . . . . 28
1.3.3 Plusieurs familles de graphes de terrains ?
. . . . . . . . . . . 29
1.4 Conclusion du chapitre . . . . . . . . . . . . . . . . . . . . . . . . . . 29

I Marches aléatoires courtes, application à diﬀérents
problèmes de l’étude des graphes de terrain
2 Similarités entre sommets

31
33
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.2 Proposition d’une mesure normalisée . . . . . . . . . . . . . . . . . . 34
2.2.1 Marches aléatoires en temps courts . . . . . . . . . . . . . . . 35
2.2.2 Mesure de conﬂuence . . . . . . . . . . . . . . . . . . . . . . . 40
2.3 État de l’art des mesures de similarité entre sommets . . . . . . . . . 42
2.3.1 Mesures locales . . . . . . . . . . . . . . . . . . . . . . . . . . 43

10

TABLE DES MATIÈRES

2.3.2 Mesures globales
. . . . . . . . . . . . . . . . . . . . . . . . . 48
2.3.3 Mesures « semi-globales » . . . . . . . . . . . . . . . . . . . . 56
2.4 Comparaison expérimentale . . . . . . . . . . . . . . . . . . . . . . . 60
2.4.1 Mise en place expérimentale . . . . . . . . . . . . . . . . . . . 61
2.4.2 Corrélation entre les méthodes . . . . . . . . . . . . . . . . . . 64
2.4.3 Comparaison sur des graphes aléatoires Erdős-Rényi . . . . . . 69
2.4.4 Comparaison sur des graphes simples comportant des clusters
75
2.4.5 Comparaison sur un graphe de terrain . . . . . . . . . . . . . 79
2.4.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
2.5 Conclusion du chapitre . . . . . . . . . . . . . . . . . . . . . . . . . . 87

3 Comparaison robuste de graphes

3.1
3.2 Comparaison de graphes ayant les mêmes sommets

89
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
. . . . . . . . . . 90
3.2.1 Distance d’édition entre graphes (GED)
. . . . . . . . . . . . 90
3.2.2
Structures identiques, pourtant sans « atome » commun . . . . 91
3.2.3 De l’isomorphisme de graphes aux distances d’édition . . . . . 93
3.3 Proposition d’une mesure de comparaison robuste . . . . . . . . . . . 94
Similarité binaire, diﬀérents cas de ﬁgure . . . . . . . . . . . . 94
3.3.1
3.3.2
Similarité continue, généralisation de GED . . . . . . . . . . . 97
3.3.3 Choix de la similarité ∆ . . . . . . . . . . . . . . . . . . . . . 99
3.3.4 Évaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
3.4 Fusion de graphes de terrain . . . . . . . . . . . . . . . . . . . . . . . 105
3.5 Application à la comparaison de ressources lexicales . . . . . . . . . . 106
. . . . . . . . . . . . . 106
. . . . . . . . . . . . . . . . . . . . 107
3.6 Conclusion du chapitre . . . . . . . . . . . . . . . . . . . . . . . . . . 109

3.5.1
3.5.2 Analyse des comparaisons

Introduction des ressources comparées

4 Enrichissement semi-automatique de réseaux lexicaux

4.2 Édition collaborative par les foules

111
4.1 Ressources lexicales, construction et évaluation . . . . . . . . . . . . . 112
4.1.1 Méthodes de construction de ressources lexicales . . . . . . . . 113
4.1.2 Évaluation des ressources lexicales . . . . . . . . . . . . . . . . 114
. . . . . . . . . . . . . . . . . . . 115
4.2.1 Plusieurs modèles . . . . . . . . . . . . . . . . . . . . . . . . . 116
4.2.2 Le cas de Wiktionary . . . . . . . . . . . . . . . . . . . . . . . 117
4.3 Enrichissement semi-automatique . . . . . . . . . . . . . . . . . . . . 121
4.3.1 Modèle de graphes bipartis pondérés
. . . . . . . . . . . . . . 121
4.3.2 Calcul de similarités, calcul de candidats . . . . . . . . . . . . 124
4.3.3 Évaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
4.4 Une implémentation fonctionnelle : le système Wisigoth . . . . . . . . 131
4.5 Conclusion du chapitre . . . . . . . . . . . . . . . . . . . . . . . . . . 132

TABLE DES MATIÈRES

11

II Clustering de graphe biparti, théories et applications135
137
5 Clustering de graphe biparti
5.1 Diﬀérentes familles d’approches . . . . . . . . . . . . . . . . . . . . . 138
5.1.1 Partitionnement de graphe biparti . . . . . . . . . . . . . . . . 138
5.1.2 Bi-clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
5.1.3 Détection de communautés sur un graphe biparti
. . . . . . . 140
5.1.4 Analyse formelle de concepts . . . . . . . . . . . . . . . . . . . 142
5.1.5 Recherche d’itemsets fréquents . . . . . . . . . . . . . . . . . . 144
5.2 De l’analyse formelle de concepts au clustering de graphe biparti . . . 145
5.2.1 AFC, extension possibiliste . . . . . . . . . . . . . . . . . . . . 145
5.2.2 Traduction en théorie des graphes . . . . . . . . . . . . . . . . 147
5.2.3 Conclusion : vers deux types de clustering ?
. . . . . . . . . . 152
5.3 Partitionnement des objets, en passant par les concepts . . . . . . . . 153
5.3.1 Clustering du graphe objets-concepts . . . . . . . . . . . . . . 154
5.3.2 Étiquetage des clusters . . . . . . . . . . . . . . . . . . . . . . 156
5.3.3 Évaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
5.3.4 Conclusions et perspectives
. . . . . . . . . . . . . . . . . . . 159
5.4 Pré-traitement par marches aléatoires . . . . . . . . . . . . . . . . . . 162
5.4.1 Généralisations de l’AFC sur données pondérées ou incomplètes162
5.4.2 Filtrage et binarisation d’un graphe biparti éventuellement

pondéré . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
5.4.3 Évaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
5.4.4 Conclusion et perspectives . . . . . . . . . . . . . . . . . . . . 172
5.5 Conclusion du chapitre . . . . . . . . . . . . . . . . . . . . . . . . . . 174

6 Application à l’organisation des résultats d’une recherche d’in-

Introduction, clustering de graphe documents-termes

175
formation
. . . . . . . . . 176
6.1
6.2 État de l’art : le clustering appliqué à la recherche d’information . . . 178
Implémentation logicielle d’une chaîne de traitement modulable (Kodex)181
6.3
6.3.1 Objectifs, motivations et choix techniques
. . . . . . . . . . . 181
6.3.2 Types de données principaux . . . . . . . . . . . . . . . . . . 182
6.3.3 Chaînes de traitement, chaînes de composants . . . . . . . . . 183
6.3.4 Gestion des options des composants . . . . . . . . . . . . . . . 184
6.3.5 Conﬁguration d’une chaîne de traitement en ligne . . . . . . . 186
6.3.6 Analyse des documents hors-ligne . . . . . . . . . . . . . . . . 188
6.4 Évaluation sur une collection de deux millions de documents . . . . . 190
6.4.1 Collection et jeu d’évaluation . . . . . . . . . . . . . . . . . . 191
6.4.2 Conﬁguration de Kodex
. . . . . . . . . . . . . . . . . . . . . 191
6.4.3 Évaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194

12

TABLE DES MATIÈRES

6.4.4 Conclusions et perspectives

. . . . . . . . . . . . . . . . . . . 201
6.5 Deux applications de Kodex . . . . . . . . . . . . . . . . . . . . . . . 202
6.5.1 Application sur l’API de recherche du Guardian . . . . . . . . 202
6.5.2 Application sur l’API de recherche de DBLP . . . . . . . . . . 204
. . . . . . . . . . . . . . . . . . . . . . . . . 206

6.6 Conclusions du chapitre

Conclusion

Bibliographie

Table des ﬁgures

Liste des tableaux

Index

209

213

227

229

231

13

Chapitre 1
Introduction

Ce chapitre a pour objectif de déﬁnir le contexte scientiﬁque dans lequel s’inscrit
ce rapport de thèse. Ce chapitre nous permet aussi de donner un certain nombre de
concepts qui seront utilisés par la suite.

Tout d’abord, la section 1.1 retrace l’histoire de l’étude des graphes : de la théorie
des graphes aux travaux actuels sur les grands graphes de terrain en passant par
le développement de l’étude des graphes sociaux. Nous présentons en section 1.2
l’ensemble des notations et concepts de la théorie des graphes qui seront utiles dans
les sections et chapitres suivants. Enﬁn la section 1.3 déﬁnit les caractéristiques
particulières communes aux graphes de terrain.

1.1 De la théorie des graphes à l’étude des graphes

de terrain

Les graphes sont des outils de modélisation et de raisonnement puissants et
néanmoins simples dans leur formalisation. Le développement des théories mathé-
matiques sous-jacentes est relativement ancien, il connaît cependant un renouveau
important depuis environ une quinzaine d’années.

On indique en général que l’étude des graphes, comme objets mathématiques,
commence en 1735 quand Leonhard Euler présente à l’académie de Saint Péters-
bourg le problème des sept ponts de Königsberg (l’article a ensuite été publié en
1741, [Euler, 1741]). Ce problème consiste à trouver un chemin qui permette de
revenir au lieu de départ après être passé une seule fois par chacun des sept ponts
de la ville. Ce type de chemin dans un graphe est maintenant nommé chemin eu-
lérien. Depuis ce premier travail s’est développée une branche des mathématiques
discrètes : la théorie des graphes [Berge, 1970]. Cette théorie s’intéresse à un cer-
tain nombre de questions pratiques, par exemple : quel est le plus court chemin
reliant deux sommets ? quelle est la taille du plus court chemin passant par tous les

14

CHAPITRE 1. INTRODUCTION

sommets ? comment, avec le moins de couleurs possibles, colorier chaque sommet
d’un graphe de façon à ce que deux sommets adjacents soient toujours de couleurs
diﬀérentes ? connaissant la capacité de chaque arête, quel est le ﬂux maximal pos-
sible entre deux sommets ? ou encore, comment séparer les sommets d’un graphe
en deux parties de même taille en retirant un nombre minimal d’arêtes entre ces
parties ? Tous ces développements ont eu, et ont encore, d’importantes applications
en chimie [Trinajstić, 1983; Faulon, 1998], en physique [Harary, 1967], en ingénierie
(électronique, télécommunication, etc.) [Ohlrich et al., 1993; Kahng et al., 2011] et
en informatique [Bollobás, 1998; Diestel, 2000]. Il convient aussi de noter que les
systèmes de représentation des connaissances constituent une autre application im-
portante de la théorie des graphes. Nous pouvons citer le formalisme des graphes
conceptuels [Chein et Mugnier, 2008], ou encore celui des réseaux bayésiens [Naïm
et al., 2011].

À partir des années 1920, et parallèlement au développement de la théorie des
graphes, un autre usage des graphes se développe en sciences humaines par l’analyse
des réseaux sociaux 1. En eﬀet, bien que cette discipline se soit fortement développée
depuis moins d’une trentaine d’années, les premiers travaux datent de la première
moitié du XXème siècle, voir [Freeman, 1996], [Wasserman et Faust, 1994, chap. 1.2]
et [Carrington et al., 2005, chap. 1]. L’analyse des réseaux sociaux, bien que s’ap-
puyant en partie sur la théorie des graphes, pose des problèmes diﬀérents de ceux
étudiés classiquement dans cette discipline. Par exemple : quels sont les acteurs (i.e.
sommets) centraux dans un groupe social ? quelles sont les diﬀérentes communau-
tés ? comment modéliser tel type de structure sociale ? Les travaux d’analyse des
réseaux sociaux s’appuient donc aussi fortement sur les statistiques et la théorie des
probabilités [Wasserman et Faust, 1994; Carrington et al., 2005]. De plus il faut
remarquer que les graphes étudiés proviennent de relevés « de terrain ».

Dans ce contexte, plusieurs découvertes faites il y a un peu plus d’une décen-
nie ont engendré un intérêt nouveau à l’étude des graphes au sein de diﬀérentes
communautés scientiﬁques. Watts et Strogatz [1998] ainsi que Barabási et Albert
[1999] ont montré que des graphes issus de systèmes réels variés (interaction de pro-
téines, liens hypertextes, réseaux sociaux, etc.) possèdent des propriétés non-triviales
communes. Dans ces graphes, deux sommets sont toujours reliés par un chemin de
longueur faible (on parle du phénomène de petit monde). Aussi, alors qu’une large
majorité des sommets ont peu de voisins, un certain nombre de sommets sont très
fortement liés. Nous présentons plus en détails ces propriétés en section 1.3. Cela
a donné un nouvel intérêt à l’étude et à l’utilisation des graphes comme outils de
modélisation et a engendré un nombre considérable de travaux qui en sortant du
1. Il faut bien remarquer que l’analyse des réseaux sociaux en sciences humaines a peu à voir
avec le développement récent de services de « réseautage » social sur internet. Ces services ouvrent
cependant de nouvelles perspectives à l’analyse des réseaux sociaux.

1.1. DE LA THÉORIE DES GRAPHES À L’ÉTUDE DES GRAPHES
DE TERRAIN
15
champ de la théorie des graphes classique tendent à créer une nouvelle discipline.
On parle de « network science » ou de « network theory » (science ou théorie des
réseaux, en français). Les graphes, issus de systèmes réels et possédant des pro-
priétés non-triviales particulières, sont souvent nommés complex networks 2 dans la
littérature anglophone (i.e. réseaux complexes). On parle parfois aussi de grands ré-
seaux d’interaction. Pour notre part, nous préférons le terme réseaux de terrain ou
graphes de terrain 3, pour bien souligner le fait que ces graphes sont issus de données
« réelles », de données de « terrain ». Nous parlerons donc de l’étude de graphes de
terrain, plutôt que de science des réseaux.

Notons que beaucoup de travaux en analyse des réseaux sociaux sont précur-
seurs de cette nouvelle discipline, tant par les problèmes abordés, les méthodes
utilisées, que par les objets d’étude (graphes issus de systèmes réels). D’une autre
manière, cette nouvelle discipline hérite aussi directement des travaux de la théorie
des graphes s’intéressant aux graphes aléatoires, en particulier les travaux d’Erdős
et Rényi [1959, 1960] et ceux qui ont suivi 4.

L’étude des graphes de terrain ou la science des réseaux est donc la discipline qui
s’intéresse à développer des outils et méthodes tâchant de modéliser et d’exploiter des
données ou systèmes réels prenant la forme de graphes. La simplicité de cette déﬁni-
tion indique bien le caractère très général de cette discipline. Eﬀectivement, malgré
sa jeunesse, une riche variété de travaux la compose (ce qui ne rend pas évident
le dessin de frontières nettes). De plus ce domaine d’étude est pluridisciplinaire à
plusieurs niveaux : tout d’abord du fait des applications (en sociologie, biologie, in-
génierie, recherche d’information, traitement automatique du langage naturel, etc.),
et ensuite du fait de l’origine des chercheurs s’y intéressant (mathématiques discrètes
et théorie des graphes, étude des réseaux sociaux, physique statistique, analyse de
données, intelligence artiﬁcielle, systèmes multi-agents, systèmes complexes, etc.).
Notons, pour ﬁnir ce rapide historique, que « l’écosystème » scientiﬁque de cette
nouvelle discipline commence à se structurer. Au niveau international, on peut citer
la conférence annuelle NetSci 5 qui existe depuis 2006. Aussi, en France la conférence
MARAMI 6 existe maintenant depuis 2010.

2. Le terme complex networks provient du parallèle existant avec les systèmes complexes. Suc-
cinctement, un système est dit « complexe » si on ne peut pas prévoir directement son évolution à
partir des règles de fonctionnement de chacun de ses composants.

3. Nous proposons d’utiliser la traduction terrain network en anglais.
4. Nous recommandons à ce propos le support de cours de van der Hofstad [2013].
5. http://netsci2013.net
6. http://lipn.fr/marami12/MARAMI2012

16

CHAPITRE 1. INTRODUCTION

Figure 1.1 – Exemple de représentation d’un graphe G = (V, E), comportant

9 sommets : V = {a, b, c, d, e, f, g, h, i} et 14 arêtes : E =(cid:8){a, b},{a, c},{b, c},
{c, c},{c, d},{c, e},{c, f},{d, e},{d, f},{d, h},{e, f},{e, g},{g, h},{g, i}(cid:9)
1.2 Notions de théorie des graphes, notations

Cette section a pour objectif d’introduire les notions de base de la théorie des
graphes, ainsi que les propriétés structurelles permettant de caractériser les graphes
de terrain. Ces notions seront utiles dans le reste de ce chapitre, ainsi que dans
les chapitres suivants. En particulier les propriétés présentées en sous-section 1.2.7
seront utilisées en section 1.3.

1.2.1 Graphes

Un graphe G = (V, E) est une paire constituée d’un ensemble de sommets V et
d’un ensemble d’arêtes E. L’ensemble des arêtes E est un sous-ensemble de V (2),
l’ensemble des paires non ordonnées de V (chaque arête étant une paire non ordonnée
de sommets). Les arêtes sont aussi parfois appelées lien. Dans la suite nous suppo-
serons que V (et donc E) est toujours un ensemble ﬁni. Notons que les notations V
et E viennent des mots vertex et edge (sommet et arête en anglais).

La ﬁgure 1.1 donne un exemple simple de graphe avec 9 sommets et 14 arêtes.
Les graphes sont habituellement dessinés, comme sur la ﬁgure, par un ensemble de
points (les sommets) reliés par des lignes (les arêtes). Notons que le placement des
sommets dans le plan (ou dans un espace à trois dimensions) n’est pas donné par le
graphe.
On appelle l’ordre d’un graphe, noté n = |V |, le nombre de sommets du graphe.
La taille d’un graphe, notée m, correspond au nombre d’arêtes : m = |E|. Deux
sommets u, v ∈ V sont dits adjacents (ou voisins) si une arête les relie, c’est-à-
dire si il existe e ∈ E telle que e = {u, v}. Par extension, on dit qu’une paire de

abcdefghi1.2. NOTIONS DE THÉORIE DES GRAPHES, NOTATIONS

17

sommets est adjacente si les deux sommets qui la composent sont adjacents. On
appelle voisinage d’un sommet v l’ensemble des sommets adjacents à V . On note
Γ(v) le voisinage de v :

Γ(v) = {u ∈ V | {v, u} ∈ E}

(1.1)
Le cardinal du voisinage d’un sommet est son degré (ou son degré d’incidence) on le
note d(v) = |Γ(v)|. Par exemple, sur le graphe donné par la ﬁgure 1.1, le voisinage
du sommet f est {c, d, e} et son degré vaut d(f) = 3.
Une arête e ∈ E telle que e = {v, v} est appelée une boucle (self-loop en anglais).
Par exemple l’arête {c, c} est une boucle sur le graphe de la ﬁgure 1.1. Un graphe
est réﬂexif s’il existe une boucle sur chacun de ses sommets : ∀v ∈ V, {v, v} ∈ E.
Il est possible de permettre que des paires de sommets soient reliées par plus
d’une arête, on parle alors d’arêtes multiples, et de multigraphe. À l’inverse on parle
de graphe simple si un graphe ne comporte aucune boucle et aucune arête multiple.
Le graphe G0 = (V 0, E0) est un sous-graphe de G = (V, E) si V 0 ⊆ V et E0 ⊆ E.
Dans ce cas, on peut écrire G0 ⊆ G. On dit qu’un sous-graphe G0 = (V 0, E0) est un
sous-graphe induit du graphe G = (V, E) par l’ensemble de sommets V 0, si toutes
les arêtes de E reliant les sommets de V 0 sont dans E0, c’est-à-dire :

∀u, v ∈ V 0 ,

{u, v} ∈ E ⇒ {u, v} ∈ E0

(1.2)

On note alors G0 = G[V 0].

1.2.2 Graphes dirigés, graphes pondérés et graphes bipartis
Cette sous section présente quelques extensions communes du concept de graphe,
extensions que nous utiliserons dans ce rapport (en particulier les graphes bipartis).
La déﬁnition de graphe présentée dans la sous-section précédente correspond au cas
particulier de graphe uni-parti, non-pondéré et non-dirigé.
Graphes dirigés Un graphe −→
G = (V, E) est dirigé si E est un ensemble de paires
ordonnées, E ⊂ V 2. On parle alors d’arcs plutôt que d’arêtes, un arc est noté (u, v)
plutôt que {u, v}. On parle parfois de graphe orienté plutôt que de graphe dirigé.
Pour chaque sommet v ∈ V on déﬁnit alors dout(v), le degré sortant de v comme
étant le nombre d’arcs partant de v ; et din(v), le degré entrant de v, le nombre d’arcs
arrivant sur v. Dans la suite de ce rapport, nous considérons par défaut les graphes
comme non-orientés, dans le cas contraire nous le préciserons explicitement.

Graphes pondérés Un graphe est dit pondéré si à chaque arête (ou arc dans le
cas d’un graphe dirigé) est associé un poids. On note alors le graphe G = (V, E, ω),
avec ω la fonction de pondération ω : E → R qui donne un poids à chaque arête (ou
arc).

18

CHAPITRE 1. INTRODUCTION

Graphes bipartis Un graphe est biparti si l’ensemble de ses sommets se découpe
en deux classes V> et V⊥ telles que les arêtes ne relient que les sommets d’une
classe à ceux de l’autre classe. C’est-à-dire, un graphe G = (V, E) est biparti si et
seulement si :



V> ∪ V⊥ = V,
V> ∩ V⊥ = ∅

et

E ⊂ (V> × V⊥) ∪ (V⊥ × V>)

il existe V>, V⊥ ⊂ V

tels que

(1.3)

Intuitivement, un graphe biparti comporte deux types de sommets (par exemple des
auteurs V> et des articles V⊥) et les arêtes ne relient jamais deux sommets du même
type (chaque article est relié à ses auteurs, mais n’est pas relié aux autres articles).
Un graphe biparti est aussi appelé bigraphe, et peut être noté G = (V = V>∪ V⊥, E)
ou directement G = (V>, V⊥, E).

Notons que l’on parle de graphe uni-parti lorsque l’on veut préciser qu’un graphe

n’est pas (a priori) biparti.

1.2.3 Représentations matricielles usuelles

Un graphe peut être représenté par une matrice. Nous présentons ici les princi-
pales matrices pouvant être associées à un graphe. Ces descriptions matricielles des
graphes seront intensément utilisées au chapitre 2 pour déﬁnir des similarités entre
sommets.

Matrice d’adjacence. Soit un graphe G = (V, E) d’ordre n = |V |, la matrice
d’adjacence de G est une matrice carrée de taille n × n. On note A cette matrice,
ses éléments se déﬁnissent de la manière suivante :

1 si {i, j} ∈ E

0 sinon.

[A]ij =

(1.4)

Chaque ligne et chaque colonne correspondent donc à un sommet, et il y a un 1
entre une ligne et une colonne si les sommets correspondants sont adjacents dans
le graphe. La ﬁgure 1.2 donne un exemple de graphe avec la matrice d’adjacence
associée.

La matrice d’adjacence d’un graphe non dirigé est symétrique (aij = aji). Les
graphes non dirigés sont ainsi parfois nommés graphes symétriques. Si une pondé-
ration ω : V → R est déﬁnie sur les arêtes du graphe alors la matrice d’adjacence
peut être déﬁnie ainsi :

ω(i, j)

0

[A]ij =

si {i, j} ∈ E
sinon.

(1.5)

1.2. NOTIONS DE THÉORIE DES GRAPHES, NOTATIONS

19



0 1 0 0 0
1 0 1 1 0
0 1 0 0 1
0 1 0 0 1
0 0 1 1 0



A =

(a) Un graphe simple

(b) Matrice d’adjacence associée

Figure 1.2 – Exemple de graphe avec sa matrice d’adjacence

Laplacien. La matrice laplacienne ou simplement le Laplacien est une autre ma-
trice souvent utilisée sur des graphes. Cette matrice, notée L, se déﬁnit ainsi :



d(i)
−1
0

[L]ij =

i = j
i 6= j et {i, j} ∈ E

si
si
sinon.

(1.6)

En notant D la matrice diagonale contenant sur chaque case de la diagonale le degré
du sommet correspondant :

d(i)

0

[D]i,j =

i = j,

si
sinon,

(1.7)

(1.8)



on a :

L = D − A

Par exemple, pour le graphe de la ﬁgure 1.2a on a :



1 0 0 0 0
0 3 0 0 0
0 0 2 0 0
0 0 0 2 0
0 0 0 0 2



D =

et L =



1 −1
−1
0 −1
0 −1
0

0
0
3 −1 −1
0
0 −1
2 −1
2

0 −1 −1

0

2
0

On utilise aussi parfois L, le Laplacien normalisé :

L = D−1/2LD−1/2

(1.9)
avec D−1/2 la matrice diagonale dont chaque case (de la diagonale) est l’inverse de
la racine carrée de la case correspondante dans D. Nous allons aussi utiliser D1/2
et D−1, qui sont construites similairement (racine carrée ou inverse des éléments
diagonaux de D).

1234520

CHAPITRE 1. INTRODUCTION

Matrice de transition. Enﬁn nous introduisons P, la matrice de transition as-
sociée au graphe :

P = D−1A

(1.10)

Par exemple, la matrice de transition du graphe de la ﬁgure 1.2a vaut :



0
1/3
0
0
0

P =

1
0
1/2
1/2
0

0
0
1/3 1/3
0
0
0
0
1/2 1/2



0
0
1/2
1/2
0

Nous reviendrons sur cette matrice au chapitre suivant, lorsque nous déﬁnirons
des marches aléatoires en temps courts sur un graphe (section 2.3.3). En eﬀet la
valeur de [P]ij correspond à la probabilité qu’un marcheur aléatoire présent sur i
avance jusqu’au sommet j.

1.2.4 Graphes complets, cliques et bicliques

.

2

Un graphe est dit complet si tous ses sommets sont adjacents, c’est-à-dire pour
tout u, v ∈ V si u 6= v alors {u, v} ∈ E. En général, les boucles sont ignorées pour
déterminer la complétude d’un graphe. On note Kn le graphe complet d’ordre n,
alors sa taille vaut : m = n(n−1)

Dans un graphe G quelconque, on nomme clique un sous-ensemble de sommets
induisant un sous-graphe complet. Une k-clique est une clique de k sommets. Une
clique maximale est un sous-ensemble maximal de sommets induisant un sous-graphe
complet ; c’est-à-dire une clique telle qu’on ne puisse plus lui ajouter de sommet sans
qu’elle perde la propriété de clique. Par exemple sur le graphe déﬁni en ﬁgure 1.1,
{c, d, e} est une 3-clique non maximale, car {c, d, e, f} est aussi une clique (qui, elle,
est maximale).
Un graphe biparti G = (V>, V⊥, E) est complet si chaque sommet u ∈ V> d’une
classe est connecté à tous les sommets v ∈ V⊥ de l’autre classe. Une biclique est un
ensemble de sommets induisant un sous-graphe biparti complet. Notons qu’il peut
exister des bicliques sur un graphe uni-parti, en eﬀet un sous-graphe induit peut
être biparti sans que le graphe complet ne le soit. Par exemple sur le graphe déﬁni
en ﬁgure 1.1, le sous-graphe induit par {d, e, g, h, i} est biparti (les deux classes
de sommets étant : {g, d} et {e, h, i}), et le sous-ensemble de sommets {d, e, g, h}
forme une biclique. Les bicliques existent, bien sûr, aussi sur les graphes bipartis.
De plus, il est facile de montrer que, sur un graphe biparti, un sous-graphe induit
est biparti et que chaque sommet garde la même classe. Comme pour les cliques,
une biclique est dite maximale si on ne peut y ajouter de sommet sans perdre la
propriété de biclique. Toujours sur le même graphe en ﬁgure 1.1, {d, e, g, h} est une

1.2. NOTIONS DE THÉORIE DES GRAPHES, NOTATIONS

21

biclique maximale car si on ajoute un sommet, soit le sous-graphe induit n’est plus
complet, soit il n’est plus biparti.

1.2.5 Chemins et composantes connexes

Un chemin entre un sommet u et un sommet v est une séquence de sommets
démarrant par u et terminant par v et telle que chaque sommet est adjacent avec son
suivant. La longueur (ou taille) d’un chemin est le nombre d’arêtes le composant,
c’est-à-dire la longueur de la séquence moins un. On note hu, x1, . . . , xk−1, vi un
chemin de longueur k allant du sommet u au sommet v, les sommets intermédiaires
étant notés x1, . . ., xk−1. On appelle parfois chaîne un chemin sur un graphe non-
dirigé, en réservant le terme de chemin pour les graphes dirigés. Dans la suite nous
utiliserons uniquement le terme de chemin, le contexte permettant de désambiguïser.
Sur un graphe dirigé un chemin est orienté, ses arcs doivent être orientés dans le
même sens que le chemin. Si le point de départ u et d’arrivée v d’un chemin (ou
d’une chaîne) se confondent, alors on parle de cycle. Ainsi une boucle est un cycle
de longueur 1.

Deux sommets sont dits connectés s’il existe un chemin de longueur quelconque
entre eux. À l’inverse deux sommets sont déconnectés s’il n’existe aucun chemin les
reliant. Deux sous-ensembles de sommets A, B sont déconnectés si chaque sommet
de A est déconnecté de tous les sommets de B.
Dans un graphe G = (V, E), un sous-ensemble de sommets V 0 ⊂ V est une
composante connexe s’il existe un chemin reliant chaque paire de sommets de V 0.
Un graphe G = (V, E) est dit connexe si V est une composante connexe. Dans un
graphe dirigé, on parle de composante fortement connexe si l’orientation des arcs est
prise en compte. C’est-à-dire une composante fortement connexe est un ensemble
de sommets reliés deux à deux et dans les deux sens par des chemins orientés.
À l’inverse, on parle de composante faiblement connexe si l’orientation des arcs est
ignorée ; il suﬃt qu’il existe un chemin de u à v (sans forcément qu’il existe de chemin
« retour » de v à u) pour que u et v appartiennent à la même composante faiblement
connexe. Une composante connexe est maximale si on ne peut lui ajouter de sommet
sans qu’elle ne perdre sa connexité. Souvent dans la littérature on utilise simplement
« composante connexe » pour parler des composantes connexes maximales. Nous
ferons cette approximation lexicale dans la suite de ce rapport.

1.2.6 Graphes aléatoires et graphes réguliers

Le terme graphe aléatoire indique en général un graphe généré avec le modèle
de graphe aléatoire de Erdős et Rényi [1959]. C’est un graphe dont les m arêtes
sont choisies aléatoirement de façon équiprobable parmi les n(n−1)
arêtes possibles
(n étant l’ordre du graphe). Un tel graphe est donc déﬁni à partir du nombre de

2

22

CHAPITRE 1. INTRODUCTION

sommets n et du nombre d’arêtes m. Un modèle de graphe aléatoire semblable (et
aussi étudié par Erdős et Rényi) consiste en un graphe où chaque arête existe avec
une probabilité p. L’espérance du nombre d’arêtes vaut donc p. n(n−1)

Un graphe régulier est un graphe dont tous les sommets ont exactement le même

.

2

degré.

1.2.7 Propriétés structurelles

Nous présentons ici les propriétés structurelles les plus couramment utilisées. Ces
mesures permettent, en particulier, de caractériser les graphes de terrain par rapport
aux graphes aléatoires. Nous présentons ces caractéristiques en section 1.3

1.2.7.1 Densité, degré moyen et connexité

Les propriétés structurelles les plus évidentes d’un graphe sont son ordre (n =
|V |) et sa taille (m = |E|). Ces deux valeurs permettent de déﬁnir deux grandeurs
classiques de l’étude des graphes : la densité et le degré moyen.

La densité, notée δ, d’un graphe est le rapport entre le nombre d’arêtes du graphe

et le nombre d’arêtes pouvant exister, soit :

δ(G) =

2|E|

|V |.(|V | − 1)

(1.11)

dans le cas d’un graphe simple non-dirigé.

Plutôt que la densité, la mesure fréquemment utilisée pour rendre compte de la

proportion en arêtes du graphe est le degré moyen :
d(v) = 2|E|
|V |

hki = 1
|V |

X

v∈V

(1.12)

Nous verrons que souvent la taille des graphes réels croît linéairement par rapport
à l’ordre. La densité tend donc vers zéro quand les graphes grandissent, alors que
le degré moyen reste à peu près constant. Pour comparer deux graphes d’ordres
diﬀérents, il est donc préférable de comparer leur degré moyen.

La connexité est une autre propriété notoire : à quel point un graphe est-il
connexe ? Généralement il existe dans les graphes réels une composante connexe
contenant une large majorité des sommets. On nomme composante connexe prin-
cipale (ou ccp) cette composante. C’est la composante connexe d’ordre 7 maximal.
Pour mesurer à quel point un graphe est connexe, on mesure simplement le rapport
entre l’ordre de cette composante principale (noté nccp) et l’ordre du graphe.

7. L’ordre d’une composante connexe est son nombre de sommets.

1.2. NOTIONS DE THÉORIE DES GRAPHES, NOTATIONS

23

1.2.7.2 Longueur moyenne des plus courts chemins

On appelle distance géodésique entre deux sommets la longueur d’un 8 plus court
chemin les reliant. Elle est déﬁnie uniquement si les sommets sont connectés. Pour
deux sommets u, v on note lu,v cette distance. Souvent, en théorie des graphes, « la
distance entre deux sommets » réfère à la distance géodésique. Bien que ce soit la
distance « classique » dans un graphe, ce n’est pas la seule qui soit pertinente. Au
chapitre 2 nous présentons un état de l’art des mesures de similarité entre sommets ;
beaucoup de ces mesures dérivent de distances (diﬀérentes de la distance géodésique).
Nous renvoyons à la section 2.3 pour un état de l’art de ces mesures.

Une propriété importante des graphes est la longueur moyenne des plus courts

chemins. On la note l :

l =

1

n(n − 1)

X

u,v∈V

lu,v

(1.13)

On déﬁnit aussi le diamètre d’un graphe comme étant la longueur maximale des plus
courts chemins, on le note d :

d = max
u,v∈V

lu,v

(1.14)

Ces deux mesures ne sont bien déﬁnies que sur un graphe connexe. Or en général
les graphes étudiés ne le sont pas. Toutefois ils comportent une composante connexe
principale, l et d sont alors calculés sur cette composante connexe principale. On les
note lccp et dccp.

1.2.7.3 Coeﬃcient de transitivité

La transitivité est une notion assez intuitive sur un réseau social : si une personne
A est amie avec une personne B qui elle-même est amie avec une personne C, on dit
qu’il y a transitivité si A et C aussi sont amies. Cela correspond à la maxime « les
amis de mes amis sont mes amis ». Cette propriété des graphes est « un classique »
de l’étude des réseaux sociaux [Wasserman et Faust, 1994]. On dit qu’un graphe est
transitif si et seulement si toute paire de sommets {u, v} connectés par un chemin
de longueur deux est adjacente. Il en découle que dans un graphe transitif, toute
paire {u, v} de sommets connectés (i.e. il existe un chemin entre u et v) est une
paire adjacente. Dit autrement, chaque composante connexe induit un sous-graphe
complet. Attention le sens de graphe transitif tel qu’il est déﬁni ici est diﬀérent
du sens généralement entendu dans la littérature de la théorie des graphes 9. Nous
préférons utiliser ce terme dans ce sens de relation transitive, cette notion est en eﬀet
plus utile dans nos travaux que celle entendue classiquement en théorie des graphes.

8. Il peut exister plusieurs plus courts chemins.
9. Dans la littérature de la théorie des graphes classique, un graphe est dit transitif si pour
toute paire de sommets {u, v} il existe un automorphisme (bijection de l’ensemble des sommets
vers lui-même qui préserve l’ensemble des arêtes) intervertissant u et v.

24

CHAPITRE 1. INTRODUCTION

Les graphes étudiés ne sont quasiment jamais transitifs ; il est néanmoins in-
téressant de mesurer à quel point un graphe peut l’être localement. Étant donnés
trois sommets u, v, z ∈ V tels que v soit adjacent aux deux autres ({u, v} ∈ E et
{v, z} ∈ E), alors le coeﬃcient de transitivité (ou coeﬃcient de clustering) mesure
la probabilité que u et z soient adjacents. On le note c et il est déﬁni de la manière
suivante :

c = 3 × nombre de triangles
nombre de fourches

(1.15)
Les triangles sont des sous-graphes complets d’ordre 3 (c’est-à-dire des cycles de
longueur 3), et les fourches des sous-graphes d’ordre 3 ne comportant que 2 arêtes.
Par exemple sur le graphe donné en ﬁgure 1.1 : {d, e, f} forme un triangle et{i, g, h}
une fourche. Le nombre de triangles est multiplié par trois, car chaque triangle
compte trois fourches (toujours sur le même exemple, {d, e, f} engendre bien trois
fourches : d-e-f, e-f-d et f-d-e). Ainsi on assure que 0 ≤ c ≤ 1. Notons que les boucles
sont ignorées pour compter triangles et fourches. Par exemple le graphe donné en
ﬁgure 1.1 (lorsque l’on retire la boucle {c, c}) comporte 5 triangles et 31 fourches,
on a donc c ’ 0.48.
chaque sommet v ∈ V :

Une variante locale de ce coeﬃcient est parfois utilisée. On déﬁnit cloc(v) pour

cloc(v) = nombre de triangles connectés à v
nombre de fourches centrées sur v

(1.16)

Lorsque v ne comporte pas de voisin ou seulement un, alors par déﬁnition cloc(v) = 0.
Le coeﬃcient hcloci pour le graphe entier est alors calculé comme la moyenne des
cloc(v) sur l’ensemble des sommets du graphe. Cette mesure est semblable à celle
donnée par l’eq. (1.15) mais donne des valeurs diﬀérentes. Elle tend en particulier
à donner plus de poids aux sommets de faibles degrés, car pour ceux-ci le nombre
de fourches (dénominateur) est forcément petit. Dans la suite nous utiliserons uni-
quement c déﬁni par l’eq. (1.15). Nous donnons toutefois cette déﬁnition locale du
coeﬃcient de transitivité pour éviter toute confusion, cette mesure étant assez ré-
gulièrement utilisée dans la littérature 10.

Notons enﬁn que le coeﬃcient de transitivité peut être interprété comme une
mesure de la densité locale. C’est-à-dire, la densité des sous-graphes induits sur des
ensembles de sommets « proches ». De nombreuses mesures de densité locale ont été
proposées dans la littérature. Nous renvoyons à [Boccaletti et al., 2006, section 2.1.1]
pour un état de l’art.

Le coeﬃcient de transitivité c peut être adapté aux graphes bipartis de la manière

suivante :

cb =

4 × nombre de carrés

nombre de chemins de longueur trois

(1.17)

10. C’est en particulier cette variante locale qui est utilisée dans l’article fondateur [Watts et

Strogatz, 1998], cf. section 1.3

1.2. NOTIONS DE THÉORIE DES GRAPHES, NOTATIONS

25

Cette extension est naturelle si l’on considère que c est le nombre de cycles non
triviaux de longueur minimale (notée lmin) divisé par le nombre de chemins de
longueur lmin − 1. Sur un graphe uni-parti les plus petits cycles non triviaux sont
bien les triangles (on considère comme trivial un cycle de deux sommets). Or sur
un graphe biparti il n’existe pas de triangle, les plus petits cycles non triviaux
sont donc bien les carrés (cycles de longueur 4). Cette mesure a été proposée par
[Robins et Alexander, 2004] et reprise ensuite par [Latapy et al., 2008]. Notons
que la mesure cloc peut, elle aussi, être adaptée aux graphes bipartis. Mais Latapy
et al. [2008] ont montré expérimentalement que les valeurs alors obtenues ne sont
pas signiﬁcativement diﬀérentes entre graphes réels et graphes aléatoires de même
taille. Cela signiﬁe que ces extensions ne capturent pas de propriété signiﬁcative.
À l’inverse, les mêmes expériences montrent que cb capture bien une densité locale
plus forte sur des graphes réels que sur des graphes aléatoires.

1.2.7.4 Distribution des degrés

On nomme distribution des degrés d’un graphe G, la fonction donnant pour
chaque degré k la probabilité de tirer (uniformément) un sommet ayant ce degré.
On note P(k) cette probabilité. Sur un graphe donné, on note P(k) la proportion
(observée) de sommets de degré k. Une des caractéristiques souvent étudiée sur les
graphes est la forme de cette distribution des degrés : est-ce que tous les sommets
ont presque le même nombre de voisins ? ou au contraire, est-ce qu’il existe des
sommets beaucoup plus connectés que les autres ? Nous revenons sur ces questions
en section 1.3.2.

Sur un graphe dirigé on peut étudier la distribution des degrés entrants Pin(k)
et la distribution des degrés sortants Pout(k). Sur un graphe biparti, on étudie la
distribution des degrés pour chacun des groupes de sommets : P>(k) et P⊥(k).

1.2.7.5 Assortativité et corrélation des degrés

On parle « d’assortativité » (assortativity, en anglais) lorsque les sommets d’un
graphe tendent à être liés principalement à des sommets « similaires ». C’est une
propriété particulièrement étudiée dans les réseaux sociaux, où elle correspond à la
notion d’homophilie : les individus ont davantage tendance à se lier à des personnes
semblables qu’à des personnes diﬀérentes (d’un point de vue de l’âge, de l’origine
sociale, etc.) [Wasserman et Faust, 1994; Carrington et al., 2005].

Nous nous intéresserons spécialement à la corrélation des degrés qui est une
assortativité particulière : est-ce que les sommets de forts degrés sont plutôt liés
à d’autres sommets de forts degrés ? ou sont-ils, à l’inverse, liés à des « petits »
sommets ? Cette caractéristique a l’intérêt de ne dépendre que de la topologie du
graphe, contrairement aux autres mesures d’assortativités qui dépendent de données

26

CHAPITRE 1. INTRODUCTION

externes (l’âge, l’origine ethnique ou sociale, etc.).

Nous présentons ici la mesure d’assortativité introduite par Newman [2003a]
pour les données numériques discrètes, donc adaptée pour mesurer la corrélation de
degrés. Notons que cette mesure s’adapte facilement à des données continues, et le
même article propose une mesure pour des données discrètes non numériques. Pour
un attribut numérique discret déﬁni sur chacun des sommets et prenant valeur dans
D, on pose exy comme la proportion d’arêtes (ou d’arcs) connectant un sommet de
valeur x à un sommet de valeur y. Ainsi les exy vériﬁent les sommes suivantes :

X

x,y∈D

X

y∈D

X

x∈D

exy = 1,

exy = ax,

exy = by

(1.18)

où ax (resp. by) est la proportion des arcs qui partent d’un sommet de valeur x (resp.
qui aboutissent sur un sommet de valeur y). Si le graphe est non dirigé alors ax = bx
et ax est simplement la proportion d’arêtes dont l’une des extrémités a pour valeur
x. L’assortativité est mesurée par le coeﬃcient de Pearson calculé pour cet attribut
en extrémité de chaque arête (ou arc) :

P
x,y∈D x.y.(exy − axby)

σaσb

ρ =

(1.19)
avec σa (resp. σb) l’écart-type de ax (resp. bx). On a −1 ≤ ρ ≤ 1. Une valeur de ρ
proche de 0 indique qu’il n’y a pas d’assortativité dans le graphe. Une valeur positive
témoigne de l’existence d’une assortativité. Et à l’inverse une valeur négative indique
une « dis-assortativité » c’est-à-dire que les sommets tendent à être connectés avec
des sommets diﬀérents (de par l’attribut considéré).

Sur un graphe dirigé G = (V, Ed), lorsque l’attribut étudié est le degré des
sommets, la formulation de ρ peut prendre la forme (facilement calculable) suivante :

ρ =

rhP

m(P
P
e∈Ed je)2ihP
e∈Ed jeke − 1
m(P
e − 1

e∈Ed je)(P

e∈Ed k2

e∈Ed j2

e∈Ed ke)2i
m(P
e0∈Ed ke0)
e − 1

(1.20)

où je (resp. ke) est le degré « en excès » du sommet source (resp. cible) de l’arc e,
et m = |Ed| le nombre d’arcs du graphe. Le degré en excès est simplement le degré
du sommet moins un. L’intérêt du degré en excès est de ne pas prendre en compte
la contribution au degré de l’arc (ou l’arête) entre les sommets de la paire que l’on
considère.
Sur un graphe non dirigé G = (V, E), il suﬃt de transformer chaque arête {u, v}
en deux arcs (u, v) et (v, u). On note Ed cet ensemble d’arcs, on a alors |Ed| =
2|E| = 2m arcs. Si pour chaque arc e0 ∈ Ed on note j0
e0) les degrés en excès
du sommet source (resp. cible), et si pour chaque arête e ∈ E, on note je et ke les
X
degrés en excès de l’un et de l’autre des sommets extrémités, on a alors :

e0 (resp. k0

X

(1.21)

e0 = X

j0

e0∈Ed

e0∈Ed

e0 = 1
k0
2

e∈E

(je + ke)

1.3. PROPRIÉTÉS NON TRIVIALES COMMUNES

27

La formulation de ρ se simpliﬁe alors :

2m (P
P
2m(P
P
e∈E jeke − 1
e) − 1
e∈E(j2

e + k2

ρ =

1
2

e∈E(je + ke))2

e∈E(je + ke))2

(1.22)

Sur un graphe biparti (non dirigé) il est intéressant de mesurer l’assortativité
entre les sommets d’un type (>) et les sommets de l’autre (⊥). On utilise donc
l’équation (1.20), où cette fois je (resp. ke) est le degré (en excès) du sommet de
type > (resp. de type ⊥) de l’arête e. Notons que cela revient à « diriger » chacune
des arêtes du graphe dans le même sens : d’un type de sommet vers l’autre.

1.3 Propriétés non triviales communes

Comme nous l’avons vu en section 1.1, à partir de 1998 il a été découvert que des
graphes issus de disciplines variées partagent des propriétés structurelles similaires
distinctes de celles des graphes aléatoires 11 et des graphes artiﬁciels en grille. Cela a
été le déclencheur d’un important renouveau dans l’étude des graphes. L’objectif de
cette section est de présenter plus en détail ces propriétés structurelles particulières.
Les deux caractéristiques principales des graphes de terrain sont présentées en
sous-section 1.3.1 pour le phénomène de petit-monde, et en sous-section 1.3.2 pour
le phénomène de distribution des degrés en loi de puissance.

1.3.1 Le phénomène de petit monde

L’une des caractéristiques en apparence la plus surprenante des graphes de ter-
rain est le phénomène de petit monde : la longueur du plus court chemin existant
entre deux sommets est toujours « faible », même dans de très grands réseaux.

Ce phénomène a été découvert d’abord sur les réseaux sociaux. Il fut expérimenté
pour la première fois par le sociologue Stanley Milgram en 1967, [Milgram, 1967].
Milgram demanda à 60 personnes du Nebraska (États-Unis) de faire parvenir une
lettre à une personne vivant dans le Massachusetts. Seulement, bien que connaissant
l’adresse du destinataire, les participants ne pouvaient passer la lettre que, de main
à main, à des personnes de leur connaissance. Les lettres qui arrivèrent à destination
ne passèrent en moyenne que par six personnes 12.

Mais ce n’est pas cette caractéristique seule qui est remarquable. En eﬀet, les
graphes aléatoires de type Erdős et Rényi [1959] présentent eux aussi une faible lon-
gueur moyenne des plus courts chemins. C’est la combinaison d’une faible longueur
moyenne des plus courts chemins (l faible) et d’un fort coeﬃcient de transitivité (c
11. Graphes aléatoires de type Erdős et Rényi [1959], voir sous-section 1.2.6
12. Il faut préciser que plus de 90% des lettres n’arrivèrent pas à destination, Milgram fut vive-

ment critiqué pour ne pas avoir signalé cela dans son article de 1967.

28

CHAPITRE 1. INTRODUCTION

fort) qui est remarquable dans les graphes réels. C’est ce que Watts et Strogatz ont
montré dans leur article de 1998. Les graphes réels se situent ainsi entre des graphes
artiﬁciels en « grilles » (fort c et fort l) et les graphes aléatoires (faible c et faible
l). Watts et Strogatz [1998] montrent que ce phénomène se vériﬁe sur des réseaux
réels de diﬀérentes origines (pas seulement sur les réseaux sociaux).

1.3.2 Distribution des degrés en loi de puissance

La seconde caractéristique commune aux graphes de terrain concerne la distribu-
tion des degrés. La distribution des degrés des graphes aléatoires (type Erdős-Rényi)
suit une loi de Poisson, cela signiﬁe que les sommets ont tous un degré relativement
proche d’une valeur moyenne. À l’inverse la distribution des degrés des graphes de
terrain tend à suivre une loi de puissance :

P(k) ≈ C.k−α,

(1.23)
où C est une constante, et α typiquement compris entre 2 et 3. Cela signiﬁe que,
bien que la majorité des sommets aient un faible degré, il existe un nombre non
négligeable de sommets de forts degrés. La distribution des degrés présente une
forte hétérogénéité. Ce phénomène a été mis en évidence par Barabási et Albert
[1999].

Les graphes dont la distribution des degrés suit une loi de puissance sont sou-
vent appelés graphes sans échelle, car la moyenne d’une variable suivant une loi de
puissance n’est pas signiﬁcative. En eﬀet lorsque α est inférieur à 2 le degré moyen
(théorique) diverge, et lorsque α est compris entre 2 et 3 c’est l’écart-type du degré
moyen qui diverge.

Notons qu’il n’est pas aisé de vériﬁer si une distribution suit rigoureusement
une loi de puissance. Une approche courante dans la littérature est d’eﬀectuer une
régression linéaire sur la distribution transformée dans un espace logarithmique.
Cette méthode est une condition nécessaire, mais n’est pas suﬃsante. Aussi l’uni-
versalité du modèle de graphe sans échelle a souvent été remise en question (voir
par exemple : [Pržulj et al., 2004; Khanin et Wit, 2006]). Récemment Clauset et al.
[2009] ont proposé une méthode robuste pour évaluer la vraisemblance qu’une distri-
bution donnée suive une loi de puissance. L’analyse de diﬀérents graphes réels amène
alors à la conclusion que pour beaucoup de graphes il est raisonnable de décrire la
distribution des degrés par une loi de puissance. C’est-à-dire que les données sont
compatibles avec l’hypothèse qu’elles proviennent d’une loi de puissance, bien que
d’autres lois de distribution puissent aussi être plausibles. À l’inverse l’hypothèse
que les valeurs suivent une loi de puissance n’est pas vraisemblable pour certaines
des distributions.

Comme le remarquent Clauset et al. [2009] dans leur conclusion, notons que
pour de nombreux problèmes le fait important est de savoir que la distribution des

1.4. CONCLUSION DU CHAPITRE

29

degrés suit une loi dite à « queue lourde » (heavy tail en anglais). C’est-à-dire que la
distribution est très hétérogène (contrairement à une distribution homogène, comme
sur un graphe aléatoire). Le fait que la distribution corresponde rigoureusement à
une loi de puissance, une loi log-normale, une loi exponentielle, etc. n’est important
que si l’on cherche une modélisation précise.

1.3.3 Plusieurs familles de graphes de terrains ?

Ces deux phénomènes (de « petit monde » et de distribution des degrés très hé-
térogène) montrent que les graphes de terrain partagent des propriétés non triviales
communes. Cela signiﬁe certainement qu’il existe des mécanismes sous-jacents com-
muns à la construction de ces diﬀérents graphes ou systèmes qu’ils modélisent. Plus
pragmatiquement, cela implique aussi que les outils et algorithmes utiles pour mani-
puler ces graphes peuvent (ou doivent) exploiter ces propriétés. Toutefois il convient
d’être prudent, car ces phénomènes n’impliquent pas forcément que tous ces graphes
soient facilement modélisables par un unique mécanisme.

Par exemple, dans la sous-section précédente, nous avons noté que la distribution
des degrés ne correspond pas toujours rigoureusement à une loi de puissance (bien
que toujours hétérogène) [Clauset et al., 2009]. Ce n’est pas la seule propriété qui
ne se comporte pas de la même manière sur tous les graphes réels. En particulier la
corrélation des degrés (voir sous-section 1.2.7.5) présente d’importantes diﬀérences
entre graphes réels [Newman, 2003b]. Le fait que les sommets de forts degrés soient
plutôt liés entre eux, ou à l’inverse liés de préférence aux sommets de petits degrés
n’est pas universel. Aussi l’étude de la fréquence relative de motifs (sous-graphes
de quelques sommets) permet de distinguer des diﬀérences de comportement entre
graphes. En eﬀet Milo et al. [2004] ont montré qu’il est possible de classer en plusieurs
catégories des graphes réels d’origines variées en observant la fréquence d’apparition
des diﬀérents sous-graphes d’ordre trois ou quatre.

1.4 Conclusion du chapitre

Ce premier chapitre d’introduction nous a permis de replacer l’étude des graphes
de terrain dans un cadre historique et scientiﬁque. Nous avons déﬁni les concepts et
notations qui seront utilisés dans la suite de ce rapport. Notamment les concepts de
base de la théorie des graphes, les notations matricielles et les propriétés structurelles
usuelles. Aussi nous avons présenté les diﬀérentes propriétés non-triviales communes
aux graphes de terrain, à savoir : un fort coeﬃcient de transitivité, une longueur
moyenne des plus courts chemin faible et une distribution des degrés très hétérogène.
La suite de ce rapport se divise en deux parties. Tout d’abord nous nous in-
téressons à la déﬁnition de mesures proximité ou similarités entre sommets d’un

30

CHAPITRE 1. INTRODUCTION

graphe. Nous utilisons ensuite ces mesures pour comparer des graphes, et dans une
application en proposant un système d’assistance à la construction de ressources
lexicales. Dans une seconde partie, nous nous concentrons sur le problème de cluste-
ring de graphe biparti. Ce problème est mis en parallèle avec les méthodes d’analyse
formelle de concepts, puis diﬀérentes méthodes nouvelles sont proposées. Enﬁn une
application de ces méthodes est proposée, elle consiste en un système de classiﬁcation
automatique des résultats d’une recherche d’information.

31

Première partie

Marches aléatoires courtes,

application à diﬀérents problèmes
de l’étude des graphes de terrain

32

33

Chapitre 2
Similarités entre sommets

Ce chapitre poursuit trois objectifs. Tout d’abord une nouvelle mesure de simila-
rité entre sommets est introduite : la conﬂuence 1. Ensuite nous établissons un état
de l’art des diﬀérentes similarités de la littérature. Enﬁn nous proposons, dans une
dernière section, une comparaison expérimentale de ces mesures.

2.1

Introduction

Nous nous intéressons dans ce chapitre aux mesures de similarité, ou mesures de
proximité, entre sommets d’un graphe. Notons que les deux notions de « proches » et
de « similaires » (pour deux sommets) seront confondus dans la suite (et « similarité »
sera préféré à « proximité »). En eﬀet les méthodes présentées ici se comprennent
naturellement comme des mesures de la proximité (entre sommets) mais peuvent
aussi se comprendre (nous allons le voir tout de suite) comme des similarités.

L’approche classique pour déﬁnir une similarité entre objets est de mesurer la
ressemblance de leurs attributs. Deux personnes sont (socialement) « similaires »
si elles ont le même âge, le même sexe, le même travail, etc. Les attributs d’un
sommet d’un graphe sont soit les voisins de ce sommet (c’est-à-dire la topologie du
graphe), soit des données supplémentaires disponibles sur les sommets en dehors
de la topologie du graphe. Dans ce chapitre, nous nous plaçons uniquement dans le
premier cas. C’est-à-dire que nous considérons que les uniques attributs connus d’un
sommet sont ses voisins. Deux sommets sont donc similaires si leurs voisinages, ou
leur « positions » dans le graphe, sont similaires. Nous allons voir que les méthodes
dites « locales » se comprennent naturellement dans ce cadre là (voir section 2.3.1).
Une autre manière d’imaginer et de comprendre les mesures de similarité entre
sommets est de considérer les arêtes d’un graphe comme des indications binaires sur
la similarité des sommets. Caricaturalement, deux sommets adjacents sont similaires,
1. Introduire une nouvelle mesure avant l’état de l’art permet de déﬁnir convenablement les

notations et concepts utilisés ensuite pour décrire concisément les méthodes de l’état de l’art.

34

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

deux sommets non-adjacents ne le sont pas. Évidemment, utilisée telle quelle, cette
information n’est pas suﬃsante 2. Deux sommets non-adjacents peuvent être simi-
laires, au moins jusqu’à un certain degré. Par exemple dans le graphe de synonymie
DicoSyn aucun des trois mots « épée », « couteau » et « poubelle » ne sont adjacents,
pourtant il est possible d’observer que « épée » et « couteau » sont plus proches
que « épée » et « poubelle ». Pour cela, il faut considérer une certaine transitivité :
« épée » et « couteau » partageant un certain nombre de voisins (dague, poignard,
lame, etc.) ce qui n’est pas le cas de « épée » et « poubelle ». Dans cette optique, deux
sommets sont similaires s’il existe de nombreux chemins courts entre eux. Aussi, il
est possible d’observer des paires de sommets peu similaires et pourtant adjacents.
Là encore la topologie du graphe peut aider à « corriger » l’indication de similarité
atomique donnée par chaque arête. L’idée est donc de calculer une similarité entre
les sommets en « lissant » la similarité binaire donnée par les arêtes du graphe. Ce
point de vue permet de comprendre intuitivement la plupart des méthodes dites
« globales » que nous allons présenter en section 2.3.2.

Ces deux approches de la similarité entre sommets dans un graphe, c’est-à-dire
similarité du voisinage et lissage des « liens de similarité » donnés par le graphe, sont
deux points de vue diﬀérents qui ne sont pas forcément antinomiques. Par exemple
mesurer le nombre de voisins communs à deux sommets, méthode qui relève a priori
du premier point de vue, est équivalent à la mesure du nombre de chemins de
longueur deux existant entre deux sommets.

2.2 Proposition d’une mesure normalisée

Avant de présenter dans la section suivante un état de l’art des diﬀérentes mesures
de similarité existant dans la littérature, nous introduisons dans cette section une
nouvelle mesure de similarité entre sommets : la conﬂuence. Cette mesure, basée sur
des marches aléatoires en temps courts, a l’avantage d’être comprise entre 0 et 1 et
d’être normalisée par rapport à un modèle de graphe aléatoire conservant le degré
de chaque sommet. Ainsi montrons que la valeur de la conﬂuence n’est pas sensible
à la densité du graphe ni aux degrés des sommets comparés.

Avant de présenter la conﬂuence (sous-section 2.2.2), nous présentons le mé-
canisme de marches aléatoires dans un graphe (sous-section 2.2.1). Nous verrons
dans la section suivante qu’il existe d’autres mesures de similarité se basant sur
des marches aléatoires semblables. Le principal avantage des marches aléatoires en
temps courts est que leur calcul se fait avec une complexité temporelle raisonnable.
2. Notons que certaines applications peuvent considérer un graphe directement comme une
similarité binaire. C’est par exemple le cas de méthodes de recherche d’information, qui pour
lutter contre la faible densité des données, considèrent en plus des mots de la requête l’ensemble
de leurs synonymes [Manning et al., 2008, section 9.2.2].

2.2. PROPOSITION D’UNE MESURE NORMALISÉE

35

Aussi le résultat d’une marche en temps court peut être comparé à la limite obtenue
pour une même marche mais après un temps inﬁni. Nous allons exploiter cette limite
en introduisant la mesure de conﬂuence.

2.2.1 Marches aléatoires en temps courts

Nous déﬁnissons ici le mécanisme de marches aléatoires dans un graphe. L’idée
est de considérer un marcheur passant aléatoirement, à chaque pas de temps, d’un
sommet à l’autre du graphe en suivant les arêtes.

2.2.1.1 Matrice de transition

Imaginons donc un marcheur présent sur un sommet du graphe et qui se déplace,
à chaque pas de temps, en suivant une arête choisie aléatoirement. Ce processus
correspond à la chaîne de Markov, ou au processus de Markov, associé au graphe.
Cette chaîne de Markov est caractérisée par la matrice de transition P suivante :

( 1

|Γ(u)|
0

si {u, v} ∈ E,
sinon.

(2.1)

P = (pu,v)u,v∈V ,

avec pu,v =

avec Γ(u) l’ensemble des voisins du sommet u. Notons que la déﬁnition matricielle
suivante est équivalente :

(2.2)
où D est une matrice diagonale contenant sur chaque composante (de la diagonale)
le degré du sommet correspondant. Le tableau 2.1 (page 44) récapitule les matrices
usuelles associé à un graphe.

P = D−1A

Remarquons qu’une ligne i de P décrit la probabilité de passer du sommet i vers
chacun des autres sommets du graphe. Si le graphe est non dirigé et ne comporte pas
de sommet isolé (c’est-à-dire chaque sommet possède au moins un voisin), alors P
est une matrice stochastique. En eﬀet il est facile de vériﬁer que la somme de chaque
ligne vaut 1. La probabilité qu’un marcheur passe d’un sommet u à un sommet v en
t pas, que l’on note prxt(u, v), est alors donnée par :

prxt(u, v) =h
P ti

u,v

= δuP tδT

v

(2.3)

avec δi un vecteur ligne constitué de zéros, sauf pour le ième élément qui vaut 1
et (.)T l’opérateur de transposition. Plus généralement, si P0 est la distribution de
probabilité initiale du marcheur sur l’ensemble des sommets (vecteur ligne) alors
P0.P t est la distribution de probabilité décrivant la position du marcheur après
t pas. La matrice P t donne l’ensemble des probabilités de passer d’un des sommets
à un autre en t pas.

36

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Il faut bien souligner que les notations utilisées ici (prxt(u, v), P, etc.) ne pré-
cisent pas le graphe concerné. Ce choix facilite la lecture, car la plupart du temps
il n’y a pas d’ambiguïté quant au graphe utilisé. Aussi nous travaillons avec des
vecteurs lignes (prxt(u, v), P0, etc.) plutôt qu’avec des vecteurs colonnes. Ce n’est
pas forcément le choix fait dans la littérature, nous le préférons pourtant car il évite
de balader des opérateurs de transposition dans les écritures 3.
Proposition 1. Si P est la matrice de transition associée à un graphe G non-dirigé
et ne comportant pas de sommets isolés, on a :

ce qui signiﬁe, pour chaque case (u, v) de D.P t :

D.P t = (D.P t)T

|Γ(u)|.prxt(u, v) = |Γ(v)|.prxt(v, u)

(2.4)

(2.5)

Autrement dit la probabilité pour un marcheur de passer de u vers v multipliée
par le degré de u est égale à la probabilité de passer de v vers u multipliée par le
degré de v.
Démonstration. Comme D est symétrique, il suﬃt de remarquer que DP tD−1 =
(P t)T. En eﬀet on a :

DP tD−1 = D(D−1A)tD−1 = (AD−1)t

De plus, comme le graphe est non-dirigé, A est symétrique, et D−1 l’est aussi car
c’est une matrice diagonale. On a alors :

(AD−1)t = (AT D−1T)t = (P T)t = (P t)T

Graphes pondérés.
Il est possible de prendre en compte une fonction de pondé-
ration w : E → R+ disponible sur les arêtes du graphe. La matrice de transition P
se déﬁnit alors de la manière suivante :

P = (pu,v)u,v∈V ,

avec pu,v =

P



w(u,v)

w∈Γ(u) w(u,w)

0

si {u, v} ∈ E,
sinon.

(2.6)

Le marcheur passe alors sur un voisin avec une probabilité proportionnelle au poids
du lien entre les deux sommets.

3. En eﬀet on trouve souvent dans la littérature exposé que la distribution d’un marcheur après

t pas vaut : (P T )tP0.

2.2. PROPOSITION D’UNE MESURE NORMALISÉE

37

Complexité. Le calcul de P t peut être fait récursivement avec P 0 = I et P t =
P t−1.P. Chaque étape consiste donc en un produit d’une matrice, a priori pleine,
par une matrice creuse. En eﬀet les graphes réels étant peu denses, leurs matrices
d’adjacence (et donc de transition) sont creuses. Ce calcul peut être fait en O(nm).
Le calcul de P t est donc eﬀectué en O(tnm).

Si l’on ne souhaite calculer qu’une seule ligne de P t, alors il faut eﬀectuer t
multiplications d’un vecteur par une matrice creuse. La complexité temporelle est
donc en O(tm).

2.2.1.2 Convergence

Sur un graphe G = (V, E) connexe, non-dirigé, non-périodique la probabilité
qu’un marcheur aléatoire soit en un sommet u après une marche de longueur inﬁnie
vaut :

π(u) =

P
|Γ(u)|
w∈V |Γ(w)|

(2.7)

(2.8)

Cette limite ne dépend donc pas du sommet de départ, et est simplement proportion-
nelle au degré du sommet d’arrivée. La démonstration se fait à l’aide du théorème
de Perron-Frobenius, nous renvoyons pas exemple à [Pons, 2007, section 2.2, page
29] pour une démonstration complète. Lorsqu’une pondération est prise en compte,
il est facile de montrer que cette limite π(u) devient :
w∈Γ(u) w(u, w)

P

π(u) =

P

P

v∈V

w∈Γ(q) w(v, w)

Graphe connexe. Cette limite est déﬁnie sur un graphe connexe, cela peut poser
problème en pratique. Une solution simple consiste à considérer chaque composante
connexe indépendamment. Notons aussi que la plus grande composante connexe
représente souvent une large majorité des sommets ; il est alors possible de se res-
treindre uniquement à cette plus grande composante connexe. Une autre solution
est de toujours considérer le graphe comme connexe pour le calcul de la limite. Les
sommets non-connectés ont alors une limite non nulle, et les sommets appartenant à
la même composante connexe une limite un peu sous-estimée. En pratique, cela ne
change pas grand chose pour des graphes dont la plus grande composante connexe
contient la quasi totalité des arêtes.

Graphe périodique. Un graphe est dit périodique si les longueurs de tous ces
cycles sont divisibles par un entier strictement supérieur à 1. Par exemple, les
graphes bipartis sont des graphes périodiques de période 2 (ce sont les seuls graphes
non-dirigés périodiques). On comprend aisément qu’une marche aléatoire ne peut
converger vers une limite unique sur un graphe périodique. Il faut, pour assurer la

38

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

convergence, que le graphe soit non-périodique (où apériodique). C’est-à-dire que
chaque sommet (de la composante connexe) puisse être atteint depuis n’importe quel
autre en un nombre quelconque de pas (à condition que ce nombre soit suﬃsamment
grand). Pour s’assurer de la non périodicité d’un graphe, une astuce simple consiste
à ajouter une boucle sur chaque sommet. Le graphe est ainsi forcé réﬂexif. Souvent
sur des données réelles aucune information n’est portée par ces liens de réﬂexivité,
car soit aucun des sommets n’est relié à lui même, soit tous le sont. Aussi pour
certaines des mesures basées sur les marches aléatoires courtes, nous transformons
initialement le graphe pour ajouter une boucle sur chaque sommet.

2.2.1.3 Graphe bipartis

Les marches aléatoires fonctionnent de la même manière sur un graphe biparti,
les déﬁnitions données jusque là sont donc toujours valables. Seulement un graphe
biparti est un graphe de période 2 : en un temps pair seuls les sommets du même
groupe que le sommet de départ sont accessibles, à l’inverse en un temps impair seuls
ceux de l’autre type sont accessibles. Utiliser des marches aléatoires sur un graphe
biparti va donc consister à eﬀectuer soit des marches en temps pairs soit des marches
en temps impairs. Un temps pair permet d’obtenir des probabilités d’atteindre les
sommets du même type, alors qu’un temps impair donne des probabilités d’atteindre
les sommets de l’autre type. Le fait important est que les marches en temps pairs,
et impairs convergent chacune, et chacune vers deux limites diﬀérentes.
Proposition 2 (Projection). Une marche aléatoire en temps pair 2t sur un graphe
biparti G = (V = V> ∪ V⊥, E) est équivalente à une marche aléatoire en temps t sur
le graphe projeté pondéré Gproj = (V = V> ∪ V⊥, Eproj, wproj). Avec {u, v} ∈ Eproj
si et seulement si u et v ont (au moins) un voisin en commun dans G, l’arête les
reliant est alors pondérée par :

wproj(u, v) = X

w∈ΓG(u)∩ΓG(v)

1

dG(w)

(2.9)

Démonstration. Comme G est biparti, sa matrice d’adjacence A est anti-diagonale
par bloc. Si l’on range les sommets de V> avant ceux de V⊥, on peut écrire A de la
manière suivante :

(2.10)
où A0 est une matrice rectangulaire de taille |V>| × |V⊥|. La matrice de transition P
associée est donc elle aussi anti-diagonale par bloc :

A =

A0T

0

" 0 A0

#

" 0 P1

#

P2

0

P =

(2.11)

2.2. PROPOSITION D’UNE MESURE NORMALISÉE

39

P1 donne la probabilité de passer d’un sommet de V> vers un sommet de V⊥, et P2
la probabilité de passer d’un sommet de V⊥ vers un sommet de V>. On remarque
alors que la matrice P 2 est diagonale par bloc :

P 2 =

Alors pour tout t ≥ 0, on a :

"

P1.P2

0

0

P2.P1

#

" (P1.P2)t

0

#

0

(P2.P1)t

P 2t = (P 2)t =

Il est alors trivial de voir qu’une marche en temps pair 2t sur G est équivalente à
une marche de temps t sur le graphe dont la matrice de transition est P 2. Montrons
que ce graphe est bien Gproj.

Considérons le graphe donné par la matrice d’adjacence pondérée suivante :

"

#

Aproj = A.D−1.A =

A0.D⊥.A0T

0

0

A0T .D>.A0

Il est facile de vériﬁer que :

ou simplement si A est binaire :

w∈V>∪V⊥

[Aproj]u,v = X
P
[A]u,w [A]w,v
y∈V>∪V⊥ [A]w,y
[Aproj]u,v = X
[Aproj]u,v = X
X

w∈ΓG(u)∩ΓG(v)

[A]u,w

1

dG(w)

v∈V>∪V⊥

w∈V>∪V⊥

(2.12)

(2.13)

(2.14)

(2.15)

(2.16)

(2.17)

(2.18)

Aproj est donc la matrice d’adjacence du graphe Gproj. De plus on peut vériﬁer que :

C’est-à-dire que le degré (ou ici le poids) d’un sommet est le même dans les graphes
projetés (les deux composantes connexes de Gproj) que dans le graphe G de départ.
Ainsi la matrice de transition de Gproj n’est autre que P 2.

Proposition 3 (Convergence). Sur un graphe G biparti et connexe, une marche
aléatoire partant uniquement de sommets de V> converge en temps pair vers 0 pour
les sommets de V⊥ et vers :

π(u) =

P
d(u)
v∈V> d(v)

40

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

pour les sommets de V>. La même marche en temps impair converge vers 0 pour les
sommets de V> et vers :

pour les sommets de V⊥.

π(u) =

P
d(u)
v∈V⊥ d(v)

(2.19)

Pour démontrer cette proposition il faut utiliser les propositions 1 et 2. En eﬀet
les deux composantes du graphe Gproj sont connexes si G est connexe, de plus
Gproj comporte par construction des boucles sur chacun de ces sommets, il est donc
non-périodique. Enﬁn il faut remarquer qu’une marche aléatoire en temps impair
partant de sommets de V> correspond à une marche aléatoire en temps pair partant
des sommets de V⊥.

2.2.2 Mesure de conﬂuence

Nous introduisons ici une mesure que nous nommons la conﬂuence. Cette mesure
se base sur des marches aléatoires en temps courts. L’idée est de normaliser la
probabilité de passer d’un sommet à un autre par la probabilité limite d’atteindre
ce sommet. Deux sommets auront un score de conﬂuence fort si la probabilité de
passer de l’un à l’autre en t pas sur le graphe est forte et si la probabilité d’atteindre
l’un ou l’autre des sommets « à l’inﬁni » est faible. Nous montrons par ailleurs que
cette probabilité limite correspond à la probabilité de passer de l’un à l’autre des
sommets dans un graphe aléatoire équivalent où chaque sommet garde le même
degré.

Nous proposons de normaliser le résultat d’une marche aléatoire en temps court

de la manière suivante :

conﬂt(u, v) =

[P t]u,v

[P t]u,v + π(v)

(2.20)

Par construction la conﬂuence est comprise entre 0 et 1. Dans le cas où [P t]u,v =
π(v), on a conﬂt(u, v) = 0.5. Si la conﬂuence est supérieure à 0.5 cela signiﬁe que
la probabilité d’aller de u vers v en un faible nombre de pas (t) est plus forte que
la probabilité d’arriver en v après un nombre de pas inﬁni. À l’inverse si la valeur
est inférieure à 0.5 alors il est plus probable de se retrouver sur v après une marche
inﬁnie que après quelques pas.

On peut vériﬁer que cette mesure est symétrique. En eﬀet d’après la proposition 1

on a : [P t]u,v = d(v)/d(u). [P t]v,u et on a π(v) = d(v)/d(u).π(u).

Interprétation par rapport à un null model.
Il est possible grâce à la propo-
sition suivante d’interpréter la limite πv et donc la conﬂuence par rapport au null
model 4 utilisé en particulier par la modularité de Newman [2006].

4. Un null model est un graphe aléatoire possédant certaines propriétés d’un graphe donnée.

2.2. PROPOSITION D’UNE MESURE NORMALISÉE

41

Proposition 4. Sur un graphe Gnm = (V, Enm) de même ordre que G et tel que
chaque sommet a le même degré que dans G, alors la probabilité qu’un marcheur
arrive en un sommet v après t pas (t > 0) depuis un sommet u vaut :

πv = d(v)P

i∈V d(i)

(2.21)

Gnm pouvant posséder des arêtes multiples et des boucles.
Démonstration. Le graphe Gnm correspond au modèle de conﬁguration 5 avec une
séquence des degrés imposée par G. Un tel graphe est construit en donnant d(i)
« demi-arêtes » à chaque sommet, et en tirant aléatoirement un couplage entre toutes
ces demi-arêtes. Si l’on note d(i) le degré d’un sommet i et m le nombre total d’arêtes,
on montre que l’espérance du nombre d’arêtes 6 eu,v entre deux sommets u et v vaut :

euv = d(u)d(v)

2m

(2.22)

Eﬀectivement, il y a d(u) arêtes partant de u et chacune a 2m points d’arrivée
possibles, et seulement d(v) de ces points d’arrivée appartiennent à v. Pour une dé-
monstration diﬀérente et plus détaillée de cette équation nous renvoyons à [Newman,
2006, section III].

La probabilité puv qu’un marcheur sur ce graphe passe du sommet u au sommet

v en 1 pas vaut donc :

puv = euv

d(u) = d(v)
2m

= πv

(2.23)

P

car on a bien m = 1
2

i∈V d(i) sur un graphe non dirigé.

Il faut alors remarquer que la probabilité de passer d’un sommet u à un sommet

v en 2 pas est bien la même que celle de passer de u à v en un seul pas :

X

w∈V

pvw.pwu = X
w∈V
= d(u)
2m
= d(u)
2m

d(u)
2m

)

( d(w)
2m
1
2m

.

.

X

w∈V

d(w)

= pvu

(2.24)

(2.25)

(2.26)

On a alors par récurrence que la probabilité d’atteindre un sommet v en temps

quelconque (> 0) vaut πv.

5. Voir par exemple [van der Hofstad, 2013, chap. 7].
6. Rappel : il peut y avoir plusieurs arêtes entre deux sommets.

42

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Mesure entre sommets adjacents. Nous proposons une variante de la conﬂuence
où la similarité entre deux sommets adjacents est calculée en retirant du graphe
l’arête les reliant. Nous nommons CFLtR cette variante. Pour chacune des m arêtes
du graphe il faut t fois multiplier un vecteur par une matrice creuse (de m éléments
non nuls) donc eﬀectuer un calcul en O(mt). Le surcoût est donc en O(m2t).

Graphe périodique et non-connexe. Notons que nous utilisons pour le calcul
de la conﬂuence, sur les graphes non bipartis, l’astuce présentée dans la sous-section
suivante qui consiste à ajouter une boucle sur chacun des sommets. Cela permet
d’éviter les phénomènes de rebond dus à une éventuelle périodicité dans le graphe.
Aussi pour un graphe non-connexe, il est possible soit de se restreindre à la plus
grande composante connexe, soit de calculer la limite comme si le graphe était
connexe (voir sous-section précédente).

2.3 État de l’art des mesures de similarité entre

sommets

Nous présentons ici un panorama aussi complet que possible des mesures de
similarité entre sommets que l’on trouve dans la littérature. Tout d’abord, en sous-
section 2.3.1, les méthodes dites « locales » sont introduites. Ce sont les méthodes ne
prenant en considération que les voisinages directs des sommets comparés. Ensuite,
en sous-section 2.3.2, nous présentons les méthodes « globales », c’est-à-dire prenant
en considération l’ensemble du graphe. Enﬁn en sous-section 2.3.3 nous exposons les
méthodes dites « semi-globales ». Ce sont des méthodes qui, comme la conﬂuence,
sont basées sur des marches aléatoires à temps courts. Elles ne dépendent donc que
d’un voisinage limité de chaque sommet.

Nous présentons ici surtout des méthodes symétriques, et seules les méthodes
symétriques seront évaluées. Notons qu’une mesure non-symétrique peut être in-
téressante dans certains cas. Par exemple, il peut être sensé que la similarité de
Cambodge « vers » Chine soit plus importante que la similarité de Chine « vers »
Cambodge.

Le présent travail est basé sur plusieurs états de l’art existants. Notamment sur
[Lü et Zhou, 2011] qui est, à notre connaissance, la revue de la littérature la plus
récente et complète. Toutefois cette revue est pensée dans le cadre du problème de
prédiction de lien. Notons que nous avons essayé de reprendre autant que possible
les notations introduites dans cet article. Concernant les méthodes à noyaux (voir
plus bas) un très bon état de l’art est proposé par Fouss et al. [2012]. Enﬁn on peut
citer [Liben-Nowell et Kleinberg, 2007] et [Hasan et Zaki, 2011] même si ces deux
travaux sont un peu moins complets.

2.3. ÉTAT DE L’ART DES MESURES DE SIMILARITÉ ENTRE
43
SOMMETS
Rappel des notations. Nous utilisons ici les notations déﬁnies au chapitre pré-
cédent. En particulier rappelons que Γ(v) est l’ensemble des voisins du sommet v, et
d(v) = |Γ(v)| est le degré de v. Nous utilisons aussi les diﬀérentes matrices déﬁnies
en section 1.2.3. Le tableau 2.1 récapitule les déﬁnitions de ces matrices.

2.3.1 Mesures locales : comparaison des voisinages

Une approche simple pour mesurer une similarité entre sommets consiste à com-
parer leurs voisinages. Deux sommets sont similaires s’ils ont les mêmes voisins. Nous
parlons pour ce type d’approche de mesures locales, car seule une connaissance du
voisinage (donc locale) de chacun des sommets concernés est utilisée. Le tableau 2.2
résume les dix similarités locales que nous présentons dans cette sous-section.

Réﬂexivité. Notons que la quasi-totalité de ces mesures fait intervenir l’intersec-
tion des voisinages (Γ(u)∩Γ(v)) et sont donc fortement dépendantes de la réﬂexivité
ou non du graphe. En eﬀet sur un graphe non-réﬂexif les sommets ne sont pas voisins
d’eux mêmes (u 6∈ Γ(u)) et donc même si u et v sont adjacents :

{u, v} 6⊂ Γ(u) ∩ Γ(v)

(2.27)

Alors que sur un graphe réﬂexif, si u et v sont adjacents on a :

{u, v} ⊂ Γ(u) ∩ Γ(v)

(2.28)
Souvent, sur des graphes réels la réﬂexivité ne code pas d’information primor-
diale. Il est possible de forcer le graphe comme réﬂexif (ou non-réﬂexif) sans perdre
d’information. Dans la suite, les graphes sont considérés par défaut comme non-
réﬂexifs. De plus, pour chacune de ces mesures, nous introduisons une variante for-
çant les graphes comme réﬂexifs. Pour une similarité notée « SIM », on note « SIM-l »
cette variante réﬂexive (« l » signiﬁant loops).

Complexité temporelle. Toutes les mesures décrites ci-dessous ont une com-
plexité temporelle moyenne de O(d), où d est le degré moyen. Et donc le calcul de la
matrice de similarité complète est eﬀectué en O(n2d). Seule la mesure d’attachement
préférentiel (AP) a une complexité plus faible, à supposer que l’on connaisse le degré
d’un sommet en O(1) le calcul de la similarité est lui aussi en temps constant (pour
une valeur, donc en O(n2) pour toute la matrice). Nous verrons que ces méthodes
locales sont les plus eﬃcaces en termes de complexité.

Généralisation. Plusieurs des mesures locales que nous déﬁnissons ci-dessous
peuvent se généraliser avec l’écriture suivante :

sim(u, v) = |Γ(u) ∩ Γ(v)|
avg(|Γ(u)|,|Γ(v)|)

(2.29)

44

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Table 2.1 – Récapitulatif des matrices usuelles associées à un graphe

1 si {i, j} ∈ E,
d(i)

si i = j,
sinon.

0 sinon.

A matrice d’adjacence

[A]i,j =

[D]i,j =

D matrice des degrés

Laplacien

0
L = D − A
L
P matrice de transition P = D−1A
L

Laplacien normalisé

L = D−1/2LD−1/2

Table 2.2 – Mesures de similarité locales

CN
JD
DS
CO

AA
RA
HPI
HDI
LHN1
PA

Voisins communs

Jaccard

Dice-Sørensen

cosinus

P
allocation de Ressources P

Adamic/Adar

hub promoted index
hub depressed index
Leich Holme Newman
attachement préférentiel

|Γ(u) ∩ Γ(v)|

|Γ(u)∩Γ(v)|
|Γ(u)∪Γ(v)|
2|Γ(u)∩Γ(v)|
|Γ(u)|+|Γ(v)|
√
|Γ(u)∩Γ(v)|
|Γ(u)|.|Γ(v)|

w∈Γ(u)∩Γ(v)

w∈Γ(u)∩Γ(v)
|Γ(u)∩Γ(v)|

min(|Γ(u)|,|Γ(v)|)

|Γ(u)∩Γ(v)|

max(|Γ(u)|,|Γ(v)|)

|Γ(u)∩Γ(v)|
|Γ(u)|.|Γ(v)|
|Γ(u)|.|Γ(v)|

1

log |Γ(w)|

1

|Γ(w)|

2.3. ÉTAT DE L’ART DES MESURES DE SIMILARITÉ ENTRE
45
SOMMETS
où avg est une fonction de moyenne. En eﬀet pour DS la moyenne arithmétique
est utilisée, la moyenne géométrique pour CO, le minimum pour HPI, et le maxi-
mum pour HDI (minimum et maximum étant les deux moyennes extrêmes). De là,
d’autres mesures sont imaginables, en utilisant par exemple la moyenne quadratique

2(x2 + y2)) ou la moyenne harmonique ( xy

x+y).

(q 1

2.3.1.1 Voisins communs (CN)

Une mesure de similarité simple consiste simplement à compter le nombre de

voisins communs :

(2.30)
Notons que sur un graphe non-dirigé, la matrice des similarités donnée par simCN
s’écrit de la manière suivante :

simCN(u, v) = |Γ(u) ∩ Γ(v)|

SCN(u, v) = A2

(2.31)
En eﬀet le nombre de voisins communs à u et v correspond au nombre de chemins
de longueur deux entre u et v.
Dans le cas d’un graphe dirigé on peut distinguer les voisins entrants et sortants.
Si l’on considère qu’un lien dirigé a → b est un lien de citation (a cite b) alors ces
mesures s’interprètent en termes de citations communes. la mesure de bibliographic
coupling (AAT) indique combien de voisins sont cités à la fois par u et v, alors que
la cocitation coupling (AT A) révèle combien de sommets citent u et v.

2.3.1.2 Jaccard (JD)

La classique mesure de Jaccard peut aussi être utilisée entre voisinages :

Jaccard(u, v) = |Γ(u) ∩ Γ(v)|
|Γ(u) ∪ Γ(v)|

(2.32)

Le nombre de voisins communs est normalisé par le nombre de voisins de l’un
ou de l’autre des sommets. Cette mesure peut être interprétée comme la probabilité
qu’un voisin de l’un ou l’autre soit voisin des deux sommets.

2.3.1.3 Index de Dice-Sørensen (DS)

La mesure de Dice [1945] ou de Sørensen [1948] 7 est une autre mesure classique
de similarité entre vecteurs binaires. Elle peut s’appliquer entre les voisinages de
deux sommets :

Dice(u, v) = 2|Γ(u) ∩ Γ(v)|
|Γ(u)| + |Γ(v)|

(2.33)

7. Cette mesure a été proposée indépendamment par ces deux botanistes, elle est donc notée

Dice, Sørensen ou parfois Dice-Sørensen dans la littérature.

46

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Notons que cette mesure s’obtient en ajoutant |Γ(u) ∩ Γ(v)| au numérateur et au
dénominateur de la mesure de Jaccard.

2.3.1.4 Cosinus (CO)

Le cosinus est une mesure de similarité entre vecteurs maintenant classique. On
l’appelle parfois Salton index, ou Salton’s cosine car cette mesure a été introduite,
dans le champ de la recherche d’information, comme mesure de similarité entre
vecteurs par Salton et al. [1975a]. Cette mesure est déﬁnie en général pour des
vecteurs réels, mais s’adapte facilement pour des vecteurs binaires. Le cosinus entre
les vecteurs d’adjacence de deux sommets peut s’écrire de la manière suivante :

q|Γ(u)|.|Γ(v)|
cosine(u, v) = |Γ(u) ∩ Γ(v)|

(2.34)

2.3.1.5 Adamic/Adar (AA)

des mesures proposées jusque là :

La mesure proposée par Adamic et Adar [2003] prend une forme un peu diﬀérente

simAA(u, v) = X

z∈Γ(u)∩Γ(v)

1

log |Γ(z)|

(2.35)

Mais elle peut aussi être comprise comme une pondération du nombre de voisins
communs. L’idée étant que plus un voisin commun est « grand » moins il contribue
à la similarité. Deux sommets vont être d’autant plus proches que leurs voisins
communs sont de faibles degrés. Notons que cette mesure a été déﬁnie pour mesurer
la similarité entre pages web.

2.3.1.6 Index d’allocation de ressources (RA)

Une mesure similaire a été introduite par Zhou et al. [2009] :

simRA(u, v) = X

z∈Γ(u)∩Γ(v)

1

|Γ(z)|

(2.36)

C’est une variante de Adamic/Adar sans l’amortissement du degré par le logarithme.
Les voisins communs de forts degrés sont donc plus fortement pénalisés.

Notons que cette mesure est équivalente à une marche aléatoire de temps 2 mul-
tipliée par le degré de départ (notée PRX2_deg par la suite, voir sous-section 2.3.3.3).

2.3. ÉTAT DE L’ART DES MESURES DE SIMILARITÉ ENTRE
SOMMETS
2.3.1.7 Hub Promoted Index (HPI)

47

Les deux mesures HPI et HDI ont été introduites par [Ravasz et al., 2002] pour

mesurer la similarité entre sommets d’un réseau métabolique.

simHP I(u, v) = |Γ(u) ∩ Γ(v)|
min(|Γ(u)|,|Γ(v)|)

(2.37)
Cela correspond donc à une normalisation du nombre de voisins communs par le
nombre de voisins du « plus petit » des sommets. Les hub ont donc tendance à avoir
des scores élevés, en particulier avec des petits sommets.

2.3.1.8 Hub Depressed Index (HDI)

|Γ(u) ∩ Γ(v)|

simHDI(u, v) =

(2.38)
C’est la mesure inverse de la précédente. Cette fois, le plus « gros » des sommets
est utilisé pour la normalisation. Les hubs ont donc tendance à avoir des scores plus
faibles.

max(|Γ(u)|,|Γ(v)|)

2.3.1.9 Leicht Holme Newman Index (LHN1)

La mesure LHN1 [Leicht et al., 2006] propose une autre normalisation du nombre

de voisins communs :

simLHN1(u, v) = |Γ(u) ∩ Γ(v)|
|Γ(u)|.|Γ(v)|

(2.39)
L’idée est de comparer le nombre de voisins communs par rapport au nombre « at-
tendu ». En eﬀet, à une constante multiplicative près, le produit |Γ(u)|.|Γ(v)| cor-
respond au nombre espéré de voisins communs à u et à v, sur un graphe aléatoire
dans lequel chaque sommet garde le même degré que sur G. Cette similarité est donc
normalisée par rapport au même null model que la conﬂuence (voir section 2.2.2).

2.3.1.10 Attachement préférentiel (PA)

L’idée de l’attachement préférentiel est que la probabilité qu’un lien se crée est
proportionnelle aux degrés des extrémités. Les liens se forment préférentiellement sur
les plus forts sommets. Cette idée est surtout utilisée dans le cadre de la modélisation
de graphe mais il en découle une mesure de similarité triviale :

simP A(u, v) = |Γ(u)|.|Γ(v)|

(2.40)
Cette similarité n’utilise que le degré de chacun des sommets, elle sera donc peu
informative (par contre sa complexité est très faible). Aussi cette mesure simple va
nous permettre, par comparaison, de mieux comprendre le comportement d’autres
similarités.

48

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Table 2.3 – Mesures de similarité globales

SimRank

mesure de Blondel et al.

exponential diﬀusion kernel

Laplacian exp. diﬀusion kernel

plus court chemin (dist. géodésique)

index de Katz
local path index

GEO
SRK
BLD
KTZ
LP
EDK
LEDK
NLEDK normalized Laplacian exp. diﬀusion kernel
CTRW
ACT
CTK
RCTK
RLK
PPR

average commute time
commute time kernel

continue time random walk

regularized compute time kernel

regularized Laplacian kernel

PageRank personnalisé

1

1+lu,v

cf. Eq. (2.43)
cf. Eq. (2.45)
(I − αA)−1 − I

A2 + αA3
exp(αA)
exp(−αL)
exp(−αL)

exp(−α(P − I))
cf. Eq. (2.58)

L+

(D − αA)−1
(I + αL)−1

(1 − d)(I − dP)−1

2.3.2 Mesures globales : noyaux, chemins et marches aléa-

toires

Nous présentons ici des mesures dites « globales », c’est-à-dire basées plus seule-
ment sur les voisinages directs des sommets mais sur le graphe complet. L’avantage
évident de ces mesures globales est leur capacité à donner une similarité non systé-
matiquement nulle à une paire de sommets n’ayant aucun voisin en commun. Ces
paires, non adjacentes et non liées par un chemin de longueur 2, ne sont en eﬀet pas
forcément distantes de la même manière. Sur un graphe creux, elles peuvent être en
nombre important, il est alors intéressant de pouvoir les discriminer.

On peut penser que ces mesures amélioreront aussi la qualité de la similarité
entre sommets ayant des voisins en commun. Par exemple une paire de sommets
reliés par 5 chemins de longueur 2 mais aucun de longueur 3 sera certainement, avec
une mesure locale, plus proche qu’une seconde paire reliée par seulement 4 chemins
de longueur 2 et 10 de longueur 3. Cela peut s’inverser avec une mesure globale,
puisque les chemins de longueur 3 seront pris en compte.

Le tableau 2.3 résume les mesures présentées dans cette sous-section.

2.3. ÉTAT DE L’ART DES MESURES DE SIMILARITÉ ENTRE
49
SOMMETS
Noyaux entre les sommets. Beaucoup de ces similarités globales déﬁnissent des
noyaux (kernel) entre les sommets du graphe [Fouss et al., 2006, 2012; Kunegis et
Lommatzsch, 2009]. Un noyau k : V ×V → R est une fonction telle que, pour chaque
couple de sommets u, v on ait :

k(u, v) = uT .v

(2.41)

avec u et v des représentations vectorielles implicites de u et v. La matrice d’un
noyau est la matrice K telle que [K]u,v = k(u, v). Notons qu’une matrice de produits
scalaires, telle que K, est parfois appelée matrice de Gram.

Les noyaux sont particulièrement utilisés en apprentissage automatique [Mohri
et al., 2012, chap. 5]. Ils permettent en eﬀet d’appliquer tout algorithme ne dépen-
dant que du produit scalaire (SVM, kernel PCA, etc.) sur les données transformées de
manière non-linéaire, et cela sans avoir à expliciter cette représentation non-linéaire
des données.

2.3.2.1 Distance géodésique (GEO)

La mesure globale la plus classique sur un graphe est la distance géodésique.
C’est-à-dire la longueur du plus court chemin existant entre deux sommets (voir
section 1.2.7.2). Cette mesure n’est pas une similarité mais une distance, pour avoir
des valeurs comparables avec les autres mesures nous prenons donc son inverse :

simGEO(u, v) =

1

1 + lu,v

(2.42)

Où lu,v est la longueur d’un plus court chemin entre u et v. Cette formule permet
d’avoir une valeur de similarité comprise entre 0 et 1. De plus simGEO vaut 0.5 pour
une paire de sommets adjacents, et 1 pour la similarité d’un sommet avec lui même.
Pour une paire de sommets déconnectés 8, on peut considérer que lu,v est inﬁni alors
simGEO vaut 0.

Notons que la principale faiblesse de cette mesure est qu’elle est peu discrimi-
nante, en particulier sur les graphes réels. En eﬀet la distance géodésique ne prend
pas en compte le nombre de chemins (courts) existant entre deux sommets. Cela
ne pose pas de problème sur un graphe artiﬁciel quand ce nombre de chemins est
à peu près constant. Mais sur les graphes réels, nous avons vu que le phénomène
de « petit monde » (section 1.3.1) fait qu’il existe toujours un chemin court entre
deux sommets. Ainsi énormément de paires de sommets ont une distance géodésique
identique, alors qu’il est possible de les discriminer plus ﬁnement en considérant le
nombre de chemins existants.

8. Notons que nous nous limitons dans la suite de ce chapitre à des graphes connexes.

50

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

2.3.2.2 SimRank (SRK)

La mesure SimRank [Jeh et Widom, 2002] se déﬁnit de manière récursive : la
similarité d’un sommet avec lui même vaut 1, et celle de deux sommets distincts est
la moyenne des similarités de leurs voisins :

1

simrank(u, v) =

P

a∈Γ(u)

P

b∈Γ(v)

simrank(a, b)

si u = v,
sinon.

(2.43)

γ

|Γ(u)||Γ(v)|

avec γ ∈ [0, 1]. Il est possible de calculer itérativement SimRank. Si l’on note Sk la
matrice des similarités après k itérations, et P la matrice de transition, on a :

S0 = I
S0
k+1 = γP SkP T
i

1
h

S0
k+1

[Sk+1]ij =

ij

∀i,∀j,

i = j,

si
sinon.

et donc :

SSRK = lim

k→∞ Sk

(2.44)

La démonstration de la convergence est faite par [Jeh et Widom, 2002].

Notons que cette similarité peut être interprétée en termes de marches aléatoires :
simrank(u, v) est l’espérance de γl, où l est la variable aléatoire donnant le temps
auquel deux marcheurs aléatoires partant de u et de v se retrouvent pour la première
fois sur un même sommet.

2.3.2.3 Mesure de Blondel et al. (BLD)

La mesure de similarité de Blondel et al. [2004] permet de mesurer la similarité
entre les sommets de deux graphes. Nous l’utilisons ici entre un graphe et lui-même.
Cette mesure se calcule alors itérativement de la manière suivante 9 :

Sk+1 = ASkAT + AT SkA
||ASkAT + AT SkA||F

, S0 = 1

(2.45)

On a :

SBLD = lim

(2.46)
Cette mesure, dans son écriture matricielle, est semblable à SimRank. Toutefois
c’est la matrice d’adjacence A qui est utilisée ici et non la matrice de transition P.
De plus SimRank force la similarité d’un sommet avec lui même à 1, ce qui n’est
pas le cas ici.

k→∞ Sk

9. ||.||F est la norme de Frobenius : ||M||F =qP

P

ij.
j m2

i

2.3. ÉTAT DE L’ART DES MESURES DE SIMILARITÉ ENTRE
SOMMETS
2.3.2.4 Similarité de Katz (KTZ)

51

La similarité dite « de Katz », ou index de Katz, a été introduite en 1953 [Katz,
1953] aﬁn de mesurer « l’importance » (i.e. la centralité) des sommets dans un graphe
social et non directement pour mesurer une similarité entre sommets. La mesure de
centralité déﬁnie par Katz est la somme de la similarité du sommet « cible » avec
tous les autres. Nous nous intéressons ici uniquement à la mesure de similarité ainsi
indirectement déﬁnie.

Cette mesure se base sur l’ensemble des chemins existant entre deux sommets,

elle est déﬁnie ainsi :

ktz(u, v) =

∞X

k=1

αk.|paths<k>(u, v)|

(2.47)

Avec paths<k>(u, v) l’ensemble des chemins de longueur k entre u et v et α un
scalaire (0 < α < 1). ktz(u, v) est donc la somme pondérée du nombre de chemins
existant entre u et v. Le nombre de chemins de longueur k étant pondéré par αk.
Si SKT Z est la matrice des similarités de Katz ([SKT Z]u,v = ktz(u, v)), et A est

la matrice d’adjacence alors on a :

∞X

k=1

αkAk

(2.48)

SKT Z =

On voit que Kkatz est, presque, la somme S d’une suite géométrique de raison

αA (presque car la somme commence à 1, non à 0). On a donc :

(2.49)
à condition que α soit plus petit que λ−1, λ étant la plus grande des valeurs propres
de A (pour que I − αA soit inversible).

Skatz = (I − αA)−1 − I

Notons que cette mesure est quasiment équivalente au noyau de Neumann (Neu-
mann kernel ou von Neumann diﬀusion kernel). En eﬀet ce noyau se déﬁnit de la
manière suivante :

SN K =

αkAk = (I − αA)−1 = SKT Z − I

(2.50)

∞X

k=0

Pour plus de détails sur le noyau de Neumann, nous renvoyons à [Kandola et al.,
2003] et à [Ito et al., 2005].

2.3.2.5 Local path index (LP)

Le local path index a été introduit par [Zhou et al., 2009]. Cette mesure se déﬁnit

ainsi :

SLP = A2 + βA3

(2.51)

52

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

C’est une approximation de la similarité Katz ne considérant que les chemins de
longueur 2 et 3. Cette approche a l’avantage d’être semi-globale 10 tout en ayant une
complexité raisonnable. En eﬀet si le graphe est peu dense, A est une matrice creuse
et donc SLP se calcule en O(nm) (m le nombre d’arêtes, et n de sommets). En eﬀet
la multiplication de A par A se fait en O(nm), celle de A2 par A se fait aussi en
O(nm), et la somme de A2 et de A3 se fait au pire en O(n2).

2.3.2.6 Exponential diﬀusion kernel (EDK)

Le noyau de diﬀusion exponentiel (ou exponential diﬀusion kernel) a été introduit

par [Kondor et Laﬀerty, 2002]. Il est déﬁni de la manière suivante :

∞X

k=0

SEDK =

αk
k! Ak = exp(αA)

(2.52)

Cette mesure est donc semblable à celle de Katz (ou noyau de Neumann), à la
diﬀérence que les chemins de longueurs k sont ici pondérés par αk
k! . L’atténuation de
l’importance d’un chemin en fonction de sa longueur est donc plus forte.

2.3.2.7 Laplacian exponential diﬀusion kernel (LEDKα)

Le Laplacian exponential diﬀusion kernel [Kondor et Laﬀerty, 2002] est une al-
ternative au Exponential diﬀusion kernel. La matrice d’adjacence est remplacée par
l’opposé du Laplacien du graphe :

SLEDKα = exp(−αL)

(2.53)
Les valeurs de SLEDKα peuvent être interprétées en termes de diﬀusion sur le
graphe. On note xu(t) la valeur en u à l’instant t. On imagine alors le phénomène
de diﬀusion suivant sur le graphe : pendant un intervalle de temps δt une quantité
[A]u,v xu(t)δt est transférée de u vers v. C’est-à-dire, si u et v sont voisins, une
quantité proportionnelle à xu(t) et à δt est passée de u vers v. On peut alors montrer
(voir [Fouss et al., 2012, section 4.2]) que [SLEDKα]u,v est la quantité présente en v,
après un temps α, étant donnée une distribution initiale intégralement en u.

2.3.2.8 Normalized Laplacian exponential diﬀusion kernel (NLEDK)

Le normalized Laplacian exponential diﬀusion kernel est une variante du noyau
LEDK. Simplement le Laplacien normalisé L est utilisé en lieu et place du Laplacien :
(2.54)
10. Cette mesure est bien semi-globale, nous avons préféré l’introduire ici plutôt que dans la
section suivante (c’est-à-dire la section 2.3.3, qui regroupe les méthodes semi-globales) car cette
méthode n’est pas basée sur les marches aléatoires en temps courts, et est une approximation
de KTZ.

SN LEDK = exp(−αL)

2.3. ÉTAT DE L’ART DES MESURES DE SIMILARITÉ ENTRE
SOMMETS
2.3.2.9 Marche aléatoire en temps continu (CTRW)

53

Les marches aléatoires en temps continu décrivent le phénomène de diﬀusion
suivant : pour chaque paire de sommets i, j, pendant un intervalle de temps δt une
quantité [P]i,j xi(t)δt est transférée de i vers j. Le résultat d’un tel phénomène de
diﬀusion pour un temps α est donné par :

SCT RW = exp(−α(P − I))

(2.55)
Pour une démonstration de cette écriture, voir par exemple [Pons, 2007, section 2.5].
Ce phénomène de diﬀusion est semblable à celui à l’œuvre avec le Laplacian
exponential diﬀusion kernel. La diﬀérence avec LEDK est qu’ici la quantité transférée
de u à v dépend du degré de u. Pour comprendre cette diﬀérence, prenons deux
sommets adjacents a et b. Imaginons qu’à un temps t donné la valeur en a et en b
est la même. Sur un intervalle de temps δt, dans le premier cas (LEDK), la même
quantité va transiter de a vers b et de b vers a. Alors que dans le second cas (CTRW),
la quantité qui transite de a vers b est proportionnelle à 1/d(a) alors que la quantité
qui transite de b vers a est proportionnelle à 1/d(b). Un « gros » sommet va donc
« recevoir » plus qu’il ne « donne ». A l’équilibre (t → ∞) la quantité présente sur
les gros sommets sera plus importante que sur les petits sommets (on montre en
réalité que l’état stable est le même que pour une marche aléatoire classique en
temps discret). Ce qui n’est pas le cas avec le premier mécanisme, où à l’équilibre
tous les sommets ont la même valeur.

Notons que LEDK peut aussi être compris en termes de marches aléatoires. Eﬀec-
tivement, cela correspond à une marche aléatoire où un marcheur sur un sommet a,
va passer sur un sommet voisin b avec une probabilité 1/dmax et rester sur le même
sommet avec une probabilité dmax − d(a), où dmax est le plus fort des degrés des
sommets du graphe. Ce mécanisme est parfois nommé lazy random walk [Kondor et
Laﬀerty, 2002, section 3.3].

2.3.2.10 Average commute time (ACT)

La distance dite « average commute time » entre u et v correspond au temps
moyen que met un marcheur aléatoire pour aller de u à v puis de v à u. On parle aussi
de resistance distance car cette mesure est proportionnelle à la résistance électrique
mesurable entre les sommets u et v, si chaque arête du graphe est une résistance (de
valeur constante pour un graphe non-pondéré) [Chandra et al., 1996].

On peut montrer [Fouss et al., 2007] que cette distance est calculable à partir de

L+ le pseudo-inverse du Laplacien du graphe :

(2.56)

(2.57)

dACT(u, v) =h
L+i

+h

L+i

− 2h

L+i

u,u

v,v

u,v

L+ pouvant être calculé de la manière suivante :

L+ = (L − 1/n)−1 + 1/n

54

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

où L est le Laplacien du graphe (L = D − A).

Aussi comme dACT est une distance, et aﬁn d’avoir une mesure comparable avec

les autres, nous prenons son inverse :

simACT(u, v) =

1

1 + dACT(u, v)

(2.58)

Notons que von Luxburg et al. [2010] ont montré que cette distance a peu d’in-
térêt sur des graphes de grande taille. Sous certaines conditions, la commute time
distance entre deux sommets u et v peut être approximée par :
d(v). Nous
allons voir que cela se conﬁrme dans notre étude expérimentale (voir section 2.4) où
ACT apparaît fortement corrélée à PA (pour rappel simP A(u, v) = d(u).d(v)).

d(u) + 1
1

2.3.2.11 Commute time kernel (CTK)

Il est aussi possible d’utiliser directement la matrice L+ comme mesure de simi-
larité. On nomme cette similarité commute time kernel. Il est possible de la calculer
de la manière suivante [Fouss et al., 2007] :

SCT K = L+ = (L − 1/n)−1 + 1/n

(2.59)

On peut montrer que [SCT K]u,v est le produit scalaire entre des représentations
de u et v dans un espace où chaque paire de sommets est séparée d’une distance
correspondant à la valeur de la distance « average commute time » (dACT), voir [Fouss
et al., 2007].

2.3.2.12 Regularized Laplacian kernel (RLK)

Un alternative à l’utilisation du pseudo-inverse du Laplacien est d’utiliser un

opérateur de régularisation sur le Laplacien :

SRLK =

αk(−L)k = (I + αL)−1

(2.60)

∞X

k=0

avec α > 0. On parle de regularized Laplacian Kernel [Ito et al., 2005].

Cette mesure a la même forme que le noyau de Neumann (ou mesure de Katz),
mais ici la matrice d’adjacence est remplacée par le Laplacien. Notons aussi que pour
un α suﬃsamment grand, cette mesure est semblable au commute time kernel (CTK)
à un facteur multiplicatif et à une constante près [Fouss et al., 2012, section 4.5].

On réfère parfois à cette mesure par le terme matrix forest index [Chebotarev et
Shamis, 1997]. En eﬀet on peut l’interpréter en termes de forêts recouvrantes. Une
forêt recouvrante (spanning forest) de G est un sous-graphe acyclique de G, qui a
les mêmes sommets que G, et comportant un sommet racine dans chacune de ses
composantes connexes. C’est une union d’arbres recouvrant G. Il a été montré par

2.3. ÉTAT DE L’ART DES MESURES DE SIMILARITÉ ENTRE
55
SOMMETS
[Chebotarev et Shamis, 1997] que (I − L)−1 est bien déﬁni, et que [(I − L)−1]i,j est
le ratio entre le nombre de forêts recouvrantes où i et j sont dans le même arbre
de racine i et le nombre total de forêts recouvrantes de G. [(I − L)−1]i,j est donc la
probabilité que i et j soient dans le même arbre dont i est racine, parmi l’ensemble
des forêts recouvrantes de G. Lorsque le graphe est pondéré, cette interprétation
doit tenir compte des poids, nous renvoyons pour plus de détails à [Chebotarev et
Shamis, 1997]. Aussi cette interprétation se généralise avec (I − αL)−1, les forêts
dont les arbres ont k arêtes sont alors pondérées avec un poids de αk [Chebotarev et
Shamis, 1998, théorème 2]. Ainsi la mesure est renforcée si i et j sont dans la même
forêt de « petits arbres ».

2.3.2.13 Regularized commute-time kernel (RCTK)

Une autre variante au commute time kernel (CTK) ou au regularized Laplacian

kernel (RLK) est le regularized commute-time kernel :
SRCT K = (D − αA)−1

(2.61)

pour 0 < α < 1. Ce noyau a été proposé par Fouss et al. [2006]. Nous allons voir
que cette mesure correspond à la mesure de PageRank personnalisé (PPR) divisée
par le degré du sommet d’arrivée.

2.3.2.14 PageRank personnalisé (PPR et PPR-avg)

Le PageRank personnalisé (personalized PageRank) est une adaptation de l’al-
gorithme classique PageRank [Brin et Page, 1998]. C’est le résultat d’une marche
aléatoire où à chaque pas le marcheur passe sur l’un des sommets voisins avec une
probabilité d et retourne sur un sommet initial avec une probabilité 1 − d.

Le PageRank personnalisé entre u et v est donc la probabilité qu’un tel marcheur
(revenant « un peu » à chaque pas sur u) soit sur v après un nombre inﬁni de pas.
Pour un sommet de départ u, si pu(t) est le vecteur de distribution de probabilité
du marcheur au temps t on a :

pu(t + 1) = (1 − d)δu + dpu(t)P

(2.62)

où δu est la certitude d’être en u, c’est-à-dire un vecteur n’ayant qu’une valeur non
nulle égale à 1 sur la uème composante. La valeur de ce vecteur converge quand t tend
vers l’inﬁni, et donne une mesure de similarité entre u et les sommets du graphe.

Il est possible d’exprimer matriciellement cette mesure. Soit SP P R une matrice

carrée telle que sa uème ligne vaille limt→∞ pu(t), on a :
SP P R = (1 − d)(I − dP)−1

(2.63)

56

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

[SP P R]u,v est donc la probabilité d’être en v après un nombre inﬁni de pas telss qu’à
chaque pas le marcheur retourne en u avec une probabilité 1 − d.
Cette mesure peut aussi s’écrire comme une somme inﬁnie :

SP P R = (1 − d)

dkP k

(2.64)

∞X

k=0

Cette écriture est intéressante car elle permet de comparer PPR aux mesures semi-
globales basées sur des marches aléatoires (comme déﬁni en section 2.2 et dans la
sous-section 2.3.3). En eﬀet on observe que SP P R est la somme, sur le temps t, du
résultat d’une marche en temps t pondéré par un facteur (1 − d)dt. Chaque marche
aléatoire de longueur i ayant un poids (1 − d)dt, la longueur moyenne des marches
1−d. Notons que la valeur de d = 0.8 souvent

utilisées vaut donc P∞

k=0(1 − d)dkk = d

utilisée correspond donc à des marches de longueur moyenne t = 4.

Il faut noter que SP P R n’est pas symétrique. Comme nous l’avons déjà remarqué
cela n’est pas forcément un défaut, toutefois aﬁn d’avoir une comparaison cohérente
nous utiliserons la mesure moyenne suivante :

SP P R_avg = 1

2(SP P R + ST

P P R)

(2.65)

On peut remarquer le lien entre le PageRank personnalisé et la mesure regularized

compute time kernel (RCTK). En rappelant que P = D−1 on a :

SP P R = (1 − d)(D−1(D − dA))−1

(2.66)
(2.67)
(2.68)
On a donc (1− d)SRCT K = SP P RD−1. La mesure [SRCT K]u,v correspond donc, à une
constante multiplicative près, au PageRank personnalisé de u vers v divisé par le
degré de v.

= (1 − d)(D − dA)−1D
= (1 − d)SRCT KD

2.3.3 Mesures « semi-globales »

On appelle mesures « semi-globales » les similarités qui utilisent un voisinage
plus large que les mesures locales sans forcément faire intervenir tout le graphe. Ces
approches se basent en général, comme la conﬂuence, sur des marches aléatoires en
temps courts. Cette sous-section présente d’autres méthodes que la conﬂuence en
s’appuyant sur les notations introduites en sous-section 2.2.1. Le tableau 2.4 résume
ces méthodes.

Notons qu’elles peuvent être classées en deux familles : soit la similarité entre
u et v est seulement liée à la probabilité d’aller de u vers v ou de v vers u, soit
elle prend en compte la probabilité d’atteindre depuis u (et depuis v) chacun des

2.3. ÉTAT DE L’ART DES MESURES DE SIMILARITÉ ENTRE
57
SOMMETS
sommets du graphe. Soit u et v sont similaires car il est probable d’aller de l’un à
l’autre en peu de pas, soit u et v sont similaires car on atteint les mêmes sommets
depuis l’un ou l’autre. Les méthodes PRXt, PRXt-avg, PRXt-deg, PRXt-dege, SRWt ainsi
que la conﬂuence (CFLt) relèvent de la première famille, les autres de la seconde.

Pour les méthodes de la seconde famille, on considère que u et v sont similaires
s’il est possible d’atteindre les mêmes sommets avec la même probabilité depuis u ou
depuis v. Pour cela chaque sommet u est représenté par la distribution de probabilité
d’un marcheur après t pas depuis u. C’est-à-dire la ligne u de la matrice P t. La
similarité entre u et v est alors donnée par une mesure entre les deux vecteurs lignes
correspondants extraits de P t. Toutes les méthodes de similarité entre vecteurs, et
en particulier entre distributions de probabilité, sont utilisables ici. Pour un état
de l’art complet des méthodes envisageables nous renvoyons à [Cha, 2007]. Nous
présentons ici seulement les méthodes que nous avons évaluées.

Table 2.4 – Mesures de similarité semi-globales utilisant des marches aléatoires à
temps courts

PRXt
PRXt_avg
PRXt_deg
PRXt_dege
SRWt
PRXt_dot
PRXt_cos
MDKt
CFLt

mesure directe

moyenne

× degré de départ
/ degré d’arrivée

Superposed Random Walk

produit scalaire

cosinus

Markov diﬀusion kernel

P t

1
2(P t + P tT )

D.P t
P t.D−1

Pt

l=1 D.P l
P t.P tT

cf. Eq. (2.81)

Pt

ZtZT

t avec Zt = 1

t

l=0 P l

conﬂuence

cf. Eq. (2.20)

2.3.3.1 Mesure directe (PRXt)

La manière la plus simple pour construire une mesure de similarité à partir de
marches aléatoires en temps courts est d’utiliser directement la probabilité d’aller
de u à v :

(2.69)
Il faut bien remarquer que cette mesure n’est pas symétrique. C’est pourquoi

u,v

simP RXt(u, v) =h

P ti

nous ne l’inclurons pas dans les évaluations.

58

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

2.3.3.2 Moyenne (PRXt_avg)

Une méthode simple pour rendre la mesure symétrique est d’utiliser la moyenne :

simP RXt_avg(u, v) = 1

P ti

2(h

u,v

+h
P ti

ce qui donne en écriture matricielle :

SP RXt_avg = 1

2(P t + P tT)

Notons que la même symétrisation est utilisée avec le average commute time

(ACT).

2.3.3.3 Multiplié par le degré de départ (PRXt_deg)

Nous avons remarqué que la probabilité d’aller de a à b multipliée par le degré
de a est égale à la probabilité d’aller de b à a multipliée par le degré de b (voir
proposition 1, Eq. (2.5)). Il suﬃt donc de multiplier [P t]u,v par le degré de u pour
avoir une mesure symétrique :

)

v,u

(2.70)

(2.71)

h
P ti

u,v

simP RXt_deg(u, v) = |Γ(u)|.

et donc matriciellement :

SP RXt_deg = D.P t

Notons que Liu et Lü [2010] proposent la mesure suivante :

simLRW t(u, v) = |Γ(u)|
2|E|

+ |Γ(v)|
2|E|

u,v

h

P ti

h

P ti

v,u

(2.72)

(2.73)

(2.74)

Or étant donnée la remarque faite précédemment, cette mesure est équivalente à
simP RX_deg (à une division par |E| près). Bien entendu cela n’est plus vrai sur un
graphe dirigé.

2.3.3.4 Divisé par le degré d’arrivée (PRXt_dege)

Une autre symétrisation similaire est possible en divisant cette fois la mesure par

le degré d’arrivée :

simP RXt_dege(u, v) =

[P t]u,v
|Γ(v)|

(2.75)

et donc matriciellement :

SP RXt_dege = P t.D−1

(2.76)
Nous adoptons la notation discutable « PRXt_dege », le « e » signiﬁant end. No-
tons que cette transformation est la même que celle qui permet de passer du perso-
nalized PageRank (PPR) au regularized compute time kernel (RCTK).

2.3. ÉTAT DE L’ART DES MESURES DE SIMILARITÉ ENTRE
SOMMETS
2.3.3.5 Somme des temps courts (SRWt)

59

Liu et Lü [2010] proposent d’utiliser la somme des marches aléatoires de longueur

variant entre 1 et t, multipliée par le degré de départ :

simSRW t(u, v) =

simP RXl_deg(u, v) = |Γ(u)|

SRW signiﬁe Superposed Random Walk. Matriciellement cela donne :

h

P li

tX

l=1

u,v

tX

l=1

SSRW t(u, v) =

2.3.3.6 Produit scalaire (PRXt_dot)

tX

l=1

D.P l

(2.77)

(2.78)

(2.79)

(2.80)

(2.81)

Les méthodes vues jusqu’ici utilisent seulement [P t]u,v et [P t]v,u. Nous introdui-
sons maintenant plusieurs méthodes adoptant un point de vue diﬀérent. Les sommets
sont comparés en utilisant les lignes correspondantes dans P t, on note [P t]u,∗ la ligne
correspondant au sommet u.

Une méthode simple pour comparer deux lignes de P t consiste à eﬀectuer un

produit scalaire :

simP RXt_dot(u, v) =h

P ti

h
P ti

T

v,∗

u,∗ .

[P t]i,∗ étant la ligne i de la matrice P t. Ce qui donne en écriture matricielle :

SP RXt_dot = P t.P tT

2.3.3.7 Cosinus (PRXt_cos)

Une autre comparaison classique repose sur le cosinus :
[P t]u,∗ .[P t]v,∗T

simP RXt_cos(u, v) =

|| [P t]u,∗ ||.|| [P t]v,∗ ||

où ||.|| est la norme 2.

2.3.3.8 Markov diﬀusion kernel (MDK)

Enﬁn la mesure appelée Markov diﬀusion kernel a été introduite par Fouss et al.

[2006, 2012] :

SM DK = ZtZ T

t

avec Zt = 1

t

P l

(2.82)

Ici chaque sommet est donc représenté par la moyenne des résultats des marches
aléatoires de longueurs variant de 0 à t. Et la similarité entre deux sommets est
calculée par un simple produit scalaire entre ces représentations. MDK est presque
le produit scalaire entre les mesures de SRW, à la diﬀérence que les marches ne sont
pas ici multipliées par le degré de départ.

tX

l=0

60

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

2.4 Comparaison expérimentale

Nous proposons dans cette section une comparaison expérimentale des mesures
de similarité présentées dans les sections précédentes. L’idée est d’observer la corré-
lation entre les valeurs de ces mesures sur diﬀérentes paires de sommets provenant
de plusieurs graphes. Nous proposons ainsi plusieurs regroupements hiérarchiques
de ces similarités basés sur la corrélation entre les valeurs de ces mesures.

Souvent dans la littérature, les comparaisons entre méthodes consistent en une
évaluation sur une tâche (prédiction de liens [Lü et Zhou, 2011; Fouss et al., 2012]
ou encore imitation d’une similarité sémantique [Hughes et Ramage, 2007]). Ce type
d’évaluation permet de savoir quelles sont les méthodes les plus eﬃcaces pour tel
problème sur telles données. Mais cela ne permet pas de comprendre les diﬀérences de
comportement des méthodes. Par exemple, il est possible que deux mesures aient des
bons scores mais ne les obtiennent pas du tout de la même façon. De telles diﬀérences
ne sont pas visibles avec une évaluation par tâche. Nous proposons ici une évaluation
complémentaire, qui permet de mieux cerner les diﬀérences (et ressemblances) entre
les mesures, sans chercher directement à savoir quelle est la « meilleure » mesure
pour telle ou telle tâche.

Certains auteurs ont déjà proposé des évaluations similaires à ce que nous ex-
posons ici, mais pas à notre connaissance concernant les mesures de similarité entre
sommets d’un graphe. Notamment Cha [2007] propose une comparaison des mesures
de similarité entre distributions de probabilité et plus récemment Choi et al. [2010]
proposent une comparaison des mesures de similarité entre vecteurs binaires. Ces
deux travaux fournissent un remarquable travail d’état de l’art. Ils proposent en
outre un regroupement hiérarchique des diﬀérentes similarités basé sur les corréla-
tions entre ces mesures calculées sur un jeu de données aléatoires. Ces comparaisons
présentent deux limites. Tout d’abord, on peut douter que le modèle de données
aléatoires utilisé incorpore les caractéristiques fréquentes des données réelles. Par
exemple, dans [Choi et al., 2010], tous les points semblent posséder à peu près le
même nombre de valeurs non-nulles (c’est-à-dire non « fausse », les vecteurs étant
binaires). Or sur des données réelles, il est tout à fait possible que certains points
aient énormément de composantes non-nulles alors que la plupart des points en ont
très peu. Certainement les méthodes ne se comportent pas de la même manière sur
des données ainsi déséquilibrées. L’autre limite de ces études est qu’elles ne per-
mettent pas d’interpréter et de comprendre pourquoi les méthodes sont regroupées
de cette façon.

Nous essayons dans l’étude proposée ici de dépasser ces deux limitations. D’une
part en travaillant sur des données aléatoires mais aussi sur des données réelles. Et
d’autre part en observant les valeurs des méthodes sur diﬀérentes familles de paires
de sommets.

2.4. COMPARAISON EXPÉRIMENTALE

61

2.4.1 Mise en place expérimentale

Nous présentons ici le protocole expérimental mis en place aﬁn de comparer les

diﬀérentes mesures présentées dans les sections précédentes.

Les graphes. Nous avons travaillé sur 8 graphes, à savoir :

• 3 graphes aléatoires de type Erdős-Rényi (ER01, ER02 et ER05),
• 2 graphes aléatoires formés de 5 clusters de 50 sommets (CGMpd et CGMtd),
• 3 graphes réels (RW_protein_lcc, RW_power et RW_dsV_lcc).

Les caractéristiques de ces graphes sont données dans le tableau 2.5. RW_power
est un graphe modélisant le réseau électrique de l’ouest des États-Unis : les som-
mets sont les générateurs et les diﬀérents postes électriques : transformateurs, nœud
d’interconnexion de lignes, etc. Les arêtes représentent les lignes électriques existant
entre ces éléments. Ce graphe a été diﬀusé par Watts et Strogatz [1998]. Le graphe
RW_protein_lcc est la plus grande composante connexe d’un réseau d’interaction de
protéines [Jeong et al., 2001]. Chaque sommet correspond à une protéine présente
dans les Saccharomyces cerevisiae (une levure), et une arête relie deux protéines si
une interaction directe a été identiﬁé entre elles. Enﬁn RW_dsV_lcc est la plus grande
composante connexe d’un graphe de synonymie des verbes du français (DicoSyn).
Nous utiliserons des graphes provenant de cette même ressource au chapitre suivant,
en section 3.5.

Les graphes aléatoires comportant des clusters (CGMpd et CGMtd) sont construits

grâce au modèle suivant.

Cluster Graph Model, CGM. Nous proposons d’appeler CGM (pour Cluster
Graph Model) le modèle consistant à construire un graphe à partir de k groupes de r
sommets chacun, tel que les arêtes soient aléatoires mais avec diﬀérentes probabilités
à l’intérieur ou entre les groupes. Une arête entre deux sommets du même groupe
existe avec une probabilité µintra, et avec une probabilité µinter entre des sommets
de groupes diﬀérents. Les deux graphes ont été construits avec k = 5, r = 50. Pour
CGMpd on a µintra = 0.15 et µinter = 0.01, alors que pour CGMtd µintra = 0.50
et µinter = 0.02. CGMtd est donc plus dense, et avec des clusters plus lisibles que
CGMpd.

62

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Table 2.5 – Pedigree des graphes utilisés pour la comparaison. n et m sont l’ordre
et la taille des graphes, hki le degré moyen, ncc le nombre de composante connexe,
« directed » indique si les graphes sont dirigés ou non, c le coeﬃcient de cluste-
ring, lccp est la longueur moyenne des plus courts chemins sur cette plus grande
composante connexe, et ρ est corrélation des degrés.

ER01
ER02
ER05
CGMpd
CGMtd
RW_protein_lcc
RW_power
RW_dsV_lcc

n
1000
1000
1000
250
250
1458
4941
8993

m
4942
9977
25029
1176
3522
1948
6594
51333

hki
9.88
19.95
50.06
9.41
28.18
2.67
2.67
11.42

ncc directed
1
F alse
1
F alse
1
F alse
1
F alse
1
F alse
1
F alse
1
F alse
1
F alse

c
0.01
0.02
0.05
0.10
0.37
0.05
0.10
0.14

ρ

lccp
3.27 −0.005
2.64 −0.005
2.03
0.002
2.86 −0.007
2.17
0.009
6.81 −0.209
0.004
18.99
4.20
0.062

Les mesures de similarité. Nous avons comparé toutes les mesures présentées
jusque là excepté celles qui sont non-symétriques. Les noms utilisés pour repérer les
méthodes correspondent à ceux indiqués précédemment. Il convient d’indiquer les
précisions suivantes :

propres de A,

• CO-l, DS-l, JD-l, etc. correspondent aux méthodes locales calculées en consi-
dérant une boucle sur chaque sommet, les autres méthodes locales sont donc
calculées sans boucle sur les sommets ;

• (SRK) SRK08 : γ = 0.8,
• (KTZ) KTZ001 : α = 0.01, KTZm09 : α = 0.9λ1 avec λ1 la plus forte des valeurs
• (LP) LP05 : α = 0.5, LP001 : α = 0.01,
• (EDK) EDK01 : α = 0.1, EDK10 : α = 1,
• (LEDK) LEDK01 : α = 0.1, LEDK05 : α = 0.5, LEDK10 : α = 1,
• (NLEDK) NLEDK05 : α = 0.5, NLEDK50 : α = 5, NLEDK100 : α = 10,
• (CTRW) CTRW05 : α = 0.5, CTRW50 : α = 5, CTRW100 : α = 10,
• (RCTK) RCTK07 : α = 0.7, RCTK09 : α = 0.8,
• (RLK) RLK08 : α = 0.8,
• (PPR-avg) PPR08-avg : d = 0.8,

Notons aussi que les méthodes CFL2R, CFL3R, CFL5R correspondent à la conﬂuence
calculée, pour les paires adjacentes, en retirant l’arête correspondante du graphe
(voir sous-section 2.2.2). Aussi toutes les méthodes semi-globales (hormis MDK et
SRW) sont calculées en ajoutant des boucles sur chaque sommet (voir section 2.2.1).

2.4. COMPARAISON EXPÉRIMENTALE

63

Table 2.6 – Récapitulatif des familles de paires de sommets utilisées pour la com-
paraison. Nous considérons que les « hubs » sont les 10% des sommets ayant le plus
de voisins, les « petits » sont les 90% restants.

graphe

ER

CGM

RW

paire

adjacents
non-adjacents
adjacents dans le même cluster
non-adjacents dans le même cluster
adjacents dans diﬀérents clusters

E
NE
C-E
C-NE
NC-E
NC-NE non-adjacents dans diﬀérents clusters
E-BB
E-BS
E-SS
NE-BB deux « hubs » non-adjacents
NE-BS
NE-SS

deux « hubs » adjacents
un « hub » et un « petit » adjacents
deux « petits » adjacents

un « hub » et un « petit » non-adjacents
deux « petits » non-adjacents

Paires échantillons. Les mesures de similarité sont évaluées sur des échantillons
des paires de sommets de chacun de ces graphes. Pour chacun des types de graphe
nous avons en eﬀet déterminé diﬀérentes familles de paires de sommets. Par exemple
sur les graphes type Erdős-Rényi, il y a deux familles : les paires adjacentes et les
paires de sommets non-adjacents. Ces diﬀérentes familles de paires de sommets sont
présentées dans le tableau 2.6. Pour chacun des graphes, 100 paires de sommets de
chaque famille ont été sélectionnées aléatoirement. Les méthodes sont comparées
sur ces paires. Ce choix de familles de paires de sommets permet d’éviter des biais
de représentation. En eﬀet ces familles séparent l’ensemble des paires de sommets
d’un graphe en des sous-ensembles de tailles très diﬀérentes. Si les paires étaient
simplement tirées au hasard, certaines familles seraient sur-représentées et d’autres
sous-représentées. Ce découpage en famille va aussi nous permettre de mieux com-
prendre les résultats, en eﬀet nous pourrons observer le comportement des méthodes
sur chaque famille de paires.

Corrélation et clustering hiérarchique. L’évaluation consiste donc à mesurer
la corrélation entre les valeurs des mesures de similarité sur ces diﬀérentes paires,
éventuellement en se limitant à certains graphes ou/et à certaines familles de paires.
Pour cela nous avons utilisé le coeﬃcient de corrélation de Pearson (voir par exemple
[Saporta, 2006, section 6.1.2]), et le coeﬃcient de Spearman [Saporta, 2006, section
6.3.1]. Le coeﬃcient de Pearson mesure la corrélation linéaire entre deux séries de
valeurs, alors que le coeﬃcient de Spearman mesure la corrélation des rangs. Deux

64

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

méthodes ont un coeﬃcient de Spearman fort (c’est-à-dire proche de 1) si leurs
valeurs induisent presque le même classement des paires de sommets, sans forcément
qu’elles soient corrélées linéairement.

Nous avons construit diﬀérents regroupements hiérarchiques des mesures de si-
milarité à partir des coeﬃcients de corrélation : initialement chaque mesure est seule
dans un groupe, puis successivement les deux groupes les « plus proches » sont ras-
semblés. Cela jusqu’à ce que toutes les mesures soient dans un seul groupe. Pour
mesurer une distance entre deux mesures s1 et s2 nous utilisons la méthode suivante :
(2.83)

d(s1, s2) = 1 − max(c(s1, s2), 0)

où c(s1, s2) est le coeﬃcient de corrélation entre s1 et s2. Notons que les corrélations
négatives sont ramenées à zéro. Cela ne change pas fondamentalement les résultats,
simplement pour une distance de 1 toutes les mesures seront regroupées. La distance
entre deux groupes est évaluée en utilisant la méthode dite de saut maximal (ou
complete linkage en anglais). Cela consiste à prendre comme distance entre deux
groupes, la plus grande des distances entre les éléments des deux groupes. Ainsi la
distance entre deux groupes (de similarités) S1 et S2 vaut :

d(S1, S2) =

max

d(s1, s2)

s1∈S1, s2∈S2
= 1 − min

s1∈S1, s2∈S2

max(c(s1, s2), 0)

(2.84)
(2.85)

Pour plus de détails sur les méthodes de clustering hiérarchique nous renvoyons à
[Gan et al., 2007, section 7.2].

2.4.2 Corrélation entre les méthodes

Nous présentons ici les résultats obtenus en étudiant la corrélation entre les
mesures de similarité sur toutes les paires sélectionnées sur chacun des 8 graphes.

Élimination des méthodes fortement similaires. La ﬁgure 2.1 présente les
corrélations de Pearson entre 83 mesures de similarité. Nous constatons tout d’abord
des corrélations très fortes entre certaines mesures. Aussi dans la suite, aﬁn de rendre
plus lisibles les résultats, une seule mesure de chacun de ces groupes sera présentée :

• DS, CO, JD, HDI : nous ne gardons que JD ;
• DS-l, CO-l, JD-l, HDI-l : de la même manière, nous ne gardons que JD-l ;
• LP05, LP001 : nous ne gardons que LP05 ;
• PPR07-avg, PPR08-avg : nous ne gardons que PPR08-avg ;
• RLK05, RLK08, RLK1 : nous ne gardons que RLK08, Notons que LEDK05 est aussi
• RCTK07, RCTK08 : nous ne gardons que RCTK07 ;

fortement corrélée avec ces mesures ;

2.4. COMPARAISON EXPÉRIMENTALE

65

• EDK10, EDK05 : nous ne gardons que EDK10 ;
• MDK2, MDK3 : nous ne gardons que MDK2 ;
• MDK5, MDK7 : nous ne gardons que MDK5 ;
• SRW3, SRW5 : nous ne gardons que SRW5 ;
• PRX2-deg, RA-l 11 : nous ne gardons que RA-l, ;
• CTRW01, CTRW05, CTRW10 : nous ne gardons que CTRW05 ;
• NLDK01, NLDK05, NLDK10 : nous ne gardons que NLDK05 ;
• CTRW01, CTRW05, CTRW10 : nous ne gardons que CTRW05 ;
• CLF5R, CLF7R : nous ne gardons que CLF5R ;

Nous avons vériﬁé que les corrélations de Spearman, un peu moins discriminantes,
amènent aux mêmes conclusions. La plupart de ces fortes corrélations indiquent la
faible inﬂuence d’un paramètre sur la valeur de la mesure. Notons tout de même la
forte corrélation des méthodes locales DS, CO, JD, HDI. D’autres groupes de mesures
sont fortement corrélées, mais leurs déﬁnitions étant mathématiquement diﬀérentes,
nous préférons les garder séparément. C’est le cas en particulier de SRW2 et KTZ001.
Les ﬁgures 2.3 et 2.2 présentent les matrices des corrélations de Pearson et de
Spearman entre les méthodes de cette liste limitée. Dans la suite les comparaisons
seront limitées à ces 60 mesures.

Quatre groupes de méthodes. On observe en ﬁgure 2.2 que les méthodes
peuvent être regroupées en quatre groupes :

(1) SRW2, KTZ001, CN-l, RA-l, HPI-l JD-l, AA-l, SRK08, EDK01, LP05, GEO, PRX*-cos ;
(2) CFL*R, LHN1, HPI, JD, RA, AA, CN ;
(3) toutes les autres méthodes ;
(4) ACT, PA, EDK10, BLD.

Notons que ces groupes sont particulièrement denses, c’est en eﬀet une conséquence
de la méthode de saut maximal. Seul le groupe (1) est un peu moins dense. Il peut
être redécoupé en 5 sous-groupes (dont un contenant les méthodes locales). On
remarque ensuite que les groupes (2), (3) et (4) sont clairement séparés. À l’inverse
le groupe (1) est moins clairement déﬁni, en particulier les méthodes des groupes (1)
et (3) sont assez fortement corrélées, à quelques exceptions près. Il en ressort donc
trois familles de méthodes, que l’on peut interpréter ainsi :

• groupe (2) : méthodes non sensibles aux chemins de longueur 1,
• groupe (4) : méthodes rapprochant les sommets de forts degrés (corrélés à PA).
• groupe (1) et (3) : les autres méthodes, plutôt sensibles aux chemins de lon-

gueur 1,

11. Ces mesures sont mathématiquement équivalentes, voir section 2.3.1.6.

66

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Figure 2.1 – Matrice des corrélations de Pearson entre les mesures de similarité
calculées sur diﬀérentes paires de sommets de 8 graphes. Les deux dendrogrammes
sont identiques, et sont calculés avec la méthode du saut maximal.

EDK05 EDK10 PA ACT CTRW10 CTRW01 CTRW05 CTRW50 CTRW100 PRX3-avg PPR07-avg PPR08-avg PRX5-dot MDK5 MDK7 PRX2-dot PRX3-dot PRX5-avg MDK2 MDK3 RCTK09 PRX5-dege LEDK10 PRX2-dege PRX3-dege RCTK07 RCTK08 LHN1-l RLK05 LEDK05 RLK1 RLK08 PRX2-avg NLEDK10 NLEDK01 NLEDK05 CTK NLEDK50 NLEDK100 LEDK50 LEDK100 KTZm09 LEDK01 CO-l DS-l JD-l HDI-l PRX3-deg PRX5-deg HPI-l RA-l PRX2-deg LHN1 SRK08 CFL3R CFL5R CFL7R BLD GEO PRX5-cos PRX2-cos PRX3-cos CFL2 CFL3 CFL5 SRW3 SRW5 KTZ001 SRW2 CFL2R RA HPI CO DS JD HDI LP001 LP05 CN-l CN AA AA-l EDK01 EDK01 AA-l AA CN CN-l LP05 LP001 HDI JD DS CO HPI RA CFL2R SRW2 KTZ001 SRW5 SRW3 CFL5 CFL3 CFL2 PRX3-cos PRX2-cos PRX5-cos GEO BLD CFL7R CFL5R CFL3R SRK08 LHN1 PRX2-deg RA-l HPI-l PRX5-deg PRX3-deg HDI-l JD-l DS-l CO-l LEDK01 KTZm09 LEDK100 LEDK50 NLEDK100 NLEDK50 CTK NLEDK05 NLEDK01 NLEDK10 PRX2-avg RLK08 RLK1 LEDK05 RLK05 LHN1-l RCTK08 RCTK07 PRX3-dege PRX2-dege LEDK10 PRX5-dege RCTK09 MDK3 MDK2 PRX5-avg PRX3-dot PRX2-dot MDK7 MDK5 PRX5-dot PPR08-avg PPR07-avg PRX3-avg CTRW100 CTRW50 CTRW05 CTRW01 CTRW10 ACT PA EDK10 EDK05 1.00.50.00.51.0tousPearson - saut maximalcorrelation2.4. COMPARAISON EXPÉRIMENTALE

67

Figure 2.2 – Matrice des corrélations de Spearman entre une sélection des mesures
de similarité, calculée sur diﬀérentes paires de sommets de 8 graphes. Les deux
dendrogrammes sont identiques, et sont calculés avec la méthode du saut maximal.

BLD EDK10 PA ACT KTZm09 SRW5 PRX3-deg PRX5-deg NLEDK100 CTRW100 LEDK10 PRX5-dot PRX3-avg PRX3-dege MDK2 PRX2-dot NLEDK50 PRX5-avg RCTK09 CTRW50 LEDK05 RLK08 PRX5-dege MDK5 PRX3-dot CTK CFL2 PRX2-avg LHN1-l PRX2-dege CFL3 CFL5 RCTK07 PPR08-avg CTRW05 LEDK01 NLEDK05 CN AA RA JD HPI LHN1 CFL2R CFL3R CFL5R PRX5-cos PRX2-cos PRX3-cos GEO LP05 EDK01 SRK08 AA-l JD-l HPI-l RA-l CN-l KTZ001 SRW2 SRW2 KTZ001 CN-l RA-l HPI-l JD-l AA-l SRK08 EDK01 LP05 GEO PRX3-cos PRX2-cos PRX5-cos CFL5R CFL3R CFL2R LHN1 HPI JD RA AA CN NLEDK05 LEDK01 CTRW05 PPR08-avg RCTK07 CFL5 CFL3 PRX2-dege LHN1-l PRX2-avg CFL2 CTK PRX3-dot MDK5 PRX5-dege RLK08 LEDK05 CTRW50 RCTK09 PRX5-avg NLEDK50 PRX2-dot MDK2 PRX3-dege PRX3-avg PRX5-dot LEDK10 CTRW100 NLEDK100 PRX5-deg PRX3-deg SRW5 KTZm09 ACT PA EDK10 BLD 1.00.50.00.51.0tous (filtre)Spearman - saut maximalspearman68

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Figure 2.3 – Matrice des corrélations de Pearson entre une sélection des similarités,
calculée sur diﬀérentes paires de sommets de 8 graphes. Cette ﬁgure est identique à
la ﬁgure 2.1 simplement pour chaque groupe de méthodes très corrélées, une seule
a été gardée.

EDK10 PA ACT KTZm09 SRW5 PRX3-deg PRX5-deg CTK CTRW50 CTRW100 NLEDK50 NLEDK100 LEDK01 JD-l HPI-l RA-l CTRW05 PRX5-dot PRX2-dot PRX3-dot PPR08-avg PRX3-avg MDK5 MDK2 PRX5-avg RCTK09 PRX5-dege PRX2-dege RCTK07 PRX3-dege LEDK10 LHN1-l LEDK05 RLK08 NLEDK05 PRX2-avg LHN1 SRK08 RA JD HPI CFL2R CFL3R CFL5R BLD LP05 CN-l CN AA AA-l EDK01 GEO PRX5-cos PRX2-cos PRX3-cos KTZ001 SRW2 CFL2 CFL3 CFL5 CFL5 CFL3 CFL2 SRW2 KTZ001 PRX3-cos PRX2-cos PRX5-cos GEO EDK01 AA-l AA CN CN-l LP05 BLD CFL5R CFL3R CFL2R HPI JD RA SRK08 LHN1 PRX2-avg NLEDK05 RLK08 LEDK05 LHN1-l LEDK10 PRX3-dege RCTK07 PRX2-dege PRX5-dege RCTK09 PRX5-avg MDK2 MDK5 PRX3-avg PPR08-avg PRX3-dot PRX2-dot PRX5-dot CTRW05 RA-l HPI-l JD-l LEDK01 NLEDK100 NLEDK50 CTRW100 CTRW50 CTK PRX5-deg PRX3-deg SRW5 KTZm09 ACT PA EDK10 1.00.50.00.51.0tous (filtre)Pearson - saut maximalcorrelation2.4. COMPARAISON EXPÉRIMENTALE

69

Corrélations de Pearson. Le regroupement résultant des corrélations de Pearson
(ﬁgure 2.3), est un peu plus fractionné. Globalement on retrouve les trois groupes
de méthodes. On peut noter les diﬀérences suivantes :

KTZ001, etc.),

• les méthodes des groupes (1) et (3) sont un peu moins homogènes,
• les méthodes CFL* sont ici regroupées avec les méthodes du groupe (1) (SRW2,
• BLD n’est plus corrélé au groupe (4),
• SRK08, fortement corrélé à LHN1, se retrouve ici proche des méthodes du
• AA, CN, AA-l, CN-l, EDK01, LP05 se retrouvent ensemble, ce sont toutes des

groupe (2) (CFL*R, et les méthodes locales sans boucles),

méthodes qui ne sont pas normées.

Mesures locales. On remarque aussi que pour les méthodes locales :

• l’ajout ou non des boucles (« -l ») est important,
• les méthodes se comportent semblablement, sauf bien sûr PA, mais aussi, et

c’est plus remarquable, LHN1.

Ces observations sur l’ensemble des graphes et l’ensemble des familles de paires
de sommets permettent donc de mettre en avant certains des comportements des
méthodes. Seulement il n’est pas évident de comprendre pourquoi telle ou telle
mesure se comporte similairement ou diﬀéremment. Pour cela nous allons continuer
la comparaison en nous restreignant à certains graphes et/ou familles de paires des
sommets.

2.4.3 Comparaison sur des graphes aléatoires Erdős-Rényi
Nous nous intéressons ici aux corrélations entre les mesures de similarité calculées
uniquement sur les trois graphes aléatoires de type Erdős-Rényi. Il est intéressant
d’observer les mesures de similarité sur des graphes aléatoires. En eﬀet comme ces
graphes ne présentent pas de structure particulière, il est facile d’observer les biais
des méthodes. En particulier nous allons nous intéresser à la sensibilité des méthodes
à la densité des graphes. Est-ce que les valeurs de similarité sont plus fortes sur des
graphes denses que sur des graphes peu denses ? De plus, nous allons observer la
sensibilité des méthodes aux chemins de longueur 1. Est-ce que deux sommets liés
par une arête ont une similarité plus forte que deux sommets non adjacents ? A
première vue, il peut sembler naturel que ce soit le cas. Pourtant la similarité entre
deux sommets ne tient pas seulement au fait qu’ils soient adjacents ou non. Au
contraire, il peut être intéressant grâce à une mesure de similarité de questionner la
valeur d’une arête : est-ce que le « reste du graphe » conﬁrme la proximité des deux
sommets ? Comme dans un graphe aléatoire les arêtes sont indépendantes, a priori

70

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Table 2.7 – Regroupement des mesures de similarité en fonction de leurs sensibilités
à la densité, sur les arêtes et non-arêtes de graphes aléatoires. Sur chaque ligne, les
méthodes sont classées par ordre alphabétique. Les * indiquent une valeur de paramètre
quelconque (parmi celles testées). Les méthodes locales sont soulignées.

sensibilité à la densité
sur les
arêtes
%

non-arêtes

sur les

%

%
%

%
−
−

&

−
&

&%
−
&

&

méthodes

AA, ACT, CFL2R, CN, CN-l, EDK01, EDK10, HPI, JD,
KTZ001, LP05, PA, PRX*-cos, RA, SRW2
GEO
CFL2, CTK, HPI-l, JD-l, LEDK01, PRX3-deg, RA-l,
RLK08
AA-l, PRX5-deg, SRW5
BLD, CFL3R, CFL5R, LHN1, SRK08
CFL3, CFL5, CTRW05, CTRW50, CTRW100,
KTZm09, LEDK05, LEDK10, LHN1-l, NLEDK05,
NLEDK50, NLEDK100, MDK2, MDK5, PRX*-avg,
PRX2-dot, PRX3-dot, PPR08-avg,
PRX*-dege, PRX5-dot, RCTK07, RCTK09

rien dans le « reste du graphe » ne rend deux sommets adjacents plus proches que
deux sommets non-adjacents.

Les ﬁgures 2.5 et 2.4 présentent les valeurs des diﬀérentes mesures de similarité
sur une sélection aléatoire d’arêtes et de paires de sommets non-adjacents dans cha-
cun des graphes. Enﬁn la ﬁgure 2.6 donne ces valeurs à la fois sur les arêtes et sur les
paires non-adjacentes. Sur chacune de ces ﬁgures un regroupement hiérarchique des
méthodes est présenté. Ces dendrogrammes sont construits à partir des corrélations
de Spearman calculées entre les lignes de valeurs aﬃchées. Les regroupements sont
donc diﬀérents dans chacune de ces ﬁgures.

Sensibilité à la densité.
Il est intéressant d’observer à partir des ﬁgures 2.5 et
2.4 la sensibilité des diﬀérentes méthodes à la densité des graphes aléatoires. Sur
chacune de ces ﬁgures, les valeurs de chaque méthode sont présentées sur les mêmes
paires de sommets de trois graphes aléatoires : ER01, ER02 et ER05. La seule diﬀérence
entre ces graphes est la densité, ER01 a une densité de 1%, ER02 de 2% et ER05 de 5%
(voir tableau 2.5). Certaines méthodes ont tendance à avoir des valeurs plus fortes
sur le graphe le plus dense, c’est l’inverse pour d’autres. De plus le comportement est
parfois diﬀérent pour les paires adjacentes (ﬁgure 2.5) et les paires non-adjacentes
(ﬁgure 2.4).

Le résultat de ces observations est résumé dans le tableau 2.7. On remarque que
toutes les méthodes locales (sauf LHN1 et LHN1-l) sont positivement corrélées à la

2.4. COMPARAISON EXPÉRIMENTALE

71

Figure 2.4 – Valeurs des similarités sur des paires de sommets non-adjacentes de
graphes Erdős-Rényi (ER). Le regroupement hiérarchique est fait par la méthode de
saut maximal à partir des corrélations de Spearman. Pour chaque groupe de paires
de sommets il est indiqué la valeur médiane suivie de la valeur moyenne. Ces valeurs
sont calculées en ramenant, pour chaque méthode, les scores de similarité entre 0 et 1
(normalisation par rapport à la plus forte valeur observée.).

1.000.800.600.400.200.00ER01ER02ER05NE0.08  -  0.120.14  -  0.150.18  -  0.180.03  -  0.110.13  -  0.210.38  -  0.380.10  -  0.130.14  -  0.150.17  -  0.170.00  -  0.050.01  -  0.030.04  -  0.040.01  -  0.060.01  -  0.050.06  -  0.060.03  -  0.080.04  -  0.070.08  -  0.080.00  -  0.030.00  -  0.070.49  -  0.440.00  -  0.030.00  -  0.080.49  -  0.440.00  -  0.020.00  -  0.060.50  -  0.450.00  -  0.020.00  -  0.060.50  -  0.450.44  -  0.410.44  -  0.621.00  -  0.940.00  -  0.070.00  -  0.110.35  -  0.340.00  -  0.070.00  -  0.100.30  -  0.290.00  -  0.080.00  -  0.120.40  -  0.390.00  -  0.080.00  -  0.110.37  -  0.360.00  -  0.070.00  -  0.100.34  -  0.320.00  -  0.070.00  -  0.100.34  -  0.320.00  -  0.080.00  -  0.110.39  -  0.360.00  -  0.110.00  -  0.220.53  -  0.470.00  -  0.110.00  -  0.220.53  -  0.470.00  -  0.060.00  -  0.040.06  -  0.060.00  -  0.040.00  -  0.020.01  -  0.010.00  -  0.060.00  -  0.040.06  -  0.060.00  -  0.050.00  -  0.040.05  -  0.050.07  -  0.120.22  -  0.240.61  -  0.610.06  -  0.120.17  -  0.220.61  -  0.610.10  -  0.130.23  -  0.250.63  -  0.640.00  -  0.000.01  -  0.020.62  -  0.620.00  -  0.010.01  -  0.050.55  -  0.550.06  -  0.090.26  -  0.280.91  -  0.910.00  -  0.010.04  -  0.050.59  -  0.600.48  -  0.480.98  -  0.981.00  -  1.000.15  -  0.160.64  -  0.640.99  -  0.990.07  -  0.080.24  -  0.250.78  -  0.780.02  -  0.020.09  -  0.100.64  -  0.640.00  -  0.000.00  -  0.010.62  -  0.620.41  -  0.430.40  -  0.410.43  -  0.430.30  -  0.340.31  -  0.340.36  -  0.360.19  -  0.240.24  -  0.240.26  -  0.260.41  -  0.450.43  -  0.430.46  -  0.450.40  -  0.350.50  -  0.510.55  -  0.550.40  -  0.350.50  -  0.510.55  -  0.550.08  -  0.130.13  -  0.140.15  -  0.150.43  -  0.440.48  -  0.490.49  -  0.490.43  -  0.440.48  -  0.490.49  -  0.490.16  -  0.200.17  -  0.180.18  -  0.180.08  -  0.130.10  -  0.120.13  -  0.130.11  -  0.150.14  -  0.150.15  -  0.150.15  -  0.160.16  -  0.160.16  -  0.160.09  -  0.120.13  -  0.130.13  -  0.130.19  -  0.220.21  -  0.220.21  -  0.210.08  -  0.140.10  -  0.120.13  -  0.130.15  -  0.200.18  -  0.200.21  -  0.210.11  -  0.160.15  -  0.150.16  -  0.160.22  -  0.250.25  -  0.260.26  -  0.260.14  -  0.170.07  -  0.070.00  -  0.000.14  -  0.170.06  -  0.060.01  -  0.010.04  -  0.080.02  -  0.030.01  -  0.010.30  -  0.320.21  -  0.210.15  -  0.150.06  -  0.100.05  -  0.050.02  -  0.02Graphes ER, non-aretes - Speaman, saut maximalRLK08RLK08LEDK01LEDK01CTKCTKCTRW05CTRW05NLEDK05NLEDK05SRK08SRK08AA-lAA-lAAAACN-lCN-lCNCNGEOGEOHPI-lHPI-lHPIHPIJD-lJD-lJDJDSRW2SRW2RARARA-lRA-lCFL2RCFL2RCFL2CFL2PRX2_avgPRX2_avgPRX2_degePRX2_degeLHN1-lLHN1-lLHN1LHN1PRX3_degPRX3_degSRW5SRW5PRX5_degPRX5_degEDK01EDK01KTZ001KTZ001PRX2_cosPRX2_cosLP05LP05PRX5_cosPRX5_cosPRX3_cosPRX3_cosACTACTPAPAEDK10EDK10NLEDK100NLEDK100BLDBLDKTZm09KTZm09CTRW100CTRW100CFL3RCFL3RCFL3CFL3PRX3_avgPRX3_avgCFL5RCFL5RCFL5CFL5MDK5MDK5MDK2MDK2PRX2_dotPRX2_dotLEDK10LEDK10LEDK05LEDK05PRX3_dotPRX3_dotPPR08_avgPPR08_avgNLEDK50NLEDK50CTRW50CTRW50PRX5_avgPRX5_avgPRX5_degePRX5_degeRCTK09RCTK09RCTK07RCTK07PRX5_dotPRX5_dotPRX3_degePRX3_dege72

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Figure 2.5 – Valeurs des similarités sur des arêtes de graphes Erdős-Rényi (ER).
Le regroupement hiérarchique est fait par la méthode de saut maximal à partir des
corrélations de Spearman. Pour chaque groupe de paires de sommets il est indiqué la
valeur médiane suivie de la valeur moyenne. Ces valeurs sont calculées en ramenant,
pour chaque méthode, les scores de similarité entre 0 et 1.

1.000.800.600.400.200.00ER01ER02ER05E0.00  -  0.050.00  -  0.130.35  -  0.340.00  -  0.050.00  -  0.120.33  -  0.340.00  -  0.040.00  -  0.090.21  -  0.230.00  -  0.040.00  -  0.090.21  -  0.230.00  -  0.070.00  -  0.210.49  -  0.460.00  -  0.050.00  -  0.070.07  -  0.070.00  -  0.010.00  -  0.060.33  -  0.390.00  -  0.010.00  -  0.060.33  -  0.390.00  -  0.020.00  -  0.080.34  -  0.400.05  -  0.090.06  -  0.110.12  -  0.120.26  -  0.280.22  -  0.240.91  -  0.910.01  -  0.010.03  -  0.040.70  -  0.700.01  -  0.020.04  -  0.080.58  -  0.580.10  -  0.100.27  -  0.290.83  -  0.820.02  -  0.020.10  -  0.120.70  -  0.700.00  -  0.000.00  -  0.010.68  -  0.680.03  -  0.030.12  -  0.130.73  -  0.720.34  -  0.340.96  -  0.961.00  -  1.000.15  -  0.160.56  -  0.560.99  -  0.990.46  -  0.460.48  -  0.480.50  -  0.500.38  -  0.350.49  -  0.510.55  -  0.550.53  -  0.530.16  -  0.180.50  -  0.490.32  -  0.330.10  -  0.130.24  -  0.240.40  -  0.420.21  -  0.270.37  -  0.410.41  -  0.420.07  -  0.070.02  -  0.020.66  -  0.660.25  -  0.250.11  -  0.100.64  -  0.640.25  -  0.250.03  -  0.030.51  -  0.520.20  -  0.210.14  -  0.140.37  -  0.390.17  -  0.180.14  -  0.140.41  -  0.430.09  -  0.090.01  -  0.010.41  -  0.420.10  -  0.110.01  -  0.010.31  -  0.330.05  -  0.050.00  -  0.000.61  -  0.610.18  -  0.180.01  -  0.010.33  -  0.350.03  -  0.030.00  -  0.000.43  -  0.440.11  -  0.110.01  -  0.010.29  -  0.320.08  -  0.090.02  -  0.020.41  -  0.440.12  -  0.130.03  -  0.030.23  -  0.270.04  -  0.040.00  -  0.000.92  -  0.910.74  -  0.720.38  -  0.370.42  -  0.450.11  -  0.110.01  -  0.010.34  -  0.370.08  -  0.080.00  -  0.000.35  -  0.390.09  -  0.090.00  -  0.000.67  -  0.680.27  -  0.260.01  -  0.010.34  -  0.370.08  -  0.080.00  -  0.000.54  -  0.560.23  -  0.230.02  -  0.020.43  -  0.440.16  -  0.160.01  -  0.010.20  -  0.250.00  -  0.000.00  -  0.000.90  -  0.890.65  -  0.630.10  -  0.110.35  -  0.370.05  -  0.060.00  -  0.000.74  -  0.730.23  -  0.230.01  -  0.010.34  -  0.360.05  -  0.050.00  -  0.000.06  -  0.090.00  -  0.000.00  -  0.000.35  -  0.380.05  -  0.040.00  -  0.000.27  -  0.290.08  -  0.090.02  -  0.020.27  -  0.270.10  -  0.110.02  -  0.020.49  -  0.520.22  -  0.260.16  -  0.160.49  -  0.500.21  -  0.250.16  -  0.160.35  -  0.380.15  -  0.180.11  -  0.110.17  -  0.180.15  -  0.170.16  -  0.16Graphes ER, aretes - Speaman, saut maximalHPIHPIJDJDSRW2SRW2RARACFL2RCFL2RLHN1LHN1CN-lCN-lCNCNAAAASRK08SRK08PRX2_cosPRX2_cosEDK01EDK01KTZ001KTZ001ACTACTPAPAEDK10EDK10LP05LP05PRX5_cosPRX5_cosPRX3_cosPRX3_cosCFL5RCFL5RCFL3RCFL3RPRX5_degPRX5_degSRW5SRW5AA-lAA-lPRX5_avgPRX5_avgPRX3_degPRX3_degKTZm09KTZm09NLEDK100NLEDK100CTRW100CTRW100MDK5MDK5MDK2MDK2PRX2_dotPRX2_dotNLEDK50NLEDK50PRX3_dotPRX3_dotPRX3_avgPRX3_avgPRX2_avgPRX2_avgLHN1-lLHN1-lPRX2_degePRX2_degeCFL2CFL2RLK08RLK08CTKCTKRCTK09RCTK09LEDK01LEDK01RCTK07RCTK07NLEDK05NLEDK05PPR08_avgPPR08_avgLEDK05LEDK05CFL3CFL3PRX3_degePRX3_degeCFL5CFL5PRX5_degePRX5_degeLEDK10LEDK10PRX5_dotPRX5_dotCTRW50CTRW50CTRW05CTRW05RA-lRA-lJD-lJD-lHPI-lHPI-lBLDBLD2.4. COMPARAISON EXPÉRIMENTALE

73

densité sur les paires non-adjacentes. L’ajout de boucles sur les sommets rend les
mesures inversement corrélées à la densité sur les paires adjacentes (sauf pour CN-l
et AA-l). Cela n’est pas surprenant au regard des déﬁnitions de ces méthodes. Plus le
graphe est dense plus la probabilité d’avoir des voisins communs augmente. Pour les
paires adjacentes, si le graphe comporte des boucles sur chaque sommet, le nombre
de voisins communs vaut au moins 2 ; si le nombre de voisins de chaque sommet
augmente (plus forte densité) alors toutes les méthodes qui normalisent le nombre
de voisins communs en fonction du nombre total de voisins ont tendance à diminuer
(jusqu’à une certaine densité à partir de laquelle cet avantage de 2 voisins communs
ne suﬃt plus, les valeurs deviennent similaires à celle des paires non-adjacentes, c’est
ce qui se passe pour AA-l).

Le comportement des méthodes globales (ou semi-globales) est moins évident.
Sur les paires non-adjacentes, une bonne partie des méthodes n’est pas corrélée à la
densité. Toutefois pour les paires non-adjacentes, il faut remarquer que ACT, CTK,
EDK*, KTZ001, LP05, PRX*-cos, SRW* sont corrélées à la densité, et PRX*-dege, PRX5-
dot, RCTK* sont inversement corrélées. Notons que plusieurs méthodes sont corrélées
à la densité seulement quand leur paramètre est de faible valeur (c’est le cas de CFL
et LEDK). Notons que KTZ001 est corrélé à la densité alors que KTZm09 ne l’est pas,
cela est dû au fait que le choix du paramètre alpha pour KTZm09 dépend du graphe
(α = 0.9λ1).

Toutes les méthodes non sensibles à la densité sur les paires non-adjacentes
sont inversement corrélées à la densité sur les paires adjacentes, sauf BLD, CFL3R,
CFL5R, LHN1 et SRK08. Le phénomène est semblable à ce qui se passe pour les
méthodes locales avec boucles : plus le graphe devient dense moins l’avantage d’être
directement lié a d’importance.

On observe donc que BLD, CFL3R, CFL5R et SRK08 sont les seules mesures globales
à ne pas être corrélées à la densité des graphes. L’astuce consistant à ignorer l’arête
liant deux sommets lors du calcul de la similarité de ces deux sommets fait que, sur
les arêtes, CFL devient non sensible à densité. Certainement la même astuce appli-
quée aux autres méthodes non sensibles à la densité pour les paires non-adjacentes
produirait le même résultat. Notons que les valeurs de CFL3R, CFL5R et de BLD ne
sont pas corrélées.

Le fait que RCTK et PRX*-dege soient inversement corrélées à la densité se com-
prend en reprenant les déﬁnitions de ces deux méthodes : leurs valeurs sont le résul-
tat d’une marche aléatoire divisée par le degré d’arrivée (marche aléatoire en temps
courts pour PRX*-dege, et marche de type PageRank personnalisé pour RCTK). Donc
plus le degré moyen devient important, plus les valeurs tendent à être faibles.

Chemin de longueur 1. Sur la ﬁgure 2.6 les colonnes marquées « E » corres-
pondent aux paires adjacentes, celles marquées « NE » aux paires non-adjacentes.

74

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Figure 2.6 – Valeurs des mesures de similarité sur trois graphes Erdős-Rényi (ER).
Le regroupement hiérarchique est fait par la méthode de saut maximal à partir des
corrélations de Spearman.

1.000.800.600.400.200.00ER01ER02ER05ENEENEENEER - Spearman, saut maximalHPIHPIJDJDRARACFL2RCFL2RLHN1LHN1AAAACNCNSRK08SRK08PRX5_cosPRX5_cosPRX3_cosPRX3_cosACTACTPAPAEDK10EDK10EDK01EDK01LP05LP05PRX2_cosPRX2_cosCN-lCN-lPRX5_degPRX5_degSRW2SRW2KTZ001KTZ001GEOGEOSRW5SRW5AA-lAA-lBLDBLDCFL5RCFL5RCFL3RCFL3RPRX5_avgPRX5_avgNLEDK100NLEDK100CTRW100CTRW100PRX3_dotPRX3_dotLEDK10LEDK10MDK5MDK5MDK2MDK2PRX2_dotPRX2_dotLEDK05LEDK05CFL3CFL3PRX3_avgPRX3_avgCFL5CFL5PPR08_avgPPR08_avgNLEDK50NLEDK50CTRW50CTRW50KTZm09KTZm09CTRW05CTRW05NLEDK05NLEDK05RLK08RLK08CTKCTKLEDK01LEDK01PRX3_degPRX3_degHPI-lHPI-lJD-lJD-lRA-lRA-lPRX2_avgPRX2_avgLHN1-lLHN1-lCFL2CFL2PRX2_degePRX2_degeRCTK09RCTK09RCTK07RCTK07PRX3_degePRX3_degePRX5_degePRX5_degePRX5_dotPRX5_dot2.4. COMPARAISON EXPÉRIMENTALE

75

Ainsi il est aisé d’observer la sensibilité des mesures aux chemins de longueur 1. On
observe que les mesures, pour la plupart, tendent à donner des valeurs plus fortes
aux paires adjacentes. Quatre groupes de mesures, à l’inverse, ne donnent pas des
valeurs plus fortes aux paires adjacentes :

(1) HPI, JD, RA, CFL2R, LHN1, AA, CN, SRK08,
(2) ACT, PA, EDK10,
(3) BLD,
(4) CFL3R, CFL5R,
(5) NLEDK100, CTRW100.

Pour autant, ces groupes de mesures ne sont pas clairement corrélés entre eux.

La diﬀérence de valeurs entre paires adjacentes et non-adjacentes est en général
plus marquée sur les graphes peu denses que sur les graphes denses. Cela rejoint
les remarques faites précédemment quant à la sensibilité des méthodes à la densité
du graphe. La plupart des méthodes donnent des valeurs aux paires adjacentes
inversement corrélées à la densité. Plus le graphe est dense, moins le fait d’être
adjacent a d’importance.

Nous avons vu ici que les valeurs absolues de beaucoup de mesures sont sensibles
à la densité du graphe. Cela signiﬁe, pour notre évaluation, que les corrélations
que l’on peut observer lorsque l’on compare les mesures sur des graphes diﬀérents
peuvent être dues à cette dépendance à la densité. Ainsi dans la suite, pour éviter
de re-détecter cette corrélation à la densité, nous étudions les corrélations entre les
mesures sur un seul graphe à la fois.

2.4.4 Comparaison sur des graphes simples comportant des

clusters

Nous présentons ici le résultat de la comparaison des méthodes sur deux graphes
aléatoires simples comportant des clusters. Ces deux graphes (CGMpd et CGMtd) sont
construits comme indiqué en section 2.4.1 Dans cette section, nous nous intéressons
uniquement au graphe CGMpd, les résultats sur CGMtd étant semblables. Sur ces
graphes nous nous sommes intéressés à quatre familles de paires de sommets : les
paires adjacentes ou non, et à l’intérieur d’un même groupe ou non (voir tableau 2.6).
La ﬁgure 2.7 présente les valeurs des similarités sur les paires non-adjacentes du
graphe CGMtd. On constate que toutes les méthodes arrivent bien à diﬀérencier les
paires intra-clusters (C-NE) des paires inter-clusters (NC-NE), mis à part le groupe de
méthodes ACT, BLD, EDK10, KTZm09, PA. Les deux groupes qui apparaissent ensuite
correspondent à la distinction entre méthodes locales et globales.

La ﬁgure 2.8 présente les valeurs des similarités sur un échantillon des paires
adjacentes et non-adjacentes du graphe CGMtd. La ﬁgure 2.9 donne la matrice des

76

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Figure 2.7 – Valeurs des similarités sur des paires de sommets non-adjacents du
graphe CGMpd. Le dendrogramme représente le résultat d’une agrégation de ces
similarités, en fonction de leurs corrélations de Spearman sur ces paires de sommets.
Le regroupement est fait par la méthode dite du saut maximal.

1.000.800.600.400.200.00CGMpd_n0C-NENC-NECGMpd, non-adjacents - Spearman, saut maximalPRX2_dotPRX2_dotMDK2MDK2RCTK09RCTK09RCTK07RCTK07RLK08RLK08LEDK05LEDK05CTKCTKCFL3RCFL3RCFL3CFL3PRX3_degePRX3_degeSRK08SRK08SRW5SRW5PPR08_avgPPR08_avgEDK01EDK01KTZ001KTZ001LEDK01LEDK01CTRW05CTRW05NLEDK05NLEDK05CTRW50CTRW50PRX5_dotPRX5_dotLEDK10LEDK10PRX3_dotPRX3_dotMDK5MDK5CFL5RCFL5RCFL5CFL5PRX5_degePRX5_degePRX5_cosPRX5_cosPRX3_degPRX3_degPRX3_avgPRX3_avgLP05LP05PRX3_cosPRX3_cosPRX2_cosPRX2_cosPRX5_avgPRX5_avgNLEDK50NLEDK50PRX5_degPRX5_degNLEDK100NLEDK100CTRW100CTRW100HPI-lHPI-lHPIHPIJD-lJD-lJDJDLHN1-lLHN1-lLHN1LHN1CFL2RCFL2RCFL2CFL2PRX2_degePRX2_degePRX2_avgPRX2_avgSRW2SRW2RARARA-lRA-lAA-lAA-lAAAACN-lCN-lCNCNGEOGEOACTACTPAPABLDBLDEDK10EDK10KTZm09KTZm092.4. COMPARAISON EXPÉRIMENTALE

77

Figure 2.8 – Valeurs des similarités sur des paires de sommets adjacents et non-
adjacents du graphe CGMpd. Le dendrogramme représente le résultat d’une agréga-
tion de ces similarités, en fonction de leurs corrélations de Spearman sur ces paires
de sommets. Le regroupement est fait par la méthode dite du saut maximal.

1.000.800.600.400.200.00CGMpd_n0C-EC-NENC-ENC-NECGMpd - Spearman, saut maximalPRX3_degPRX3_degNLEDK50NLEDK50CTRW50CTRW50PRX5_avgPRX5_avgPRX2_cosPRX2_cosMDK2MDK2LEDK05LEDK05CFL5CFL5PRX5_degePRX5_degePRX2_dotPRX2_dotHPI-lHPI-lJD-lJD-lRA-lRA-lCFL2CFL2PRX2_degePRX2_degePRX2_avgPRX2_avgLHN1-lLHN1-lSRW2SRW2AA-lAA-lCN-lCN-lRLK08RLK08CTKCTKRCTK09RCTK09RCTK07RCTK07PPR08_avgPPR08_avgNLEDK05NLEDK05LEDK01LEDK01CFL3CFL3PRX3_degePRX3_degePRX3_avgPRX3_avgCTRW05CTRW05GEOGEOPRX5_degPRX5_degNLEDK100NLEDK100CTRW100CTRW100LP05LP05KTZm09KTZm09EDK01EDK01KTZ001KTZ001SRW5SRW5PRX3_dotPRX3_dotMDK5MDK5LEDK10LEDK10PRX5_dotPRX5_dotPRX5_cosPRX5_cosPRX3_cosPRX3_cosCFL5RCFL5RCFL3RCFL3RSRK08SRK08HPIHPIJDJDLHN1LHN1CFL2RCFL2RRARAAAAACNCNEDK10EDK10BLDBLDACTACTPAPA78

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Figure 2.9 – Matrice des corrélations de Spearman entres les diﬀérentes mesures,
sur des paires adjacentes et non-adjacents de CGMpd. Les deux dendrogrammes
sont identiques à celui de la ﬁgure 2.8. Ils sont calculés par la méthode dite du saut
maximal.

PA ACT BLD EDK10 CN AA RA CFL2R LHN1 JD HPI SRK08 CFL3R CFL5R PRX3-cos PRX5-cos PRX5-dot LEDK10 MDK5 PRX3-dot SRW5 KTZ001 EDK01 KTZm09 LP05 CTRW100 NLEDK100 PRX5-deg GEO CTRW05 PRX3-avg PRX3-dege CFL3 LEDK01 NLEDK05 PPR08-avg RCTK07 RCTK09 CTK RLK08 CN-l AA-l SRW2 LHN1-l PRX2-avg PRX2-dege CFL2 RA-l JD-l HPI-l PRX2-dot PRX5-dege CFL5 LEDK05 MDK2 PRX2-cos PRX5-avg CTRW50 NLEDK50 PRX3-deg PRX3-deg NLEDK50 CTRW50 PRX5-avg PRX2-cos MDK2 LEDK05 CFL5 PRX5-dege PRX2-dot HPI-l JD-l RA-l CFL2 PRX2-dege PRX2-avg LHN1-l SRW2 AA-l CN-l RLK08 CTK RCTK09 RCTK07 PPR08-avg NLEDK05 LEDK01 CFL3 PRX3-dege PRX3-avg CTRW05 GEO PRX5-deg NLEDK100 CTRW100 LP05 KTZm09 EDK01 KTZ001 SRW5 PRX3-dot MDK5 LEDK10 PRX5-dot PRX5-cos PRX3-cos CFL5R CFL3R SRK08 HPI JD LHN1 CFL2R RA AA CN EDK10 BLD ACT PA 1.00.50.00.51.0CGMpdSpearman, saut maximalspearman2.4. COMPARAISON EXPÉRIMENTALE

79

coeﬃcients de corrélation de Spearman calculés à partir de ces valeurs. Il est plus
intéressant d’observer ici les valeurs sur les paires adjacentes en même temps que les
valeurs des paires non-adjacentes. En eﬀet on observe assez clairement trois groupes
de méthodes :

• celles donnant des valeurs plus fortes aux paires adjacentes inter-clusters (NC-

E) qu’aux paires non-adjacentes mais intra-clusters (C-NE),

• celles donnant des valeurs moins fortes aux paires adjacentes inter-clusters

(NC-E) qu’aux paires non-adjacentes mais intra-clusters (C-NE),

• les méthodes EDK10, BLD, ACT, PA qui semblent complètement ignorer la struc-

ture en clusters.

La diﬀérence entre les deux premiers groupes provient de la sensibilité des méthodes
aux chemins de longueur 1. Il est intéressant de noter que la considération des boucles
ou non sur les méthodes locales les fait passer de l’un à l’autre des deux premiers
groupes. De même l’astuce de CFL*R permet bien d’éviter que les paires adjacentes
entre deux clusters aient des valeurs plus fortes que des paires non-adjacentes mais
dans le même cluster. Les méthodes globales donnant des valeurs moins fortes aux
paires adjacentes inter-cluster qu’aux paires non-adjacentes intra-cluster (second
groupe) sont : CFL*R, LEDK10, MDK5, PRX3-dot,PRX5-dot, PRX3-cos, PRX5-cos, SRK08.

2.4.5 Comparaison sur un graphe de terrain

Nous nous intéressons maintenant aux valeurs des méthodes sur un graphe réel.
En eﬀet pour éviter de capturer des comportements dus aux diﬀérences de densité
des graphes, et pour éviter de multiplier les données à analyser, nous nous limitons
à un seul graphe.

Les ﬁgures 2.10 et 2.12 donnent les valeurs des méthodes pour trois groupes de
paires de sommets non-adjacents et adjacents, pour le graphe dsV_lcc. Les dendro-
grammes présentés sur la droite des ﬁgures sont construits comme précédemment
avec la méthode de saut maximal à partir des corrélations de Pearson 12. Chaque
colonne correspond à une paire de sommets : soit deux sommets de forts degrés
(BB), soit un sommet de fort degré et un de faible degré (BS), soit deux sommets
de faibles degrés (SS). Arbitrairement, nous considérons que les sommets de forts
degrés sont les 10% les plus connectés, les 90% restant sont ceux de « petit » degré.
Cette décomposition en trois groupes de paires va nous permettre de comprendre le
comportement des méthodes face aux degrés des sommets. Est-ce que la similarité
entre deux gros a tendance à être plus forte que celle entre deux petits ?
12. Nous utilisons les corrélations de Pearson plutôt que celles de Spearman, les regroupements

étant ici plus discriminants et informatifs avec les coeﬃcients Pearson.

80

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Figure 2.10 – Valeurs des similarités sur des paires de sommets non-adjacents du
graphe dsV_lcc. Le dendrogramme représente le résultat d’une agrégation de ces
similarités, en fonction de leurs corrélations de Pearson sur ces paires de sommets.
Le regroupement est fait par la méthode dite du saut maximal.

1.000.800.600.400.200.00RW_dsV_lccNE-BBNE-BSNE-SSRW_dsV_lcc, non-adjacents - Pearson, saut maximalJD-lJD-lJDJDCFL2RCFL2RCFL2CFL2PRX2_avgPRX2_avgNLEDK05NLEDK05LEDK01LEDK01CTRW05CTRW05PRX5_avgPRX5_avgPPR08_avgPPR08_avgNLEDK50NLEDK50PRX3_avgPRX3_avgCFL3RCFL3RCFL3CFL3CFL5RCFL5RCFL5CFL5GEOGEOPRX3_cosPRX3_cosPRX2_cosPRX2_cosPRX5_cosPRX5_cosPRX5_degePRX5_degeRCTK09RCTK09PRX3_degePRX3_degeRCTK07RCTK07RLK08RLK08LEDK05LEDK05PRX3_dotPRX3_dotMDK5MDK5PRX2_dotPRX2_dotCTKCTKLEDK10LEDK10PRX5_dotPRX5_dotMDK2MDK2SRK08SRK08LHN1-lLHN1-lLHN1LHN1PRX2_degePRX2_degeHPI-lHPI-lHPIHPIPRX3_degPRX3_degSRW5SRW5PRX5_degPRX5_degEDK01EDK01LP05LP05AA-lAA-lAAAACN-lCN-lCNCNKTZ001KTZ001SRW2SRW2RARARA-lRA-lCTRW100CTRW100CTRW50CTRW50NLEDK100NLEDK100ACTACTPAPAEDK10EDK10BLDBLDKTZm09KTZm092.4. COMPARAISON EXPÉRIMENTALE

81

Figure 2.11 – Matrice des corrélations de Pearson entres les diﬀérentes mesures,
sur des paires non adjacentes de dsV_lcc. Les deux dendrogrammes sont identiques,
et identiques à celui de la ﬁgure 2.10. Ils sont calculés par la méthode dite du saut
maximal.

KTZm09 BLD EDK10 PA ACT NLEDK100 CTRW50 CTRW100 RA-l RA SRW2 KTZ001 CN CN-l AA AA-l LP05 EDK01 PRX5-deg SRW5 PRX3-deg HPI HPI-l PRX2-dege LHN1 LHN1-l SRK08 MDK2 PRX5-dot LEDK10 CTK PRX2-dot MDK5 PRX3-dot LEDK05 RLK08 RCTK07 PRX3-dege RCTK09 PRX5-dege PRX5-cos PRX2-cos PRX3-cos GEO CFL5 CFL5R CFL3 CFL3R PRX3-avg NLEDK50 PPR08-avg PRX5-avg CTRW05 LEDK01 NLEDK05 PRX2-avg CFL2 CFL2R JD JD-l JD-l JD CFL2R CFL2 PRX2-avg NLEDK05 LEDK01 CTRW05 PRX5-avg PPR08-avg NLEDK50 PRX3-avg CFL3R CFL3 CFL5R CFL5 GEO PRX3-cos PRX2-cos PRX5-cos PRX5-dege RCTK09 PRX3-dege RCTK07 RLK08 LEDK05 PRX3-dot MDK5 PRX2-dot CTK LEDK10 PRX5-dot MDK2 SRK08 LHN1-l LHN1 PRX2-dege HPI-l HPI PRX3-deg SRW5 PRX5-deg EDK01 LP05 AA-l AA CN-l CN KTZ001 SRW2 RA RA-l CTRW100 CTRW50 NLEDK100 ACT PA EDK10 BLD KTZm09 1.00.50.00.51.0RW_dsVnon-adjacentsPearson, saut maximalcorrelation82

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Les ﬁgures 2.11 et 2.13 donnent les matrices des corrélations de Pearson obtenues

à partir des valeurs présentées respectivement dans les ﬁgures 2.10 et 2.12.

Comportement sur les paires non-adjacentes. Les ﬁgures 2.10 et 2.11 pré-
sentent les résultats sur des paires non-adjacentes. Sur la ﬁgure 2.11, on remarque
une structure en trois groupes de taille équivalente :
(NE.1) JD-l, JD, CFL*R, CFL*, PRX*-avg, NLEDK05, NLEDK50, LEDK01, CTRW05, PPR08-

avg, GEO, PRX*-cos,

(NE.2) PRX*-dege, RCTK*, RLK08, LEDK05, LEDK10, PRX*-dot, MDK*, CTK, SRK08,

LHN1-l, LHN1, HPI-l, HPI,

(NE.3) PRX*-deg, SRW5, EDK01, EDK10, LP05, AA-l, AA, CN-l, CN, KTZ001, SRW2, RA,

RA-l, CRTW100, CRTW50, NLEDK100, ACT, PA, BLD, KTZm09,

En observant les valeurs des mesures sur la ﬁgure 2.10, il est possible de com-
prendre les diﬀérences de comportement entre ces trois groupes. Les mesures du
groupe (NE.3) pénalisent les paires formées de petits sommets (SS et BS) et privi-
légient les paires de forts sommets (BB). Les méthodes du groupe (NE.2) semblent
soit pénaliser les paires BB, soit être insensibles aux degrés des sommets. Enﬁn les
méthodes du groupe (NE.1) ont un comportement intermédiaire : les paires BB ont
des valeurs fortes sans que les autres paires ne soient pénalisées.

Il faut remarquer que la corrélation entre les groupes (NE.2) et (NE.3) est
faible, alors que les mesures du groupe (NE.1) sont relativement corrélées avec les
groupes (NE.2) et (NE.3). Aussi parmi les groupes (NE.2) et (NE.3) deux sous-
groupes compacts sont remarquables :
• sous-groupe de (NE.2) : MKD2, SRK08, LHN, LHN-l, PRX2-dege, HPI-l, HPI,
• sous-groupe de (NE.3) : ACT, PA, EDK10, BLD, KTZm09,

aucune valeur forte sur les paires BB ;

aucune valeur forte sur les paires BS et SS ;

Les méthodes de ces deux sous-groupes ont nettement un comportement biaisé en
fonction des degrés des paires observées. Cela est cohérent au regard des formules
de certaines de ces méthodes (en particulier LHN, PRX2-dege ou HPI pour le premier
groupe, et PA pour le second groupe), cela est un peu moins évident pour les autres
méthodes.

Comportement sur les arêtes. Les ﬁgures 2.13 et 2.12 présentent les résultats
sur des paires adjacentes. On constate en particulier sur la ﬁgure 2.13 que les diﬀé-
rences de comportement des méthodes sont beaucoup plus marquées sur les arêtes
que sur les paires non-adjacentes. On remarque la présence de deux groupes aux
comportements opposés :

2.4. COMPARAISON EXPÉRIMENTALE

83

(E.1) NLEDK*, LHN1-l, PRX*-dot, MDK*, PRX*-dege, LEDK10, LEDK05, RLK08, PRX*-

avg, PPR08-avg, RCTK*, CTK, CTRW100,

(E.2) ACT, BLD, EDK*, KTZm09, LP05, PA,

On comprend avec la ﬁgure 2.12 que les méthodes du groupe (E.1) privilégient les
paires de petits sommets (SS) ; et à l’inverse les méthodes du groupe (E.2) favorisent
les paires de gros sommets (BB).

Les autres méthodes ont des comportements plus ou moins semblables à l’un ou
l’autre de ces groupes. Notons en particulier que les méthodes AA, AA-l, CN, CN-l,
KTZ001 et dans une moindre mesure SRW* et RA ont un comportement analogue
aux méthodes du groupe (E.2). Les méthodes restantes sont plutôt corrélées avec
le groupe (E.1). Sauf les méthodes CFL*R qui se démarquent en n’étant corrélées ni
à l’un ni à l’autre de ces deux groupes. On observe sur la ﬁgure 2.12 qu’un certain
nombre de paires BS et SS, ont une valeur nulle (case blanche) avec CFL*R alors
qu’elles ont une valeur plutôt forte avec CFL* et les méthodes du groupe (E.1). Ces
paires de sommets tiennent donc leur forte similarité uniquement grâce à l’arête qui
les relie.

Notons que les méthodes du groupe (E.1) correspondent principalement aux mé-

thodes du groupe (NE.2), et celles du groupe (E.2) font toutes partie du groupe (NE.3).
C’est-à-dire que, à quelques exceptions près, les méthodes ont un comportement
semblable sur les paires adjacentes et non-adjacentes.

2.4.6 Conclusion

La première des conclusions de cette évaluation comparative est que les méthodes
ACT, BLD, EDK10, PA ont plusieurs comportements qui rendent ces méthodes diﬃci-
lement utilisables en pratique. Ce n’est pas surprenant pour PA. De plus le fait que
ACT se comporte similairement à PA (ce qui n’est pas trivial) avait été démontré
par von Luxburg et al. [2010]. Ces comportements sont plus surprenants pour EDK
et BLD. Ces évaluations permettent ensuite de mettre en exergue certaines des pro-
priétés des mesures. À savoir la sensibilité des méthodes à la densité des graphes,
aux chemins de longueurs 1, et aux degrés des sommets observés.

Comme attendu, il n’est pas aisé d’observer une diﬀérence de comportement
notable entre les diﬀérentes méthodes locales (sauf bien sûr pour PA). Toutefois la
considération des boucles modiﬁe clairement les méthodes pour les paires adjacentes.
Il faut aussi remarquer que HPI et LHN1 se diﬀérencient des autres méthodes locales.
Ces deux méthodes pénalisent fortement les similarités entre sommets de forts degrés
(ce qui n’est pas surprenant à la vue de leurs formules). Aussi LHN1 se montre
insensible à la densité sur des graphes aléatoires, ce qui là encore est cohérent par
rapport à sa déﬁnition utilisant un null model de même densité.

84

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Figure 2.12 – Valeurs des similarités sur des paires de sommets adjacents du
graphe dsV_lcc. Le dendrogramme représente le résultat d’une agrégation de ces
similarités, en fonction de leurs corrélations de Pearson sur ces paires de sommets.
Le regroupement est fait par la méthode dite du saut maximal.

1.000.800.600.400.200.00RW_dsV_lccE-BBE-BSE-SSRW_dsV_lcc, adjacents - Pearson, saut maximalPRX3_cosPRX3_cosPRX2_cosPRX2_cosPRX5_cosPRX5_cosHPIHPIJD-lJD-lJDJDSRK08SRK08LHN1LHN1CFL5RCFL5RCFL3RCFL3RCFL2RCFL2RLEDK01LEDK01HPI-lHPI-lCFL5CFL5CFL3CFL3CFL2CFL2CTRW50CTRW50CTRW05CTRW05NLEDK05NLEDK05LHN1-lLHN1-lPRX2_dotPRX2_dotMDK2MDK2PRX3_degePRX3_degeRCTK07RCTK07PRX2_degePRX2_degeLEDK10LEDK10RLK08RLK08LEDK05LEDK05PRX3_avgPRX3_avgPPR08_avgPPR08_avgPRX2_avgPRX2_avgPRX3_dotPRX3_dotMDK5MDK5PRX5_avgPRX5_avgNLEDK50NLEDK50PRX5_degePRX5_degeRCTK09RCTK09CTKCTKPRX5_dotPRX5_dotNLEDK100NLEDK100CTRW100CTRW100AA-lAA-lAAAACN-lCN-lCNCNKTZ001KTZ001SRW2SRW2RARASRW5SRW5PRX5_degPRX5_degPRX3_degPRX3_degRA-lRA-lEDK01EDK01LP05LP05KTZm09KTZm09ACTACTEDK10EDK10BLDBLDPAPA2.4. COMPARAISON EXPÉRIMENTALE

85

Figure 2.13 – Matrice des corrélations de Pearson entres les diﬀérentes mesures,
sur des paires adjacentes de dsV_lcc. Les deux dendrogrammes sont identiques, et
identiques à celui de la ﬁgure 2.12. Ils sont calculés par la méthode dite du saut
maximal.

PA BLD EDK10 ACT KTZm09 LP05 EDK01 RA-l PRX3-deg PRX5-deg SRW5 RA SRW2 KTZ001 CN CN-l AA AA-l CTRW100 NLEDK100 PRX5-dot CTK RCTK09 PRX5-dege NLEDK50 PRX5-avg MDK5 PRX3-dot PRX2-avg PPR08-avg PRX3-avg LEDK05 RLK08 LEDK10 PRX2-dege RCTK07 PRX3-dege MDK2 PRX2-dot LHN1-l NLEDK05 CTRW05 CTRW50 CFL2 CFL3 CFL5 HPI-l LEDK01 CFL2R CFL3R CFL5R LHN1 SRK08 JD JD-l HPI PRX5-cos PRX2-cos PRX3-cos PRX3-cos PRX2-cos PRX5-cos HPI JD-l JD SRK08 LHN1 CFL5R CFL3R CFL2R LEDK01 HPI-l CFL5 CFL3 CFL2 CTRW50 CTRW05 NLEDK05 LHN1-l PRX2-dot MDK2 PRX3-dege RCTK07 PRX2-dege LEDK10 RLK08 LEDK05 PRX3-avg PPR08-avg PRX2-avg PRX3-dot MDK5 PRX5-avg NLEDK50 PRX5-dege RCTK09 CTK PRX5-dot NLEDK100 CTRW100 AA-l AA CN-l CN KTZ001 SRW2 RA SRW5 PRX5-deg PRX3-deg RA-l EDK01 LP05 KTZm09 ACT EDK10 BLD PA 1.00.50.00.51.0RW_dsVadjacentsPearson, saut maximalcorrelation86

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

Nous avons observé que les valeurs de toutes les méthodes locales (sauf LHN1)
ainsi qu’un certain nombre des méthodes globales (ou semi-globales) sont sensibles à
la densité du graphe. Cela ne pose pas forcément de problème dans la pratique, mais
il faut être prudent si l’on souhaite comparer les valeurs de similarités entre plusieurs
graphes. Par exemple, si l’on cherche à mesurer, pour deux sommets présents dans
plusieurs graphes, quel est le graphe dans lequel ils sont les plus proches. En parti-
culier les méthodes (semi-) globales ACT, CTK, EDK*, KTZ001, LP05, PRX*-cos, SRW*
sont à éviter si la sensibilité à la densité a de l’importance.

Aussi nous avons vu que les méthodes ne donnent pas toutes la même importance
aux chemins de longueur 1. Là encore, cela ne pose pas forcément de problème.
Notamment si l’on utilise une mesure uniquement sur des paires non-adjacentes. En
revanche si l’on souhaite utiliser une mesure pour comparer des paires adjacentes
et non-adjacentes, alors il faut être prudent. L’idée introduite avec la conﬂuence,
consistant à retirer du graphe l’arête reliant deux sommets u et v lorsque l’on calcule
leur similarité, fonctionne pour éviter un biais dû à un chemin de longueur 1.

Sur l’exemple d’un graphe réel, nous avons pu observer que les méthodes ont
des comportements remarquables par rapport aux degrés des sommets. Certaines
méthodes ont un biais en faveur des paires de forts degrés, alors que d’autres ont un
biais pénalisant les paires de forts degrés. Il faut faire attention, en particulier, aux
méthodes AA, ACT, BLD, CN, CTRW*, EDK, KTZ*, NLEDK100, PRX*-deg, RA, SRW*
qui privilégient les paires de sommets de forts degrés, et aux méthodes HPI, LHN1,
RCTK*, PRX*-dege qui, elles, pénalisent fortement ces paires de forts degrés. Cette
dernière propriété est certainement très importante dans nombre d’applications.
Pour autant, mis à part quelques exceptions (en particulier [Leicht et al., 2006]),
cette caractéristique est rarement étudiée dans la littérature. Par exemple, Liben-
Nowell et Kleinberg [2007] montrent que la mesure de Katz (KTZ*) donne de bons
résultats pour le problème de prédiction de liens, pour autant ils n’observent pas
qu’elle tend à privilégier les paires de forts degrés (ce qui la rend inutilisable dans
certaines applications).

La mesure de conﬂuence déﬁnie précédemment se distingue de plusieurs manières.
Elle est non corrélée à la densité et a un comportement « équilibré » par rapport
aux degrés des sommets (elle n’avantage ni ne pénalise les sommets de forts degrés).
Cela n’est pas surprenant étant donné la normalisation par rapport au null model
(voir proposition 4). De plus l’astuce consistant à retirer l’arête avant de calculer la
similarité d’une paire adjacente (CFL*R) permet de rendre la mesure insensible aux
chemins de longueur 1. Les seules mesures étudiées présentant des caractéristiques
similaires sont SRK et LHN1. Mais ces deux mesures tendent à pénaliser plus forte-
ment les paires de forts degrés. De plus la conﬂuence est une mesure semi-globale
contrairement à LHN1, et a une complexité plus faible que SRK. Notons tout de
même que le comportement de SRK est remarquable : LHN1 est déﬁnie par rapport

2.5. CONCLUSION DU CHAPITRE

87

au même null model que la conﬂuence par contre rien n’indique cela, a priori, pour
SRK.

Il faut enﬁn remarquer qu’en dehors de ces diﬀérences les mesures sont fortement

corrélées.

2.5 Conclusion du chapitre

Dans ce chapitre, nous avons introduit la conﬂuence, une nouvelle mesure de
similarité entre sommets d’un graphe, basée sur des marches aléatoires en temps
courts. Cette mesure, déﬁnie entre 0 et 1, est normalisée par rapport à la similarité
attendue entre ces deux sommets sur un graphe aléatoire conservant uniquement
les degrés de chaque sommet. Aussi nous proposons d’ignorer volontairement l’arête
liant deux sommets lorsque l’on veut calculer la similarité entre ces sommets.

Nous avons ensuite dressé un état de l’art des mesures de similarité entre som-
mets d’un graphe. Elles se classent en trois catégories : les méthodes locales, globales
et semi-globales. Les méthodes locales n’utilisent que les voisinages immédiats des
sommets alors que les méthodes globales et semi-globales utilisent potentiellement
tout le graphe. Nous avons essayé autant que possible d’indiquer les relations ma-
thématiques pouvant exister entre ces mesures.

Enﬁn nous avons proposé une étude comparative de plus de 80 mesures. Cette
étude consiste à comparer les mesures sur diﬀérentes familles de paires de sommets
sur ces diﬀérents graphes réels ou artiﬁciels. Cette étude permet de mieux com-
prendre les comportements des méthodes, en particulier par rapport à la densité
du graphe, aux chemins de longueur 1 et aux degrés des sommets évalués. Nous
renvoyons à la conclusion faite en section 2.4.6 pour un résumé des résultats.

Nous avons essayé d’être aussi exhaustif que possible dans l’état de l’art proposé.
Toutefois d’autres méthodes pourraient y être ajoutées. En particulier il manque
la mesure globale introduite par Leicht et al. [2006]. Il aurait été d’autant plus
intéressant d’étudier cette méthode, qu’elle est déﬁnie par rapport au même null
model que la conﬂuence (la conﬂuence garde tout de même l’avantage d’être déﬁnie
entre 0 et 1 et d’avoir une complexité plus faible).

Aussi cette comparaison expérimentale n’est qu’une première étape exploratoire.
Cela nous a permis de détecter des propriétés intéressantes qui demanderais main-
tenant à être formalisées et étudiées plus en détail. Cela peut être fait par une étude
analytique, mais aussi par des expérimentations plus ciblées. Par exemple il serait
possible de tracer, pour une série de graphes aléatoires, le nuage de points indiquant
la moyenne des similarités entre sommets non-adjacents et la densité du graphe. Ces
mesures serait plus lisibles que celles que nous avons proposées, à condition de se
limiter à un nombre restreint de mesures.

Nous nous sommes concentrés dans ce chapitre sur des méthodes déﬁnies pour

88

CHAPITRE 2. SIMILARITÉS ENTRE SOMMETS

des graphes non dirigés, connexes et non pondérés. Une perspective évidente est
donc d’étendre ce travail aux graphes non-connexes, pondérés, dirigés, etc.

89

Chapitre 3
Comparaison robuste de graphes

3.1

Introduction

Nous nous intéressons dans ce chapitre à la comparaison de graphes partageant
les mêmes sommets. Comment mesurer à quel point deux graphes G1 = (V, E1) et
G2 = (V, E2) ont une structure similaire ? Bien que limité au cas où les deux graphes
partagent le même ensemble de sommets, ce problème de comparaison répond à des
applications concrètes ; par exemple pour comparer des ressources lexicales déﬁnies
sur le même ensemble de mots. Est-ce que ces deux dictionnaires de synonymie sont
semblables ? Est-ce qu’une ressource construite automatiquement est similaire à ce
dictionnaire étalon ?

Aussi nous allons voir que limiter le problème général de la comparaison de
graphes à ce cas particulier amène à des questions auxquelles, semble-t-il, les mé-
thodes de l’état de l’art ne se sont pas confrontées. Les mesures « classiques »,
ramenées à ce cas simple, consistent toutes plus ou moins à compter le nombre
d’arêtes en accord et en désaccord. Ces méthodes ne prennent donc pas en compte
la structure formée par les arêtes. En eﬀet, deux sommets peuvent être « proches »
dans deux graphes (c’est-à-dire, en substance, être lié par un nombre important de
chemins courts 1) sans pour autant être adjacents dans les deux graphes. De cette
manière deux graphes peuvent comporter exactement, et très clairement, les mêmes
clusters et n’avoir pourtant que peu d’arêtes en commun, et donc être peu similaires
d’après une mesure classique.

Après avoir constaté ce problème nous proposons une solution consistant à gé-
néraliser une distance d’édition de graphes. L’idée est que le coût d’ajout (ou de
suppression) d’une arête dépende de la structure du graphe entre les deux sommets
concernés. Ce coût est calculé grâce à la mesure de conﬂuence déﬁnie au chapitre 2.
Nous présentons une évaluation de la méthode sur diﬀérents graphes artiﬁciels.

1. Nous renvoyons au chapitre 2 pour une discussion plus en détail à propos des mesures de

similarité entre sommets dans un graphe.

90

CHAPITRE 3. COMPARAISON ROBUSTE DE GRAPHES

Nous introduisons aussi une méthode se basant sur la même idée pour fusionner
deux graphes. Le résultat de cette méthode est, par construction, compris entre
l’intersection et l’union des ensembles d’arêtes des deux graphes.

Enﬁn ces méthodes sont employées pour comparer des réseaux de synonymes.
Ce travail, appliqué aux graphes de synonymie, tend à donner un éclairage nouveau
sur la notion de synonymie. En eﬀet nous montrons que plusieurs graphes issus
de dictionnaires papiers diﬀérents, présentent un assez faible accord au niveau des
arêtes alors qu’ils sont fortement en accord d’après notre méthode de comparaison.
Il semble donc plus pertinent de considérer la synonymie pour la similarité qu’elle
peut induire entre les mots, plutôt que comme un « sac » de relations binaires.

Ce travail vient à la suite de plusieurs publications. Dans [Gaillard et al., 2011]
nous nous sommes d’abord interrogés sur ce problème à partir du cas pratique
de comparaison de réseaux lexicaux. Puis dans [Navarro et al., 2012a] nous avons
proposé une analyse plus abstraite du problème. Ce chapitre prolonge ces travaux, en
particulier en introduisant une généralisation de la distance d’édition entre graphes
et en proposant une évaluation plus solide.

3.2 Comparaison de graphes ayant les mêmes som-

mets

Nous nous intéressons donc à la comparaison de deux graphes partageant le
même ensemble de sommets. Dans cette première section, nous déﬁnissons une for-
mulation de la distance d’édition entre graphes, extrêmement simple dans ce cas.
Nous montrons ensuite les problèmes que pose une telle comparaison arête par arête.
Enﬁn, nous présentons un tour de la littérature qui semble indiquer que ce problème
n’a encore jamais été abordé.

3.2.1 Distance d’édition entre graphes (GED)

Une méthode classique pour comparer des graphes est d’utiliser une distance
d’édition [Gao et al., 2010]. Ces distances entre graphes sont inspirées des distances
d’édition déﬁnies originellement entre chaînes de caractères [Levenshtein, 1966]. Une
distance d’édition (« graph edit distance » en anglais) est déﬁnie entre deux graphes
G1 = (V1, E1) et G2 = (V2, E2) comme la série la moins coûteuse de modiﬁcations
permettant de rendre G1 et G2 identiques. Les modiﬁcations sont généralement des
insertions, suppressions ou substitutions de sommets et d’arêtes, appliquées à l’un
des graphes. Le problème est complexe quand il nécessite de trouver un appariement
des sommets (voir sous-section 3.2.3). Dans le cadre de notre travail, puisque V1 =
V2 = V la situation est beaucoup plus simple. En eﬀet, les seules opérations possibles
sont la suppression et l’ajout d’arêtes. Si le coût d’ajout ou de suppression d’une

3.2. COMPARAISON DE GRAPHES AYANT LES MÊMES
SOMMETS
arête est 1, alors la distance entre G1 et G2 vaut :

91

ED(G1, G2) = |E1 ∩ E2| + |E2 ∩ E1|

(3.1)
Remarquons que ED(G1, G2) ∈ [0,|E1| + |E2|]. Aussi on préfère généralement la
mesure normalisée suivante :

GED(G1, G2) = |E1 ∩ E2| + |E2 ∩ E1|

|E1| + |E2|

(3.2)

Cette mesure est bien déﬁnie pour une paire de graphes partageant le même ensemble
de sommets. Sa valeur est comprise entre 0 et 1 : 0 indique un accord total entre
les graphes et 1 un désaccord complet. Cet accord ou désaccord est mesuré, comme
nous allons le voir, au niveau des arêtes.

3.2.2 Structures identiques, pourtant sans « atome » com-

mun

Nous montrons ici qu’une mesure de type GED est insuﬃsante : en eﬀet, deux
graphes peuvent présenter une structure similaire sans pour autant avoir d’arête en
commun.

La ﬁgure 3.1 donne un exemple de deux graphes très similaires mais sans arête
commune. En eﬀet, on remarque que dans le graphe G1 comme dans le graphe G2 les
sommets pairs et les sommets impairs forment deux groupes assez nets qui ne sont
reliés entre eux que par le sommet 1. Les deux graphes sont complètement diﬀérents
au niveau des arêtes. On a donc GED(G1, G2) = 1 la distance maximale, alors que
l’on s’attendrait à une mesure indiquant une assez forte similarité entre ces deux
graphes.

Le problème pointé du doigt ici est que les deux graphes comportent exacte-
ment les mêmes clusters mais construits par des arêtes diﬀérentes. Les graphes sont
donc considérés à tort comme dissemblables. Le désaccord est créé par des paires
de sommets proches dans les deux graphes, mais qui ne sont adjacentes que dans
l’un des graphes. Chacune des paires de sommets en désaccord est considérée indé-
pendamment des autres, indépendamment de la similarité qui peut exister entre ces
sommets.

Il est possible d’imaginer le cas dual, c’est-à-dire un désaccord créé par des paires
de sommets « non-proches » mais quand même adjacents dans l’un des deux graphes.
Le problème se pose en particulier entre deux graphes en accord sur une certaine
structure en cluster mais en conﬂit sur un ensemble d’arêtes aléatoires entre ces
clusters. Ces deux graphes peuvent être considérés comme non similaires à cause
de ces arêtes « entre les clusters », alors même que leurs structures en cluster sont
les mêmes. Pour que le problème apparaisse, il suﬃt d’augmenter suﬃsamment

92

CHAPITRE 3. COMPARAISON ROBUSTE DE GRAPHES

(a) graphe G1

(b) graphe G2

Figure 3.1 – Exemple jouet de deux graphes similaires pourtant sans arête com-
mune.

le nombre de clusters. En eﬀet, le nombre d’arêtes entre les clusters peut devenir
(largement) plus grand que le nombre d’arêtes à l’intérieur, sans pour autant que la
densité de ces arêtes aléatoires ne rende illisibles les clusters. Prenons par exemple
deux graphes formés de 1 000 cliques identiques de 5 sommets. Cela fait donc 10 000
arêtes « intra-cliques » pour 5 000 sommets. On peut imaginer sur chacun de ces
graphes avoir 100 000 arêtes aléatoires entre ces diﬀérentes cliques. La probabilité
qu’il existe un lien entre deux sommets est donc inférieure à 0.01, alors que la
probabilité d’avoir un lien entre deux sommets du même cluster est de 1. Les cliques
sont donc toujours très nettes, bien que chaque sommet ait plus de voisins avec le
« reste du monde » qu’avec les sommets de sa clique. Ces deux graphes comparés
avec GED apparaîtront distants, car ils n’ont que 10% de leurs arêtes en commun,
pourtant leurs structures, formées des cliques, sont les mêmes.

Dans un premier cas, un désaccord est considéré alors que les paires sont proches
dans chacun des graphes d’après leurs structures. Dans le second cas un désaccord
est compté alors que dans les deux graphes les paires sont distantes. Les deux si-
tuations problématiques présentées ici proviennent du fait qu’une mesure de type
GED considère les arêtes comme indépendantes les unes des autres sans considérer
la structure qu’elles dessinent. Nous présentons en section 3.3 une analyse plus ﬁne
de ce problème, avant de présenter une généralisation de GED qui y réponde.

1357911131502681012141357911131502681012143.2. COMPARAISON DE GRAPHES AYANT LES MÊMES
93
SOMMETS
3.2.3 De l’isomorphisme de graphes aux distances d’édition
Avant de proposer une méthode qui réponde aux problèmes exposés, nous reve-
nons ici sur les méthodes existantes de comparaison de graphes. Il existe, en eﬀet,
une littérature importante sur la comparaison de graphes.

L’approche classique quand on cherche à comparer deux graphes est la recherche
d’isomorphisme. Deux graphes G1 = (V1, E1) et G2 = (V2, E2) sont isomorphes s’il
existe une fonction bijective f : V1 7→ V2 telle que :

{u, v} ∈ E1 ⇔n

f(u), f(v)o ∈ E2

∀u, v ∈ V,

(3.3)

Dans la pratique il existe rarement un isomorphisme parfait entre deux graphes
G1 et G2. On cherche alors à déterminer à quel point un isomorphisme approché
existe entre G1 et G2 ou encore s’il existe un sous-graphe de G1 isomorphe (ou
quasi-isomorphe) à G2.

Un nombre important de méthodes a été proposé pour répondre à ces problèmes.
La plupart consistent à calculer une distance d’édition entre graphes. C’est-à-dire à
chercher le coût minimal des éditions nécessaires pour passer de l’un des graphes à
l’autre. Les opérations généralement disponibles sont l’insertion, la substitution et la
suppression de sommets et d’arêtes. Les méthodes prennent ou non en considération
des attributs sur les sommets et arêtes. Elles varient aussi dans la manière de déﬁnir
les coûts des éditions et d’explorer l’espace des solutions. Pour plus de détails sur
ces méthodes nous renvoyons à la revue proposée par Gao et al. [2010].

Il est important de remarquer que le cas typique d’utilisation des méthodes de
comparaison de graphes de la littérature est très diﬀérent de ce que nous cherchons
à faire ici. En eﬀet, la comparaison de graphes est souvent employée pour répondre
à un problème de reconnaissance de forme. Typiquement pour rechercher, dans une
collection de graphes, ceux similaires à un graphe requête donné. Les graphes en-
codent par exemple des molécules [He et Singh, 2006; Jiang et al., 2007] ou des
images [Sanfeliu et Fu, 1983; Robles-Kelly et Hancock, 2005]. Ils sont de taille mo-
deste (quelques dizaines ou centaines de sommets) et leurs sommets sont « inter-
changeables » (c’est-à-dire ils n’ont pas d’étiquette, ou alors chaque étiquette est
partagée par un ensemble de sommets). La principale diﬃculté est alors de chercher
un appariement (qui peut être implicite) des sommets qui minimise la distance. Le
fait d’avoir un appariement imposé est donc un cas particulier pour les approches de
la littérature. De plus ce cas ne correspond pas aux situations d’applications usuelle-
ment visées par les méthodes. Aussi la plupart des méthodes sont soit inapplicables
(car il est impossible de forcer un appariement des sommets) soit deviennent triviales
et souﬀrent du même problème que la mesure GED déﬁnie précédemment.

Notons que les méthodes basées sur des noyaux (voir par exemple [Gärtner et al.,
2003]), restreintes à notre cas particulier, répondent certainement aux problèmes
que nous avons soulevés dans la sous-section précédente. Mais ni ces problèmes, ni

94

CHAPITRE 3. COMPARAISON ROBUSTE DE GRAPHES

le fait que les méthodes puissent (ou non) y répondre ne sont mis en avant dans la
littérature. Aussi l’un des avantages de la méthode que nous proposons, par rapport à
ce que pourrait donner une méthode basée sur un noyau, est que la valeur obtenue est
comparable avec la valeur de GED déﬁnie par l’équation (3.2). Ainsi il est possible
de comparer la similarité « structurelle » des deux graphes à leur accord au niveau
des arêtes (GED). Notons aussi que la complexité de nos méthodes est plus faible
car elle ne nécessite un calcul que sur les paires en désaccord et non sur l’ensemble
des paires de sommets. Nous y reviendrons en conclusion, mais il serait certainement
pertinent d’explorer le parallèle entre les méthodes à noyaux et l’approche que nous
développons ici.

3.3 Proposition d’une mesure de comparaison ro-

buste

Nous proposons tout d’abord dans cette section une analyse plus ﬁne des pro-
blèmes soulevés en sous-section 3.2.2. Cette analyse nous amène à proposer une
version plus robuste de la distance d’édition permettant de comparer deux graphes
partageant les mêmes sommets. L’idée suivie dans cette section consiste à ramener
au niveau des paires de sommets une information sur la topologie plus large du
graphe, cela en utilisant une mesure de similarité entre sommets. Nous évaluons
ensuite cette méthode sur diﬀérents jeux de graphes artiﬁciels.

3.3.1 Similarité binaire, diﬀérents cas de ﬁgure

Les deux cas problématiques soulevés en section 3.2.2 résultent du fait que les
arêtes sont considérées indépendamment les unes des autres. En eﬀet, GED suppose
qu’un graphe n’est qu’un ensemble de jugements binaires indépendants : soit deux
sommets sont en relation, soit ils ne le sont pas. Or, comme nous l’avons vu en sous-
section 3.2.2, deux sommets peuvent être proches sans être adjacents. Intuitivement,
s’il existe beaucoup de chemins courts entre deux sommets, la situation n’est pas
la même que s’il n’existe que peu de chemins (longs) entre ces sommets. Pourtant
ces deux situations sont considérées comme équivalentes avec une mesure de type
GED.

Pour comprendre ces diﬀérents cas, supposons que l’on dispose d’une mesure
binaire évaluant si deux sommets sont « proches » (i.e. similaires) d’après la topologie
du graphe, et ce indépendamment du fait qu’ils soient adjacents ou non. Pour chaque
paire de sommets, quatre cas sont possibles :
• adjacente et similaire (on note « 11 »),
• adjacente mais non-similaire (« 10 »),

3.3. PROPOSITION D’UNE MESURE DE COMPARAISON
ROBUSTE

95

• non-adjacente mais similaire (« 01 »),
• non-adjacente et non-similaire (« 00 »).

On comprend que les situations qui posaient problème jusque là correspondent au cas
où une paire est adjacente (et a priori similaire) dans un graphe et non-adjacente
mais similaire dans l’autre, et au cas où une paire est non-adjacente (et a priori
non-similaire) dans un graphe et adjacente mais non-similaire dans l’autre.

Les tableaux 3.1 montrent le changement de perspective qui s’opère lorsque l’on
considère une similarité binaire, en plus de la notion d’adjacence. Quand on ne
considère que l’adjacence, le tableau 3.1a présente les quatre cas possibles pour une
paire donnée. Le tableau 3.1b présente les seize cas existant lorsque l’on considère, en
plus, une telle similarité binaire. Sur le tableau 3.1a les interprétations (ok = accord,
ko = désaccord) des quatre cas sont peu discutables. Par contre, pour évaluer les
seize cas du tableau 3.1b des choix doivent être faits. Les qualiﬁcations présentées
correspondent aux choix suivants :

• priorité de l’adjacence sur la similarité,
• un conﬂit sur l’adjacence est « résolu » s’il n’y a pas de conﬂit sur la similarité.
Une façon de comprendre ces choix est de considérer que la similarité vient soit
conﬁrmer soit mettre en doute l’adjacence. Ainsi on a pour une paire :
• adjacente mais non-similaire (10) : doute sur l’adjacence (noté 1?),
• non-adjacente mais similaire (01) : doute sur la non-adjacence (0?),
• adjacente et similaire (11) : certitude sur l’adjacence (1!),
• non-adjacente et non-similaire (00) : certitude sur la non-adjacence (0!).

Ainsi, tant qu’une arête est présente (ou absente) dans les deux graphes, peu importe
si il y a un doute ou non, il n’y a pas de conﬂit (ok+ ou ok−). Aussi lors d’un conﬂit

Table 3.1 – Diﬀérents cas possibles lors de la comparaison d’une paire de sommets
entre deux graphes et proposition de qualiﬁcation des conﬂits. En (a) on ne considère
que l’adjacence (1 : la paire est adjacente, 0 : elle ne l’est pas). En (b) on considère l’adjacence
et une similarité binaire entre sommets (11 : paire adjacente et similaire, 10 : adjacente mais
non similaire, 10 : non adjacente mais similaire, 00 : non adjacente et non similaire).
(a) on ne considère que l’adjacence.

(b) on considère l’adjacence et une similarité bi-
naire.

1
ok+
ko

0
ko
ok−

1
0

11
1!
ok+
ok+
[ok+]
ko

10
1?
ok+
ok+
ko?
[ok−]

01
0?
[ok+]
ko?
ok−
ok−

00
0!
ko
[ok−]
ok−
ok−

11
10
01
00

1!
1?
0?
0!

96

CHAPITRE 3. COMPARAISON ROBUSTE DE GRAPHES

(paire adjacente d’un coté seulement) si la paire est en doute sur l’un des graphes
mais conﬁrmée sur l’autre alors le conﬂit est évité (cas [ok+] et [ok−]). Il reste alors
deux cas de conﬂit :

• ko : 11 face à 00 (i.e. 1! face à 0!) on parle de conﬂit fort,
• ko? : 10 face à 01 (i.e. 1? face à 0?) on parle de conﬂit faible.

Notons qu’il peut être pertinent de ne pas considérer comme désaccord les conﬂits
faibles. En eﬀet, avec l’interprétation proposée, ce cas correspond au fait que les
deux graphes présentent un doute. On ne sait donc pas s’il doit y avoir une arête ou
non, mais les deux graphes sont prêts à « faire la concession ».

Il faut remarquer que l’interprétation inverse consistant à considérer la simila-
rité prioritaire sur l’adjacence amène au même résultat. Deux paires similaires de la
même manière dans les deux graphes ne seraient pas en conﬂit et l’adjacence vien-
drait résoudre les conﬂits entre paires similaires seulement dans un des graphes. Une
dernière solution serait de ne considérer que la similarité en ignorant l’adjacence. Le
problème qui se pose alors est que le désaccord de similarité entre les paires non-
adjacentes peut « écraser » tout le reste. En eﬀet le nombre de paires non-adjacentes
est souvent de plusieurs ordres de grandeur plus grand que le nombre d’arêtes. Les
conﬂits de structure qui peuvent apparaître sur l’ensemble de ces paires, même peu
nombreux en proportion, dominent complètement la comparaison. Il faudrait sinon
reprendre la mesure pour la normaliser par le nombre total de paires de sommets.
Ainsi en considérant une similarité binaire, qui vient conﬁrmer ou remettre en
cause l’adjacence, on observe que les deux exemples problématiques illustrés en
section 3.2.2 peuvent être résolus. En eﬀet le premier cas (ﬁgure 3.1) correspond à
une comparaison où beaucoup des paires sont dans la situation 1! face à 0?. Alors
que dans le second cas, la comparaison est faussée par un nombre important de
paires 1! face à des paires 0?. Une similarité binaire peut donc être utilisée de la
sorte pour proposer une comparaison robuste entre graphes. C’est le travail qui a
été proposé dans [Navarro et al., 2012a]. Nous nous limitons ici à exploiter cette idée
pour comprendre le problème, nous proposons en eﬀet dans la suite une méthode
utilisant une similarité continue plutôt que binaire.

Similarité seulement pour les paires non-adjacentes ? Il est possible de choi-
sir de ne considérer la similarité que pour les paires non-adjacentes (ou symétrique-
ment seulement sur les paires adjacentes). Cela revient à s’interdire de remettre en
question les arêtes (ou les non-arêtes). Les tableaux 3.2 présentent ces deux situa-
tions. Dans certains cas spéciﬁques, il peut être intéressant de limiter la considération
d’une similarité aux seules paires adjacentes, mais en pratique c’est surtout la mise
en cause des paires non-adjacentes qui est intéressante. Il peut y avoir plusieurs rai-
sons à cela. Tout d’abord on peut disposer d’une connaissance extérieure qui indique

3.3. PROPOSITION D’UNE MESURE DE COMPARAISON
ROBUSTE

97

Table 3.2 – Comparaison d’une paire de sommets entre deux graphes quand une
similarité est considérée seulement sur les paires non-adjacentes ou seulement sur
les paires adjacentes.

(a) Seules les paires non-adjacentes peuvent
être mises en doute par la similarité.
00
0!
ko
ok−
ok−

1
1!
ok+
[ok+]
ko

01
0?
[ok+]
ok−
ok−

1
01
00

1!
0?
0!

(b) Seules les paires adjacentes peuvent être
mises en doute par la similarité.

11
1!
ok+
ok+
ko

10
1?
ok+
ok+
[ok−]

0
0!
ko
[ok−]
ok−

11
10
0

1!
1?
0!

que les arêtes sont toutes correctes, et que donc les conﬂits ne peuvent être minimisés
même lorsque c’est une arête entre deux sommets éloignés qui pose problème. Une
autre raison, plus pragmatique, est que ces cas (paire 1? face à 0!) sont rares. En
eﬀet les graphes réels contiennent peu d’arêtes mises en doute par la topologie, et
peu de celles-ci correspondent à des paires non-adjacentes dans l’autre graphe. Enﬁn
pour résoudre le premier problème présenté en section 3.2.2, il suﬃt de prendre en
compte une similarité seulement pour les paires non-adjacentes. Dans la pratique, ce
problème est plus fréquent que le second. Notons qu’alors il n’existe plus de conﬂit
faible. Dans la suite nous nous limiterons donc à calculer une similarité sur les paires
non-adjacentes lors d’un conﬂit.

Une autre façon de comprendre ces approches « partielles » est de considérer que
les graphes sont corrects mais incomplets ou complets mais incorrects. En eﬀet si
l’on considère qu’un graphe est un ensemble d’arêtes, alors dire qu’un graphe est
correct signiﬁe que toutes ses arêtes sont justes et ne peuvent être remises en cause,
par contre il est possible qu’il en manque. À l’inverse dire qu’un graphe est complet
signiﬁe que toutes les arêtes sont bien présentes, par contre il peut y en avoir « en
trop ». Quand on ne considère la similarité que pour les paires non-adjacentes, alors
on autorise la topologie à remettre en cause seulement des paires non-adjacentes.
Les arêtes sont considérées comme correctes dans tout les cas.

3.3.2 Similarité continue, généralisation de GED

Nous avons vu que les limites d’une distance d’édition simple peuvent être dépas-
sées en ajoutant au niveau des paires de sommets une information sur la topologie
plus large du graphe. Nous explorons dans cette sous-section une généralisation de la
distance d’édition faisant intervenir une similarité prenant une valeur réelle entre 0
et 1. L’idée consiste simplement à calculer le coût d’une édition élémentaire grâce à la
mesure de similarité. Ainsi les éditions en contradiction avec la structure du graphe
coûteront cher, alors que celles en accord avec la structure seront peu coûteuses.

98

CHAPITRE 3. COMPARAISON ROBUSTE DE GRAPHES

Nous supposons donc avoir une mesure de similarité ∆ comprise entre 0 et 1. Une
paire de sommets éloignés d’après la structure du graphe (adjacents ou non) aura
une valeur proche de 0, à l’inverse une paire de sommets proches aura une valeur
proche de 1. Nous présentons dans la sous-section 3.3.3 les qualités nécessaires de
cette similarité. Cette mesure nous permet de déﬁnir une fonction de coût pour les
deux opérations possibles : l’insertion d’une arête (add) ou la suppression d’une arête
(del). Pour un graphe G = (V, E), nous déﬁnissons ces deux fonctions ainsi :

∀{u, v} ∈ E,
∀{u, v} ∈ E,

delG(u, v) = ∆G(u, v)
addG(u, v) = 1 − ∆G(u, v)

(3.4)
(3.5)

La distance d’édition consiste à mesurer le coût minimal des modiﬁcations à
apporter à G1 et à G2 pour les rendre identiques. La formulation donnée par l’équa-
tion (3.2) se généralise donc de la manière suivante :

  X

{u,v}∈E1∩E2

min(cid:16)

1

|E1| + |E2|

delG1(u, v), addG2(u, v)(cid:17)
+ X

min(cid:16)

{u,v}∈E1∩E2

addG1(u, v), delG1(u, v)(cid:17)!

(3.6)

Chaque arête présente uniquement dans l’un des deux graphes doit être, soit ajoutée
à l’un des graphes, soit supprimée de l’autre graphe. On peut vériﬁer que si les valeurs
de add et del sont toujours égales à 1 on retombe sur l’équation (3.2).

Il est possible d’établir un parallèle avec l’approche présentée précédemment si

l’on considère les correspondances suivantes :
• paire « 00 » (ou « 0! ») : add(u, v) = 1,
• paire « 01 » (ou « 0? ») : add(u, v) = 0,
• paire « 10 » (ou « 1? ») : del(u, v) = 0,
• paire « 11 » (ou « 1! ») : del(u, v) = 1.

Ainsi, dans le cas d’une similarité binaire, cette généralisation de GED revient à ne
compter les conﬂits que dans les cas où une paire « 0! » est face à une paire « 1! ».
Les conﬂits dit faibles (« 0? » face à « 1? ») ne sont pas comptés.

Seulement des ajouts. Comme nous l’avons vu précédemment il peut être sou-
haitable de ne considérer la similarité que sur les paires non-adjacentes. Cela permet
en particulier de dépasser la principale des limites de GED (voir ﬁgure 3.1) sans
avoir à calculer de similarité pour chacune des arêtes. Ici cela revient à interdire la
suppression d’arête ou, ce qui est équivalent, à décider que le coût d’une suppression

3.3. PROPOSITION D’UNE MESURE DE COMPARAISON
ROBUSTE

99

est toujours supérieur au coût d’un ajout dans l’autre graphe. On arrive alors à la
mesure suivante :

GU D(G1, G2) =

1

|E1| + |E2|

  X

addG2(u, v) + X

u,v∈E1∩E2

u,v∈E1∩E2

!

addG1(u, v)

(3.7)

Nous nommons GU D cette mesure, pour graph union dissimilarity. En eﬀet cette
mesure correspond au coût normalisé nécessaire pour transformer G1 et G2 en leur
union.

Coût du désaccord. La mesure GU D donne donc une distance d’édition entre
deux graphes qui est robuste aux désaccords quand ceux-ci sont résolus par l’ajout
peu coûteux d’une arête. Toutefois il peut être utile, lorsque l’on compare deux
graphes, de savoir si le désaccord provient d’un faible nombre d’arêtes en conﬂit
ou alors de nombreux conﬂits peu coûteux à résoudre. Pour cela nous proposons la
mesure suivante :

1

  X

addG2(u, v)+ X

GU C(G1, G2) =

|E1 ∩ E2| + |E1 ∩ E2|

addG1(u, v)
(3.8)
GU C signiﬁe Graph Union Cost, c’est donc le coût moyen de résolution des conﬂits.
On observe que :

u,v∈E1∩E2

u,v∈E1∩E2

!

GU D(G1, G2) = GED(G1, G2).GU C(G1, G2)

(3.9)

Enﬁn, notons que pour comprendre le désaccord entre deux graphes, il est pos-
sible de tracer la distribution des coûts d’ajout des arêtes en conﬂit. C’est-à-dire
d’aﬃcher combien d’arêtes peuvent être ajoutées pour chaque plage de « prix »
(entre 0 et 1). Une telle courbe permet d’observer la réparation des paires en conﬂit,
par rapport à leur similarité dans le graphe où elles ne sont pas adjacentes.

3.3.3 Choix de la similarité ∆

Le choix de la similarité utilisée est primordial. Il convient ici de contraindre
un peu mieux cette mesure. Nous avons déjà supposé que cette similarité prend
valeur entre 0 et 1. De plus, il faut que la mesure ne soit pas inﬂuencée par certaines
propriétés globales du graphe. En particulier il est souhaitable que la similarité ne
soit pas sensible à la densité. Nous avons vu au chapitre précédent que c’était le
cas de certaines mesures. Par exemple pour le cosinus entre vecteurs d’adjacence :
plus le graphe est dense plus cette similarité tend à être importante. Sur un graphe
complet le cosinus entre n’importe quelle paire de sommets vaut 1.

100

CHAPITRE 3. COMPARAISON ROBUSTE DE GRAPHES

Null model. Une manière eﬃcace de contraindre la similarité utilisée pour GU D
est de préciser qu’elle ne doit pas mesurer simplement à quel point deux sommets
u et v sont proches, mais à quel point u et v sont plus proches qu’ils ne le seraient
dans un graphe aléatoire équivalent. On appelle null model 2 ce graphe aléatoire
équivalent. L’utilisation d’un tel null model permet de se « protéger » des propriétés
du graphe aussi présentes dans le null model. En eﬀet si deux sommets ont une
forte similarité dans le graphe et dans le null model alors cette similarité n’est pas
due aux propriétés particulières du graphe. Par exemple si le null model a la même
densité que le graphe, une similarité forte causée par la densité sera forte aussi
dans le null model. Un null model classique dans l’étude des grands graphes de
terrain est celui utilisé par la modularité [Newman, 2003b]. Il consiste en un graphe
aléatoire où chaque sommet possède le même degré que dans le graphe étudié. Ce
modèle correspond au modèle de conﬁguration 3 avec une séquence de degrés imposée
par le graphe étudié. Ainsi la densité du graphe et la distribution des degrés sont
conservées, en revanche la structure qui nous intéresse ici, c’est-à-dire les clusters ou
« zones denses », est perdue.

Conﬂuence. Aﬁn de respecter ces contraintes, nous proposons d’utiliser la mesure
de conﬂuence introduite au chapitre précédent (voir section 2.2.2). Voici un rappel
de sa formulation :

conﬂt(u, v) =

(3.10)

[P t]u,v

[P t]u,v + πv

où [P t]u,v est la probabilité d’atteindre un sommet v en partant de u en t pas
aléatoires sur le graphe, et πv la probabilité d’atteindre v en un temps inﬁni. En
eﬀet nous avons vu (cf. proposition 4) que πv est aussi l’espérance de passer de u à
v en un nombre quelconque de pas dans le null model que nous souhaitons utiliser.
Nous utilisons l’astuce consistant à ajouter une boucle sur chaque sommet du
graphe avant d’eﬀectuer le calcul de la conﬂuence. Aussi pour comparer deux graphes
non-connexes nous calculons la limite πv comme si les graphes étaient connexes. (voir
section 2.2.2 pour plus de détails).

3.3.4 Évaluation

Nous présentons ici une évaluation de GU D sur plusieurs graphes artiﬁciels. Nous
utilisons pour cela des graphes artiﬁciels construits à partir du modèle de graphes
CGM présenté au chapitre précédent (sous-section 2.4.1). Pour rappel ces graphes
sont formés de k groupes de r sommets, avec une probabilité µintra qu’une arête

2. On pourrait traduire null model par « modèle de référence » en français.
3. Ce modèle consiste en un graphe aléatoire où les degrés de chaque sommet sont ﬁxés, nous
renvoyons à [van der Hofstad, 2013, chap. 7] pour plus de détails sur le modèle de conﬁguration.

3.3. PROPOSITION D’UNE MESURE DE COMPARAISON
ROBUSTE

101

existe entre deux sommets du même groupe, et µinter entre des sommets de groupes
diﬀérents.

L’objectif de cette première évaluation est de vériﬁer le comportement de GU D

dans les quatre cas suivants :

(i) graphe CGM face à un autre graphe CGM avec les mêmes clusters,
(ii) graphe CGM face à un autre graphe CGM mais avec des clusters diﬀérents,
(iii) graphe CGM face à un graphe aléatoire de type Erdős-Rényi,
(iv) graphe aléatoire de type Erdős-Rényi face à un autre graphe aléatoire de type

Erdős-Rényi.

Dans le cas (i) la valeur de GU D doit a priori être faible alors que celle de GED
ne l’est pas forcément. Dans les trois autres cas, GU D et GED doivent détecter la
non-similarité des deux graphes, c’est-à-dire présenter des valeurs fortes.

3.3.4.1 Cas (i) : Graphes issus du même modèle

Les courbes de la ﬁgure 3.2 présentent les résultats de comparaison entre graphes
CGM construits avec les mêmes paramètres (et les mêmes groupes de sommets). Les
graphes sont tous construits avec r = 50 et k = 5. Pour la ﬁgure 3.2a µinter = 0.01
et µintra varie entre 0 et 1 ; à l’inverse sur la ﬁgure 3.2b µintra = 0.5 et µinter varie
entre 0 et 1. Nous sommes donc dans une situation où les sommets appartiennent
aux mêmes clusters dans chaque graphe, mais par contre les liens constituant ces
clusters, étant aléatoires, ne sont a priori pas les mêmes.

Analyse des résultats des ﬁgures 3.2. On observe tout d’abord, sur la ﬁ-
gure 3.2a, que lorsque µintra vaut 0 alors GED = 1 et GU D est très proche de 1.
En eﬀet dans ce cas les clusters ne sont pas du tout déﬁnis, les seules arêtes du
graphe sont complètement aléatoires et la probabilité qu’elles soient en accord est
faible. Ensuite lorsque µintra augmente, GU D diminue nettement. On a donc bien
le comportement souhaité : GU D détecte que les graphes sont proches même si la
proportion d’arêtes en commun en faible. Pour µintra = 1, chaque cluster est une
clique, donc parfaitement en accord entre les deux graphes.

Sur la ﬁgure 3.2b, le nombre de liens aléatoires entre les clusters augmente jusqu’à
ce que les deux graphes deviennent presque 4 complets. Au départ GU D détecte
que les deux graphes sont très similaires (alors que GED indique que seulement la
moitié environ des arêtes sont communes). Ensuite GED et GU D augmentent à
cause du désaccord créé par les arêtes inter-cluster, mais arrivées à un certain seuil
(environ 0.2) les deux mesures décroissent. En eﬀet le nombre important des arêtes
inter-clusters fait que ces arêtes commencent à être en accord. Les deux graphes
convergent vers un même graphe quasiment complet.

4. En eﬀet, comme µintra = 0.5, les graphes ne sont jamais complets même quand µinter = 1.

102

CHAPITRE 3. COMPARAISON ROBUSTE DE GRAPHES

(a) µintra varie et µinter = 0.01.

(b) µintra = 0.5 et µinter varie.

Figure 3.2 – Évaluation de GU D sur diﬀérentes paires de graphes CGM. Sur toutes
les courbes, les deux graphes comparés sont issus du même modèle de graphe CGM avec
m = 50 et k = 5. GU D est calculée avec t = 4. Chaque point donne la moyenne pour 20
réalisations, l’écart-type (faible) est donné par les barres verticales associés à chaque point
(presque invisibles).

3.3.4.2 Cas (ii), (iii) et (iv) : graphes non similaires

Pour les diﬀérentes courbes de la ﬁgure 3.3 nous nous plaçons initialement dans
le cas limite de la section 3.2.2. En eﬀet les deux graphes comparés sont construits à
partir d’un graphe CGM « coupé en deux ». C’est-à-dire la moitié de ses arêtes est sé-
lectionnée au hasard pour former le premier graphe Ga ; les arêtes restantes forment
le second graphe Gb. Ainsi ces deux graphes n’ont aucune arête en commun, pour-
tant les groupes de sommets (les clusters) sont les mêmes. Le graphe de départ est
construit avec les paramètres suivants : k = 5, m = 50, µintra = 0.5 et µinter = 0.01.
Ces deux graphes correspondent à des graphes CGM construits avec des probabili-
tés µintra et µinter divisées par deux. Mais nous nous plaçons ainsi artiﬁciellement
dans le pire des cas, puisque les graphes n’ont aucune arête en commun. L’un de
ces graphes (ou les deux) est alors transformé soit par un re-câblage aléatoire des
arêtes, soit par une permutation des sommets.
Permutation des sommets. Soit X ⊆ V un ensemble de (1 − τ).|V | sommets
choisis aléatoirement et σ : V \ X 7→ V \ X une permutation aléatoire des sommets
de V non présents dans X. On déﬁnit alors f : V 7→ V une bijection de V sur V
telle que : si u ∈ X alors f(u) = u et si u /∈ X alors f(u) = σ(u). On peut ainsi
déﬁnir le graphe τ•

G= (V,

τ•
E) tel que :
τ•

E=n{f(u), f(v)}, {u, v} ∈ E

o

(3.11)

Pour tout τ ∈ [0, 1], τ•

G est donc isomorphe à G par f. Si τ = 0 alors τ•

G= G.

0.00.20.40.60.81.0µintra0.00.20.40.60.81.0nbrepet=20 n=50 k=5 p_intra=1.0 p_inter=0.01 abcisse=p_intra b1=0.0 b2=1.01 pas=0.1 t=4 we=True   GEDGUD0.00.20.40.60.81.0µinter0.00.20.40.60.81.0nbrepet=20 n=50 k=5 p_intra=0.5 p_inter=1.0 abcisse=p_inter b1=0.0 b2=1.01 pas=0.1 t=4 we=True   GEDGUD3.3. PROPOSITION D’UNE MESURE DE COMPARAISON
ROBUSTE

103

(a) Ga face à τ•
Gb. Une proportion τ des
sommets de Gb est renommée aléatoire-
ment.

(b) Ga face à τ ,→
Gb. Une proportion τ des
arêtes de Gb est re-câblée aléatoirement.

Ga face à τ ,→

(c) τ ,→
Gb. Une proportion τ des
arêtes de chaque graphe est re-câblée
aléatoirement.

Figure 3.3 – Évaluation de GU D sur diﬀérentes paires de graphes aléatoires. Sur
toutes les courbes, pour τ = 0 les deux graphes sont issus d’un graphe CGM « coupé en
deux ». Ensuite l’un des graphes ou les deux sont transformés. Le graphe CGM de départ est
construit avec les paramètres suivants : k = 5, m = 50, µintra = 0.5 et µinter = 0.01. GU D
est calculée avec t = 4. Chaque point donne la moyenne pour 20 réalisations, l’écart-type
(faible) est donné par les barres verticales associés à chaque point.

0.00.20.40.60.81.0τ0.00.20.40.60.81.0G1 vs. τ•G2 GEDGUD0.00.20.40.60.81.0τ0.00.20.40.60.81.0G1 vs. τG2 GEDGUD0.00.20.40.60.81.0τ0.00.20.40.60.81.0τG1 vs. τG2 GEDGUD104

CHAPITRE 3. COMPARAISON ROBUSTE DE GRAPHES

Re-câblage des arêtes. Soit A ⊆ E un ensemble de (1 − τ).|E| arêtes choisies
aléatoirement, et B un second ensemble de τ.|E| paires de sommets choisies aléa-
toirement et non présentes dans A. C’est-à-dire B ⊆ PV2 \ A, avec PV2 l’ensemble
des parties de taille deux de V . On construit alors le graphe τ ,→
τ ,→
E ) tel que
τ ,→
E = A ∪ B. Alors τ ,→
G a autant d’arêtes que G, mais τ.|E| de ses arêtes sont aléa-
toires, les (1− τ).|E| restantes sont communes. Quand τ = 1 alors 1,→
G est un graphe
aléatoire de type Erdős-Rényi ayant la même densité que G.

G = (V,

En ﬁgure 3.3a, l’un des graphes n’est pas transformé, l’autre subit une permu-
tation des sommets. Sur la ﬁgure 3.3b, les arêtes de l’un des deux graphes sont
re-câblées aléatoirement, l’autre graphe reste inchangé. Enﬁn pour la ﬁgure 3.3c, les
arêtes des deux graphes sont re-câblées aléatoirement. Dans les trois cas l’axe des
abscisses représente τ, c’est-à-dire soit la proportion de sommets inter-changés, soit
la proportion d’arêtes re-câblées.

Analyse des résultats de la ﬁgure 3.3. Pour τ = 0, c’est-à-dire quand on
compare deux graphes CGM ayant les mêmes clusters mais aucune arête en commun
(GED = 1), on observe que GU D vaut un peu plus de 0.2. La similarité de ces
graphes initiaux est donc bien repérée. Ensuite on observe que sur les trois courbes
la valeur de GU D augmente quand τ croît. C’est un comportement souhaitable car
dans les trois cas la dose d’aléatoire introduite rend petit à petit les deux graphes
« étrangers ». Notons qu’à l’inverse la valeur de GED diminue. Cela n’est pas sur-
prenant, car au ﬁl des transformations certaines arêtes deviennent par hasard en
accord. C’est sur la ﬁgure 3.3a que la valeur de GU D devient la plus forte (supé-
rieure à 0.6). Sur la ﬁgure 3.3b elle vaut environ 0.6 pour τ = 1 et elle vaut environ
0.5 pour τ = 1 sur la ﬁgure 3.3c. Ce comportement est souhaitable, en eﬀet, pour
τ = 1 sur la ﬁgure 3.3a, les deux graphes ont des structures en cluster mais ces
structures sont indépendantes l’une de l’autre, le désaccord coûte donc cher. Sur la
ﬁgure 3.3b le désaccord est un peu plus faible, car sur le graphe devenu aléatoire, les
arêtes manquantes sont moins chères à ajouter (conﬂuence proche de 0.5). Lorsque
les deux graphes sont aléatoires (ﬁgure 3.3c pour τ = 1) les arêtes à ajouter ont
toutes une conﬂuence proche de 0.5, GU D est donc proche de 0.5.

3.3.4.3 Conclusion de l’évaluation

Nous avons montré que la mesure GU D se comporte bien comme attendu sur
une série d’expériences avec des graphes artiﬁciels. En particulier deux graphes com-
portant les mêmes clusters apparaissent bien comme similaires même si ces clusters
ne sont pas formés des mêmes arêtes. Nous avons aussi pu vériﬁer que GU D indique
correctement la dissimilarité de deux graphes soit aléatoires, soit ayant des clusters
diﬀérents.

3.4. FUSION DE GRAPHES DE TERRAIN

105

Cette évaluation pourrait être poursuivie sur des graphes aléatoires ayant des
structures en clusters plus réalistes. En particulier en utilisant le modèle de graphes
LFR proposé par Lancichinetti et al. [2008] pour comparer des méthodes de clustering
de graphe. Ces graphes permettent en eﬀet d’imposer une distribution des degrés en
loi de puissance, ainsi qu’une répartition de la taille des clusters en loi de puissance.

3.4 Fusion de graphes de terrain

En présence de deux graphes sur les mêmes sommets, il peut être utile des les
regrouper en un seul. En se basant sur cette idée consistant à ramener au niveau
des arêtes (en conﬂit) une information sur la topologie du graphe, nous proposons
ici une méthode permettant de fusionner deux graphes.

Lorsque l’on veut fusionner deux graphes ayant les mêmes sommets, deux opé-
rations évidentes sont envisageables : calculer l’union ou l’intersection des arêtes.
L’union correspond à une démarche optimiste : une arête est « acceptable » du mo-
ment qu’elle est présente dans l’un des deux graphes. À l’inverse, l’intersection est
une approche pessimiste : une arête n’est « acceptable » que si elle est présente dans
les deux graphes. Nous proposons une méthode intermédiaire qui consiste à accepter
les arêtes présentes dans les deux graphes ainsi que les arêtes présentes seulement
dans un des graphes si la similarité de la paire correspondante dans l’autre graphe est
supérieure à un certain seuil. Ce seuil peut être diﬀérent dans chacun des graphes.
On note Eτ1,τ2 l’ensemble d’arêtes ainsi fusionnées :
Eτ1,τ2 = (E1 ∩ E2) ∪ F τ1

(3.12)

1 ∪ F τ2

2

avec :

1 =n{u, v} ∈ E2 ∩ E1| conﬂt,G1(u, v) > τ1
2 =n{u, v} ∈ E1 ∩ E2| conﬂt,G2(u, v) > τ2

o
o

F τ1

F τ2

(3.13)

(3.14)

Il est facile de vériﬁer que par construction, pour tout τ1, τ2 ∈ [0, 1], on a :

E1 ∩ E2 ⊂ Eτ1,τ2 ⊂ E1 ∪ E2

(3.15)

Le graphe ainsi construit contient toutes les arêtes présentes dans G1 et G2, et

une partie de celles présentes uniquement dans un seul des graphes.

Notons qu’il est possible d’imaginer un mécanisme de fusion analogue consistant
à retirer de l’union des deux graphes les paires présentes seulement dans un graphe
et ayant une similarité en dessous d’un certain seuil (dans ce graphe).

106

CHAPITRE 3. COMPARAISON ROBUSTE DE GRAPHES

3.5 Application à la comparaison de ressources

lexicales

Nous présentons dans cette section comment la mesure GU D peut être appliquée
pour comparer des ressources lexicales, en particulier des réseaux de synonymie.
Nous montrons que des diﬀérences « apparentes » entre des ressources, c’est-à-dire
un GED fort, peuvent être rattrapées lorsque ces ressources sont comparées avec
GU D. Ce qui signiﬁe que ces ressources même si elles n’utilisent pas les mêmes
arêtes déﬁnissent la synonymie de manière semblable.

3.5.1 Introduction des ressources comparées

Pour appliquer notre méthode nous comparons six paires de graphes de synony-
mie. Ces graphes sont issus de deux ressources pour le français et de deux ressources
pour l’anglais. Un graphe est extrait de chacune de ces ressources pour chaque partie
du discours (noté POS par la suite pour Part Of Speech en anglais). À savoir un
graphe pour les noms (noté N par la suite), un pour les verbes (V) et un pour les ad-
jectifs (A). Nous présentons ici ces ressources et le tableau 3.3 résume les pedigrees
de ces graphes.

ROB et LAR. Robert et Larousse, ce sont deux dictionnaires de synonymie du fran-
çais. Ils font partie des sept dictionnaires de synonymes papier numérisés pour créer
DicoSyn. Ces numérisations ont été réalisées par l’INALF (aujourd’hui ATILF 5).
Chaque sommet correspond à un lemme (et à une POS), il n’y a pas de sous-sens
indiqué. Deux sommets sont adjacents si au moins l’un des deux apparaît dans la
liste des synonymes de l’autre. Les graphes sont donc non-dirigés.

PWN. WordNet [Fellbaum, 1998] de l’Université de Princeton, est la ressource lexi-
cale de référence pour l’anglais. Elle est organisée en synsets liés par diverses rela-
tions. Un synset étant un ensemble mots synonymes partageant un sens ou un usage
particulier. Les graphes construits à partir de WordNet sont tels que, pour chaque
POS, deux lemmes sont liés s’ils apparaissent ensemble dans au moins un synset.

ROG. Pour Roget thesaurus 6, c’est une ressource créée au XIXe siècle par le lexi-
cographe Peter Mark Roget. Les mots sont organisés en une hiérarchie de classes.
Nous avons construit un graphe par POS en liant les mots apparaissant ensemble
dans les feuilles de cette arborescence.

5. http://www.atilf.fr/
6. http://www.gutenberg.org/ebooks/10681

3.5. APPLICATION À LA COMPARAISON DE RESSOURCES
LEXICALES

107

Table 3.3 – Pedigree des 12 graphes de synonymie. n et m sont l’ordre et la taille
des graphes, hki le degré moyen, c le coeﬃcient de clustering, λ est le coeﬃcient de la loi de
puissance calculée par régression linéaire sur la distribution des degrés, r2 est le coeﬃcient
de corrélation de cette régression. nccp et mccp sont l’ordre et la taille de la plus grande
composante connexe et lccp est la longueur moyenne des plus courts chemins sur cette plus
grande composante connexe.

LAR

Graphes
A
N
V
A
N
V

ROB

PWN

ROG

n
5510
12159
5377
7693
24570
7357
21479
A
N 117798
11529
V
13193
A
29793
N
15258
V

m
21147
31601
22042
20011
55418
26567
22164
104929
23019
30078
83581
40591

<k>
7.68
5.20
8.20
5.20
4.51
7.22
2.06
1.78
3.99
4.56
5.61
5.32

c

0.2099
0.2023
0.1728
0.1446
0.1142
0.1192
0.6859
0.6894
0.4722
0.5644
0.6551
0.6066

λ (r2)

-2.06 (0.88)
-2.39 (0.88)
-1.94 (0.88)
-2.05 (0.94)
-2.34 (0.94)
-2.01 (0.93)
-2.82 (0.92)
-3.27 (0.95)
-2.46 (0.90)
-2.39 (0.91)
-2.45 (0.87)
-2.39 (0.87)

nccp
5011
9802
5193
6306
20785
7056
4406
12617
6534
6325
15331
7996

mccp
20826
30091
21926
19103
53006
26401
11276
28608
20806
23466
68476
33465

lccp
4.92
6.10
4.61
5.26
6.08
4.59
9.08
9.89
5.93
6.35
6.51
6.09

3.5.2 Analyse des comparaisons

Pour comparer ces réseaux, la première étape est de les réduire à leur ensemble
de sommets communs. En eﬀet les couvertures lexicales des ressources ne sont pas
identiques. Un certain nombre de mots ne sont présents que dans l’un des deux
graphes. Le tableau 3.4 donne les valeurs de GED, GU D et GU C pour les six
paires de graphes comparées. Le cardinal de l’ensemble des sommets communs est
aussi donné.

On observe que ROB et LAR sont relativement distants comparés avec GED,
ils n’ont de fait qu’un peu plus de la moitié de leurs arêtes en commun. Cela est
surprenant pour ces deux ressources d’origine similaire, mais le point intéressant est
que le score GU D est très faible. Eﬀectivement, comme l’indique GU C, le coût des
conﬂits entre les deux graphes est faible. C’est-à-dire les arêtes présentes dans l’un
et non dans l’autre des graphes, sont formées de sommets proches dans les deux
graphes.

Les valeurs observées entre ROG et PWN sont diﬀérentes. L’accord mesuré avec
GED est en eﬀet très important, et il ne varie pas énormément avec GU D. Ces
deux ressources ont donc peu d’arêtes en commun, et ces arêtes sont coûteuses à
ajouter. Le contexte et la méthode de création de ces deux ressources expliquent
certainement ces diﬀérences.

108

CHAPITRE 3. COMPARAISON ROBUSTE DE GRAPHES

Table 3.4 – Valeurs de GED et de GU C entre diﬀérentes paires de réseaux de
synonymie. GU D et GU C sont calculés avec des marches aléatoires de temps t = 4.

Graphes

ROB / LAR

ROG / PWN

A
N
V
A
N
V

|Va ∩ Vb| GED GU D GU C
0.24
0.32
0.28
0.91
0.95
0.82

4809
10881
4973
6853
16576
5459

0.45
0.48
0.48
0.87
0.89
0.85

0.11
0.15
0.13
0.79
0.84
0.70

(a) ROB face à LAR pour les verbes

(b) ROG face à PWN pour les verbes

Figure 3.4 – Distributions des coûts d’ajout (calculés avec la conﬂuence) des paires
de sommets en conﬂit. Les coûts sont calculés avec des marches aléatoires de temps t = 4.

Pour comprendre plus en détail ces comparaisons la ﬁgure 3.4 présente l’his-
togramme de répartition du coût des arêtes manquantes dans l’un ou l’autre des
graphes. Pour dix intervalles de coût, les barres verticales indiquent le nombre
d’arêtes ayant un coût d’ajout dans cet intervalle. Chaque histogramme (d’une cou-
leur donnée) indique donc la répartition des paires de sommets manquantes dans
l’un ou l’autre des graphes en fonction de leur coût d’ajout. Les résultats sont pré-
sentés pour les graphes entre verbes, les histogrammes sont très similaires pour les
autres parties du discours.

On vériﬁe bien qu’une large part des paires en désaccord entre ROB et LAR ont
un coût d’ajout faible (< 0.3). Dans les deux cas un certain nombre de paires ont un
coût entre 0.95 et 1. Ce sont les paires de sommets réellement en désaccord. Entre
ROG et PWN, la majorité des paires sont dans ce cas. Mais on observe aussi qu’entre
ROB et LAR, il existe un nombre signiﬁcatif de paires dans ce cas. Une large part de
ces fortes dissimilarités viennent de paires de sommets non-connectés dans l’un des
deux graphes.

0.00.20.40.60.81.0coût d'ajout050010001500200025003000350040004500Nombre de paires ROB.V LAR.V0.00.20.40.60.81.0coût d'ajout02000400060008000100001200014000Nombre de paires ROG.V PWN.V3.6. CONCLUSION DU CHAPITRE

109

3.6 Conclusion du chapitre

Nous avons dans ce chapitre introduit une méthode de comparaison de graphes
partageant le même ensemble de sommets. Cette méthode est robuste dans le sens
où deux graphes n’ayant aucune arête en commun mais tels que les sommets sont
proches de la même manière dans chacun d’eux seront considérés comme similaires
alors qu’une mesure basée simplement sur le nombre d’arêtes communes les indique-
rait comme complètement dissemblables. Deux graphes ayant exactement les mêmes
clusters mais construits par des arêtes en partie diﬀérentes sont typiquement dans
ce cas. Notre méthode généralise une distance d’édition entre graphes en calculant
les coûts d’édition à partir de la mesure de conﬂuence déﬁnie au chapitre précédent.
Le comportement de la méthode est vériﬁé sur diﬀérentes conﬁgurations de
graphes aléatoires. Et une application pour comparer des réseaux de synonymie est
présentée. Cette application permet en particulier de montrer que certains graphes
de synonymie ayant environ seulement la moitié de leurs arêtes en commun, sont
pourtant très proches.

Diﬀérentes pistes de travaux futurs sont envisageables. Tout d’abord, l’évaluation
de la méthode peut être étendue à des graphes artiﬁciels plus réalistes. En particulier
il serait intéressant d’étudier le comportement de GU D sur des graphes artiﬁciels
présentant un recouvrement entre les clusters ou une distribution des degrés plus
hétérogène. Par exemple en utilisant le modèle LFR [Lancichinetti et al., 2008].

Aussi, d’autres mesures de similarité peuvent être utilisées pour calculer les coûts
d’édition. La conﬂuence a l’avantage d’avoir une complexité raisonnable et d’être
normalisée par rapport à un null model. Mais l’utilisation de certaines méthodes
globales (le PageRank personnalisé par exemple) permettrait d’être plus robuste sur
des graphes peu denses. En eﬀet deux sommets peuvent avoir un score de conﬂuence
nul sur un graphe peu dense, uniquement parce qu’ils ne sont connectés que par des
chemins « trop longs » par rapport à la longueur des marches aléatoires utilisées.

Aussi il est envisageable d’utiliser une méthode de clustering pour évaluer les
coûts d’ajout. Mais plusieurs problèmes se posent alors. Tout d’abord si le graphe
est « trop » dense alors les méthodes de clustering ont tendance à ne faire qu’un seul
cluster, les sommets deviennent alors tous similaires ce qui n’est pas souhaitable.
Enﬁn un second problème est que la plupart des méthodes de clustering ne gèrent
pas de recouvrement. Et donc un sommet présent dans deux clusters ne sera similaire
qu’avec les sommets d’un seul de ces clusters.

Enﬁn l’application sur les réseaux lexicaux peut être poursuivie de plusieurs ma-
nières. Il est possible d’utiliser notre méthode pour évaluer des ressources lexicales
construites automatiquement. En eﬀet il est possible que deux ressources ayant une
même valeur de GED par rapport à un étalon aient des valeurs de GU D très diﬀé-
rentes. La méthode peut aussi être employée pour détecter les « zones » réellement
diﬀérentes entre deux ressources, permettant ainsi de guider une analyse d’erreur.

110

CHAPITRE 3. COMPARAISON ROBUSTE DE GRAPHES

Aussi la méthode de fusion que nous avons présentée pourrait être employée. Il est
possible notamment d’imaginer évaluer cette méthode de fusion sur une tâche don-
née. Il s’agirait de mesurer si un système de traitement automatique du langage
(TAL) donne de meilleurs résultats en utilisant directement l’union ou l’intersection
de deux graphes ou en utilisant le graphe fusionné avec notre méthode.

111

Chapitre 4
Enrichissement semi-automatique
de réseaux lexicaux

Nous proposons dans ce chapitre un système endogène et semi-automatique d’en-
richissement de ressources lexicales. Ce système repose sur une mesure de similarité
entre sommets d’un graphe, semblable à celles introduites au chapitre 2.

Les ressources lexicales sémantiques sont nécessaires pour beaucoup de systèmes
de traitement automatique du langage naturel (TAL). Pour autant alors que le trai-
tement de l’anglais est doté de WordNet [Fellbaum, 1998], de l’Université de Prince-
ton (ci-après noté PWN), ressource éprouvée depuis de nombreuses années, plusieurs
langues telles que le français ne bénéﬁcient encore d’aucune ressource de qualité
satisfaisante. « We desperately need linguistic resources ! » écrit Sekine [2010], souli-
gnant qu’il n’est pas réaliste de penser qu’une seule institution pourra développer des
ressources à large échelle, qu’une collaboration est donc nécessaire et que partager
les ressources est crucial. La première diﬃculté s’opposant à la construction de telles
ressources découle des modalités de développement et du compromis coût/qualité
qui en résulte. Recourir à des experts pour construire manuellement des ressources
coûte cher. Par ailleurs, on ne peut préjuger de la qualité des ressources construites
automatiquement (donc bruitées), qui devraient être validées par des experts, ce qui
ramène au problème initial. Enﬁn, dans le cas où le recours à la validation par des
experts est envisageable, la mise en place d’une mesure d’accord entre ces experts
est problématique. C’est à partir de ce constat que nous développons un système
semi-automatique d’enrichissement de réseaux lexicaux. Ce système permet de lis-
ter des « candidats synonymes » pour chaque mot d’une ressource en construction.
Le contributeur peut alors valider ou invalider ces candidats. L’ajout de liens de
synonymie à la ressource est ainsi accéléré, car elle se résume maintenant en une
simple validation par l’utilisateur. Ce système peut soit permettre d’accélérer une
construction collaborative, soit assister des lexicographes aﬁn de réduire le coût
d’élaboration d’une ressource. Nous explorons ici la première piste en présentant

CHAPITRE 4. ENRICHISSEMENT SEMI-AUTOMATIQUE DE
RÉSEAUX LEXICAUX

112

une mise en œuvre du système sur le dictionnaire collaboratif Wiktionary.

Après avoir décrit, en section 4.1, les tentatives antérieures de construction de
ressources et dressé un inventaire des diﬀérentes méthodes d’extraction de rela-
tions lexico-sémantiques, nous nous intéresserons tout particulièrement aux diﬃcul-
tés liées à l’évaluation des ressources. Nous présentons, en section 4.2, de nouvelles
tendances fondées sur l’édition collaborative qui constitue une piste intéressante
pour la construction de ressource. Dans ce cadre, Wiktionary, un dictionnaire libre
disponible en ligne, nous paraît être une clé pour régler simultanément le problème
du coût de développement et, dans une certaine mesure, celui de l’évaluation. Nous
présentons, en section 4.3, un processus d’enrichissement semi-automatique visant
à densiﬁer les réseaux de synonymie extraits de cette ressource. Nous mesurons
l’impact de l’utilisation de diﬀérentes sources de données sur ce processus en sec-
tion 4.3.3. Enﬁn en section 4.4, nous présentons Wisigoth une implémentation de
notre système.

Notons que ce travail a été présenté dans les publications suivantes : [Sajous
et al., 2011a,b], et deux étapes intermédiaires ont auparavant été publiées dans :
[Navarro et al., 2009; Sajous et al., 2010].

4.1 Ressources lexicales, construction et évalua-

tion

PWN est probablement le seul projet de construction d’une ressource lexicale
sémantique à connaître un tel succès et à être aussi largement utilisé. D’autres pro-
jets suscités par cette réussite, tels qu’EuroWordNet [Vossen, 1998] et BalkaNet [Tu-
ﬁs, 2000], prévoyaient une couverture moins ambitieuse. Malheureusement, ces res-
sources se sont ﬁgées dès la ﬁn de leur développement initial (alors que PWN conti-
nue d’évoluer). Jacquin et al. [2007] ont pointé les faiblesses de la partie française
d’EuroWordNet et ont proposé des méthodes automatiques pour ajouter des relations
manquantes. Ces méthodes, comme celles que nous énumérons en section 4.1.1, bien
qu’intéressantes, nécessiteraient une validation manuelle par des experts (donc coû-
teuse) pour produire une ressource ﬁable.

Les problèmes de temps et coût de développement, ainsi que de disponibilité
des ressources, sont de plus en plus pris en compte : en linguistique de corpus,
par exemple, une méthode « AGILE », empruntée à la gestion de projets, a été
proposée par Voormann et Gut [2008] pour permettre simultanément de maximiser
la taille d’un corpus et de ses annotations tout en réduisant le temps et le coût de
son développement. Brunello [2009] s’est intéressé au problème de disponibilité et
propose une méthode pour construire des corpus libres en recourant au web et aux
méta-données qui identiﬁent des pages web sous licence libre.

4.1. RESSOURCES LEXICALES, CONSTRUCTION ET
ÉVALUATION
4.1.1 Méthodes de construction de ressources lexicales

113

Proposées initialement par Hearst [1992] pour repérer en corpus les relations
d’hyperonymie, les approches par patrons lexico-syntaxiques ont été aﬃnées par Pan-
tel et Pennacchiotti [2006] pour réduire l’intervention manuelle nécessaire. De tels
patrons ne sont pas toujours aisés à mettre au point et sont par ailleurs spéciﬁques
à la langue étudiée.

Des méthodes d’enrichissement mutuel de ressources par liens interlangues con-
sistent à projeter des relations sémantiques en suivant des liens de traduction. Sagot
et Fišer [2008] ont construit WOLF, un WordNet libre du français, en utilisant plu-
sieurs ressources existantes pour amorcer d’une part une base de concepts (basés sur
la synonymie) français et anglais et pour construire un index interlangue à partir
duquel les projections ont été opérées. Un travail similaire mais utilisant Wiktionary
a été proposé par Hanoka et Sagot [2012]. De même, Soria et al. [2009] ont utilisé
des liens de traduction pour eﬀectuer une « fertilisation mutuelle automatique » de
deux lexiques, italien et chinois, ayant une structure similaire à celle de PWN. Ces
expériences, comparables à ce que nous proposons dans la suite de ce chapitre, ont
le mérite de prouver la faisabilité de la méthode, mais ne permettent pas de produire
des ressources ﬁables en l’absence de validation manuelle.

De nombreux travaux ont enﬁn porté sur la mise au point de calculs de simila-
rité sémantique : diﬀérentes propriétés sont associées aux mots et la propension à
partager des propriétés communes est interprétée comme le signe d’une proximité
sémantique. La plupart de ces méthodes reposent sur des modèles vectoriels : les
mots sont représentés par des vecteurs dont chaque coordonnée représente une pro-
priété du mot et la similarité sémantique entre mots est donnée par un calcul de
similarité entre ces vecteurs. Les méthodes diﬀèrent ensuite par les propriétés qui
constituent les vecteurs et les algorithmes de calcul de similarité. Un mot peut être
représenté, par exemple, par un vecteur contenant ses cooccurrents en corpus ou ses
contextes syntaxiques. Heylen et al. [2008] utilisent la similarité entre vecteurs pour
extraire des relations de synonymie et comparent les résultats obtenus en utilisant
des sacs de mots et des contextes syntaxiques. Ils étudient également l’impact de
certaines propriétés linguistiques (fréquence, spéciﬁcité et classes sémantiques) sur
la similarité ainsi calculée : les contextes syntaxiques s’avèrent plus eﬃcaces que les
sacs de mots, les meilleurs résultats sont obtenus avec les termes de haute fréquence
et appartenant à des classes sémantiques abstraites. L’eﬀet de la spéciﬁcité séman-
tique n’est pas probant. Ils montrent également que les vecteurs rapprochés par
leur méthode n’ayant pas trait à la synonymie reﬂètent souvent d’autres relations
sémantiques (cohyponymie et hyperonymie/hyponymie). Des comparaisons de diﬀé-
rentes mesures et pondérations de contextes syntaxiques sont données dans Curran
et Moens [2002]. Van der Plas et Bouma [2005] étudient quels contextes syntaxiques
particuliers mènent aux meilleurs résultats, observant par exemple qu’une meilleure

CHAPITRE 4. ENRICHISSEMENT SEMI-AUTOMATIQUE DE
RÉSEAUX LEXICAUX

114

précision est obtenue avec la relation objet qu’avec la relation modiﬁeur.

Ces méthodes peuvent également être utilisées avec des propriétés non-linguis-
tiques. Ollivier et Senellart [2007] et Weale et al. [2009] par exemple utilisent la
structure hypertextuelle de Wikipédia et de Wiktionary comme objet de leur calcul
de similarité. Ces méthodes peuvent donner de bons résultats sur certaines tâches
(78 à 93% sur des tests de type TOEFL) mais ne sont pas reproductibles en dehors
de ces architectures spéciﬁques (e.g. réseaux classiques de type WordNet en cours de
construction).

L’approche que nous présentons par la suite (section 4.3) est assez proche de ces
dernières méthodes. La première des diﬀérences est que nous utilisons un modèle
de graphe biparti pondéré plutôt qu’un modèle vectoriel (ce n’est qu’une diﬀérence
de point de vue, mais qui permet d’imaginer utiliser les similarités « globale » et
« semi-globale » vues au chapitre 2). Une seconde diﬀérence est que nous proposons
une méthode endogène 1, c’est-à-dire que la ressource elle-même est utilisée pour
calculer les similarités permettant de l’enrichir. Aussi, comme nous allons le voir,
nous proposons un processus de construction semi-automatique.

4.1.2 Évaluation des ressources lexicales

Bien qu’ayant donné lieu à de nombreux travaux, le problème de l’évaluation des
ressources construites automatiquement reste ouvert. Une approche traditionnelle
est la comparaison à une ressource étalon. En supposant l’existence et la disponibilité
d’une telle ressource, cette dernière a ses limites et donc l’évaluation qui en découle
également. L’étalon choisi peut en eﬀet avoir été conçu dans une optique particulière
(comme c’est le cas pour les dictionnaires) et ne reﬂéter qu’un aspect de la réalité. Il
n’est donc nécessairement ni générique, ni approprié à toute évaluation. Un étalon
doit être lui-même évalué ou caractérisé avant d’être choisi pour une évaluation
donnée. Nous avons montré au chapitre précédent (section 3.5.1) qu’il n’y a pas
nécessairement de consensus entre les ressources de référence, en particulier si on
les considère comme des « sacs de liens ». Ainsi, le choix de la ressource étalon
inﬂuence largement une évaluation. Les résultats obtenus ne permettent donc pas
de tirer des conclusions déﬁnitives : si, par exemple, un système propose deux mots
comme synonymes et qu’ils n’apparaissent pas comme tels dans l’étalon, le système
est-il en défaut ou est-ce dû au caractère non-exhaustif de l’étalon ?

Une évaluation manuelle peut également consister à extraire un échantillon de
relations et à les étiqueter manuellement comme correctes ou incorrectes. Cependant,
un tel jugement est entâché de la subjectivité liée notamment à une représentation
du monde diﬀérente pour chaque annotateur et mène souvent à un accord inter-
juges faible. En cas d’accord satisfaisant, celui-ci est régulièrement utilisé comme

1. Mis à part lorsque pour les contextes syntaxiques issus de Wikipédia.

4.2. ÉDITION COLLABORATIVE PAR LES FOULES

115

seul critère de qualité alors que rien ne garantit la pertinence des jugements rendus.
Murray et Green [2004] ont analysé les facteurs corrélés à des taux d’accords faibles
sur une tâche de désambiguïsation lexicale et ont montré que les accords élevés sont
corrélés à des compétences lexicales comparables entre les annotateurs et non à des
compétences élevées. Deux juges « naïfs » sont susceptibles d’obtenir le même taux
d’accord que deux experts ; ajouter un expert à un groupe de naïfs tend à diminuer
ce taux. Ils concluent que la mesure de l’accord inter-annotateurs seule ne constitue
pas un critère de qualité d’une annotation et qu’elle doit être couplée à une mesure
de compétence par rapport à l’annotation envisagée.

Enﬁn, on peut comparer plusieurs ressources via une évaluation par la tâche
en mesurant les performances d’un système utilisant les diﬀérentes ressources pour
réaliser une tâche donnée. Des ressources lexicales sémantiques peuvent être évaluées
par exemple, dans des tâches de recherche d’information, de traduction automatique,
de WSD 2, etc. Pour estimer les performances de ce système, le processus d’évaluation
doit pouvoir déterminer, pour une entrée donnée, quelle sortie doit être produite. Ce
problème nécessite alors la construction d’un étalon et soulève les mêmes problèmes
que ceux mentionnés plus haut. Par exemple, Kilgarriﬀ [1998] a montré les diﬃcultés
rencontrées dans la préparation d’un étalon pour la campagne SENSEVAL.

Enﬁn il faut noter le rôle central de la granularité notamment dans la modéli-
sation de la synonymie. Dans une tâche de WSD, Palmer et al. [2007] ont montré
qu’un regroupement des sens d’un dictionnaire électronique permettait de régler des
désaccords « à la marge » entre annotateurs. De manière générale, l’accord augmen-
tait de 10 à 20% lorsqu’il était mesuré sur une annotation du système utilisant une
granularité plus grosse. Cependant, ils soulignent qu’atteindre des taux d’accords
très élevés avec des mots fortement polysémiques est un objectif non réaliste. Ils
notent également qu’accroître cet accord n’est pertinent que si cela n’aﬀecte pas ou
peu la qualité résultante des applications de TAL.

4.2 Édition collaborative par les foules

Depuis la création de Wikipédia, l’exactitude des ressources construites colla-
borativement « par les foules » a été mise en doute. Wikipédia était initialement
la seule ressource de ce type et la question de son bien-fondé a mené à une polé-
mique : Giles [2005] a prétendu que l’encyclopédie en ligne était aussi précise que
l’encyclopédie Britanica, qui a pour sa part réfuté les critères d’évaluation utilisés.
Depuis, les wikis et les ressources collaboratives se sont multipliés. Plus modérés que
Giles, Zesch et Gurevych [2010] ont montré, à travers une tâche de mesure de simi-
larité sémantique, que les ressources fondées sur « la sagesse des foules » ne sont pas
2. WSD signiﬁe Word-sense disambiguation, voir par exemple [Manning et Schütze, 1999, chap.

7]

CHAPITRE 4. ENRICHISSEMENT SEMI-AUTOMATIQUE DE
RÉSEAUX LEXICAUX

116

meilleures que celles fondées sur « la sagesse des linguistes », mais sont sérieusement
compétitives. Elles dépassent même les ressources construites par les experts dans
certains cas, notamment en termes de couverture.

Dans cette section, nous présentons les diﬀérents modèles de construction « par
les foules » de ressources lexicales. Nous présentons ensuite une description détaillée
de Wiktionary.

4.2.1 Plusieurs modèles

Au problème de la qualité du contenu s’ajoute celui de la participation « des
foules ». En eﬀet, les approches collaboratives n’impliquent pas la seule participa-
tion de collègues et d’étudiants mais doivent mobiliser une plus large population ne
partageant pas nécessairement les motivations des chercheurs. Plusieurs tendances
récentes refaçonnent le domaine de la création de ressources. Parmi elles, quelques
chercheurs ont mis au point des jeux en ligne destinés à attirer des internautes « or-
dinaires » qui jouent pour le plaisir indépendamment de la ﬁnalité première du jeu.
C’est le cas de JeuDeMots, développé par Lafourcade [2007], qui a réussi à collecter un
nombre important de relations sémantiques (essentiellement non typées, mais égale-
ment des relations telles qu’hyperonymie et méronymie). Cependant, réaliser un jeu
suﬃsamment distrayant est une tâche diﬃcile en soi et il est probable que certaines
tentatives échouent si elles ne produisent pas des jeux suﬃsamment attractifs.

Le système « Mechanical Turk », créé par Amazon (AMT), consiste à déﬁnir
des micro-tâches que des travailleurs (turkers) accomplissent contre une rétribution
minimale. Ces tâches, ou HITs (human intelligence tasks), permettent d’eﬀectuer ce
dont un système informatique est incapable : étiqueter une image, exprimer sa pré-
férence pour une couleur, etc. Utilisé généralement dans le commerce électronique,
ce système a également servi à juger la qualité des articles de Wikipédia en re-
cueillant les appréciations d’un groupe d’utilisateurs. Des chercheurs ont également
eu recours à ce modèle pour mettre en place des systèmes d’évaluation. Snow et al.
[2008], par exemple, ont évalué les performances d’annotateurs naïfs utilisés comme
turkers dans des tâches telles que désambiguïsation lexicale, jugement de similarité
sémantique, annotation de sentiments dans les documents textuels, etc. Ils ont ob-
tenu, dans 5 tâches sur 7, le résultat contre-intuitif qu’un système entraîné sur les
annotations des turkers surpasse celui entraîné par un expert unique. Ils expliquent
ce résultat par le fait que le jugement de plusieurs naïfs permet de corriger le biais
introduit par l’unicité d’un annotateur, fût-il expert. Le modèle d’Amazon a montré
qu’il pouvait être approprié à des tâches d’annotation. Il faut néanmoins accueillir
ce constat avec mesure et garder à l’esprit qu’AMT a été conçu pour accomplir des
tâches courtes et ne doit être utilisé que pour ces tâches-là. Dans le cas contraire,
les turkers peu scrupuleux peuvent être tentés d’abuser le système en passant un
minimum de temps sur chaque tâche et en donnant des réponses au hasard. Par

4.2. ÉDITION COLLABORATIVE PAR LES FOULES

117

ailleurs, même avec des « turkers honnêtes », vériﬁer la compétence d’un annotateur
pour une tâche donnée peut être nécessaire (cf. section 4.1.2). Notons enﬁn que,
comme le soulignent Adda et al. [2011], ces systèmes de Mechanical Turk posent des
questions sociales et éthiques légitimes.

La mise en place d’infrastructures sophistiquées et coûteuses dans la perspective
d’une édition « par les foules » présente le risque d’aboutir à des coquilles vides atten-
dant indéﬁniment d’être remplies. En eﬀet, dans le contexte actuel de compétition
pour attirer les internautes, les plateformes dépourvues de contenu, aussi promet-
teuses soient-elles, n’attirent personne. Toute solution envisagée qui sous-estime la
contrainte d’attractivité est vouée à l’échec. Seules quelques initiatives collabora-
tives et quelques réseaux sociaux ont réussi à subsister et concentrent la majorité
des internautes. Le simple fait de s’adosser à l’une de ces « success stories » du web
laisse espérer une multitude de visiteurs. Wikipédia et Wiktionary sont certainement
parmi les meilleurs exemples de réussite et la communauté du TAL peut apporter
ses compétences et outils à leurs utilisateurs, bénéﬁciant en retour d’un nombre
important de contributeurs. C’est dans cette optique de « piggybacking » que s’ins-
crivent les travaux présentés ici. En s’appuyant sur l’architecture de Wiktionary et
son contenu déjà existant, nous proposons aux contributeurs des synonymes calculés
automatiquement aﬁn qu’ils les valident. Nous espérons ainsi aider ces contributeurs
à accélérer l’enrichissement du dictionnaire tout en proﬁtant de son infrastructure
pour établir des méthodes également applicables à d’autres types de ressources.

4.2.2 Le cas de Wiktionary

Wiktionary, le « compagnon lexical de Wikipédia », est un dictionnaire multi-
lingue libre et accessible en ligne. Comme les autres satellites de la Wikimedia
Foundation, c’est un projet collaboratif : n’importe quel internaute peut contribuer
et ses modiﬁcations sont publiées immédiatement. Chaque article peut mentionner
des informations sur l’étymologie, contenir des gloses, exemples, traductions et des
relations sémantiques. Du point de vue du TAL, Wiktionary peut sembler un terrain
de jeu idéal et la couverture lexicale apparente renforce cette impression. Nous pré-
sentons ci-après un examen plus approfondi qui nuance des propos souvent (trop ?)
enthousiastes.

4.2.2.1 Encodage et structuration des données

Précisons tout d’abord qu’il existe, comme pour Wikipédia, un Wiktionary par
langue. Et ces diﬀérents Wiktionary sont, a priori, indépendants et chacun contient
des informations concernant le lexique de potentiellement n’importe quelle langue
(ces informations étant données dans la langue du Wiktionary). Ainsi, par exemple,
le Wiktionary anglais contient des mots français déﬁnis en anglais. Notons que le

CHAPITRE 4. ENRICHISSEMENT SEMI-AUTOMATIQUE DE
RÉSEAUX LEXICAUX

118

Wiktionary français se nomme « Wiktionnaire ».

Comme tous les projets de la Wikimedia Foundation, les Wiktionary utilisent
un système de gestion de contenu appelé MediaWiki 3. Le contenu des articles est
enregistré dans un langage nommé code wiki. Malheureusement, ce langage ne vient
pas avec une spéciﬁcation formelle, et des déviances par rapport au langage standard
(supposé) sont fréquentes. En analysant qualitativement les modiﬁcations des contri-
buteurs, nous avons remarqué un nombre signiﬁcatif d’erreurs dans les articles dûes
à une incompréhension ou à un non-respect de la syntaxe. Nous pressentons égale-
ment l’eﬀet, peut-être plus dommageable, que nombre d’utilisateurs ne deviendront
jamais contributeurs seulement parce qu’ils resteront rebutés par cette syntaxe.

Un article contient potentiellement plusieurs sections de langues, la première
étant celle de l’édition du Wiktionary consultée. Une section de langue peut contenir
plusieurs sections liées à une catégorie syntaxique donnée. Dans une telle section, on
peut trouver des gloses et des exemples, parfois répartis selon plusieurs sens. Ensuite
viennent les traductions et les relations sémantiques.

Une grande variation existe cependant par rapport à ce cas prototypique. Chaque
Wiktionary a ses propres conventions et au sein d’une langue donnée (c’est-à-dire
pour un Wiktionary), les conventions écrites ne sont pas toujours respectées. La
notion de ﬂexibilité étant revendiquée comme une propriété intrinsèque des projets
de la Wikimedia Foundation, l’analyse automatique du contenu de Wiktionary n’est
pas tâche aisée. Elle l’est encore moins lors de l’analyse des ﬁchiers historiques
retraçant la chronologie des articles (i.e. contenant toutes leurs versions datées),
lorsqu’à la fois syntaxe et conventions évoluent avec le temps.

4.2.2.2 Nomenclature et couverture lexicale

Le Wiktionary anglais fait état 4 de « 2 038 436 entries with English deﬁnitions
from over 400 languages » et la version française de « 1 821 097 articles [qui] dé-
crivent en français les mots de plus de 700 langues ». Ces chiﬀres impressionnants
sont néanmoins à tempérer : les méta-articles (pages d’aide, de discussion, etc.)
sont comptés comme entrées et nombre de mots étrangers (à la langue du Wiktio-
nary considéré) sont curieusement inclus dans la nomenclature (e.g. lexicon apparaît
comme une entrée anglaise dans le Wiktionary français). Les formes ﬂéchies font
l’objet d’articles quand on pourrait les attendre dans les articles consacrés à leur
forme citationnelle. Alors que dans l’édition française, les locutions sont catégori-
sées comme telles, l’édition anglaise les classe sous la catégorie de leur tête (voire
une autre catégorie), ce qui gonﬂe artiﬁciellement le nombre de mots annoncé (e.g.
« caught on the hop » apparaît comme un verbe standard).

3. http://www.mediawiki.org
4. Ces chiﬀres datent de 2010.

4.2. ÉDITION COLLABORATIVE PAR LES FOULES

119

Table 4.1 – Couverture lexicale du Wiktionnaire (2011) et de Morphalou (1.0) par
rapport aux lexiques extraits des corpus Frantext, Le Monde et Wikipédia

Taille de la
nomenclature

Couverture des vocabulaires des corpus (%)

Frantext

Le Monde

Wikipédia

Morph. Wikt.
41 005 134 203 29 604
7 384 18 830 6 964
15 208 42 263 10 014

N.
V.
Adj.

∩ Morph. Wikt. M∪W Morph. Wikt. M∪W Morph. Wikt. M∪W
31,6
72,2
84,7

23,5
66,3
73,9

80,6
86,5
84,6

54,1
80,0
76,8

84.4
87.1
94.0

76,4
84,2
88,9

47,3
75,1
78,9

58,1
80,8
88,1

26,7
71,5
72,4

Malgré des catégorisations surprenantes et des imports automatiques massifs (la
moitié des noms sont des gentilés, cf. section 4.2.2.3), le nombre de mots contenus
dans les éditions française et anglaise de Wiktionary reste élevé. Nous avons évalué
sa couverture lexicale eﬀective par rapport à celle du Trésor de la Langue Française
informatisé (TLFi). Nous avons pour cela examiné le recouvrement entre les noms,
verbes et adjectifs (formes canoniques) du dictionnaire collaboratif par rapport à
ceux présents dans Morphalou 5. Nous voyons dans le tableau 4.1 que le Wiktionnaire
comprend trois-quart des noms du TLFi, quasiment tous ses verbes et deux tiers de
ses adjectifs.

Nous avons également évalué la proportion couverte par chaque dictionnaire du
vocabulaire des trois corpus suivants : 30 millions de mots issus de 515 romans du
XXe siècle de la base Frantext, 200 millions de mots issus des articles du quotidien
Le Monde (période 1991 à 2000) et 260 millions de mots extraits de l’encyclopédie
Wikipédia (version 06/2008). Nous avons lemmatisé et catégorisé ces corpus avec
TreeTagger 6, puis gardé les lemmes catégorisés dont le nombre d’occurrences est
supérieure ou égale à 5. Nous observons que les deux lexiques suivent la même
tendance : le vocabulaire de Frantext est mieux couvert que celui du Monde, lui-même
mieux couvert que celui de Wikipédia. La moindre couverture pour Wikipédia peut
s’expliquer notamment par la diversité des domaines couverts et des rédacteurs, ainsi
que par la présence des mots étrangers. Le faible pourcentage de noms couverts peut
s’expliquer par le fait que les articles de l’encyclopédie comportent de nombreux mots
isolés inconnus de TreeTagger qu’il étiquette souvent comme des noms communs. On
observe sur les trois corpus que le Wiktionnaire couvre plus de noms et de verbes (2
à 7%) et que Morphalou couvre mieux les adjectifs (1 à 4%). Si l’on construit l’union
des deux nomenclatures (noté M∪W), on augmente clairement la couverture pour
les noms (5%) et les adjectifs (10%).

Ces observations nous amènent à formuler le constat que, bien que bruité et
souﬀrant d’un manque de formalisme, Wiktionary, par sa couverture et sa dispo-
5. Lexique distribué par l’ATILF issu de la nomenclature du TLFi, disponible à l’adresse :

http://www.cnrtl.fr/lexiques/morphalou/

6. http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/

CHAPITRE 4. ENRICHISSEMENT SEMI-AUTOMATIQUE DE
RÉSEAUX LEXICAUX

120

Table 4.2 – Wiktionary : croissance des éditions française et anglaise de 2007 à
2010.

FR

ANG

Entrées
Syn.
Trad.
Entrées
Syn.
Trad.

2007

Noms Verbes Adjectifs
38 973
9 670
106 061
65 078
12 271
172 158

6 968
1 793
43 319
10 453
3 621
37 405

2010
Verbes

Noms

Adjectifs

3 158 (x1.8)

17 054 (x1.8)

11 787 106 068 (x2.7) 17 782 (x2.6) 41 725 (x3.5)
4 111 (x1.6)
2 522
25 066 153 060 (x1.4) 49 859 (x1.2) 32 949 (x1.3)
17 340 196 790 (x3.0) 67 649 (x6.5) 48 930 (x2.8)
9 574 (x2.1)
4 483
34 338 277 453 (x1.6) 70 271 (x1.9) 54 789 (x1.6)

28 193 (x2.3)

8 602 (x2.4)

nibilité, mérite d’être envisagé comme une ressource lexicale intéressante pour le
TAL. De plus, les résultats présentés dans le tableau 4.1 conﬁrment les observa-
tions de Zesch [2010] : les ressources créées par des experts et celles construites
collaborativement par les foules ne se confondent pas mais peuvent contenir des
connaissances complémentaires. En eﬀet, s’il est vrai que l’on trouve dans les en-
trées du Wiktionnaire absentes du TLFi, des néologismes liés notamment au domaine
d’internet tels que googler et wikiﬁer, ou des variations régionales (dracher) et fran-
cophones (diplomation, hommerie), le Wiktionnaire compte également des termes
spécialisés comme clitique et métier (comme adjectif, « une application métier »)
et d’autres, passés dans l’usage courant comme sinogramme, homophobie, sociétal,
fractal, ergonomique, médicaliser, étanchéiﬁer, désactiver, décélérer, paramétrer. . .

4.2.2.3 Entrées et relations sémantiques : une croissance à deux vitesses
Aﬁn d’étudier comment évolue une ressource collaborative telle que Wiktionary,
nous avons analysé son « dump historique » 7. Ces ﬁchiers contiennent les versions
intégrales de tous les articles après chaque édition. Comme on le voit dans la ﬁ-
gure 4.1, l’édition anglaise a connu une croissance constante en termes d’entrées
(nombre de mots renseignés) alors que l’édition française a connu deux sauts. Le
premier, début 2006, est dû à un import automatique d’articles du Dictionnaire de
l’Académie Française (DAF). Des imports d’articles du Littré ont également été eﬀec-
tués, de manière plus étalée. Le nombre d’imports automatiques pour l’anglais est
non signiﬁcatif. On observe pour le français un second saut en 2008, plus important,
qui concerne les noms et les adjectifs. Ce saut est dû à un import automatique de
76 347 gentilés (il n’aﬀecte donc pas les verbes) extraits d’un site spécialisé.

Les contributeurs étant plus enclins à ajouter de nouvelles entrées, la croissance
du nombre de relations est plus lente que celle des entrées. Parmi elles ﬁgurent es-

7. Ces dumps sont disponibles à l’adresse : http://download.wikipedia.org/

4.3. ENRICHISSEMENT SEMI-AUTOMATIQUE

121

sentiellement des synonymes et quelques antonymes, les autres relations restant très
rares. La ﬁgure 4.2 montre l’évolution du nombre de relations sémantiques et de
liens de traduction dans les éditions anglaise et française. Aucun import automatisé
concernant les traductions n’est explicitement mentionné dans Wiktionary. Cepen-
dant, nous avons remarqué dans l’édition française un ajout massif de traductions
eﬀectué début 2006 par un automate, sans que cet import ne soit documenté. Après
examen, nous avons trouvé une note succincte dans la page de discussion de l’auteur
de cet automate mentionnant l’import depuis un site en ligne dont ni l’origine du
contenu ni la licence n’étaient clariﬁés. Finalement, malgré une augmentation im-
portante du nombre de relations sémantiques et de liens de traduction, l’écart par
rapport à la croissance du nombre de mots continue de se creuser (cf. tableau 4.2).
Or ces relations sémantiques sont des données essentielles pour l’exploitation par les
applications de TAL.

4.3 Enrichissement semi-automatique

Pour combler l’écart entre le nombre d’entrées et celui des relations sémantiques,
nous proposons de développer un système d’enrichissement de réseaux sémantiques,
à commencer par les relations de synonymie. Ainsi nous décrivons ci-après une mé-
thode d’enrichissement de ressource par calcul de liens de synonymie candidats dans
une perspective de validation « par les foules ». L’idée est d’utiliser un graphe biparti
extrait de la ressource et/ou de données externes (sous-section 4.3.1), pour calcu-
ler, avec des mesures de similarité (sous-section 4.3.2), des candidats synonymes qui
pourront être validés (ou rejetés) par un contributeur. Nous proposons une valida-
tion de la méthode en sous-section 4.3.3. Cette évaluation permet aussi de mesurer
l’impact des diﬀérentes similarités et sources de données utilisées.

4.3.1 Modèle de graphes bipartis pondérés

Nous nous appuyons dans les expériences qui suivent sur des graphes bipartis
pondérés. Aﬁn d’homogénéiser la présentation, toutes les sources de données que
nous utiliserons seront modélisées par un graphe G = (V, V 0, E, w) constitué de
deux sous-ensembles de sommets :

• V , l’ensemble des mots présents dans Wiktionary pour la langue et la catégorie
• V 0, un second ensemble de sommets provenant des diﬀérentes sources de don-

syntaxique considérée,

nées considérées.

E est l’ensemble des arêtes (E ⊆ (V × V 0)) modélisant les relations qu’entretiennent
les sommets de V avec ceux de V 0, un poids est donné à chaque arête par la fonction
w : E → R+.

CHAPITRE 4. ENRICHISSEMENT SEMI-AUTOMATIQUE DE
RÉSEAUX LEXICAUX

122

Figure 4.1 – Wiktionary : évolution du nombre de mots et imports automatisés.

Figure 4.2 – Wiktionary : évolution des relations sémantiques et des liens de
traduction.

20032004200520062007200820092010050000100000150000200000NomsVerbesAdjectifsImports DAFImports LittréAnglaisFrançais20032004200520062007200820092010NomsVerbesAdjectifsImports Webster 1913Imports Century 1911200420052006200720082009201001000020000300004000050000synonymesantonymeshyperonymesméronymesholonymesAnglaisFrançais2005200620072008200920104.3. ENRICHISSEMENT SEMI-AUTOMATIQUE

123

Nous considérons les graphes suivants :
• Graphe de traduction GW t = (V, VW t, EW t, wW t)

V 0 = VW t est ici constitué des traductions des mots du Wiktionnaire étudié. EW t
représente les liens de traduction : une arête relie v ∈ V à t ∈ VW t si t est une
traduction de v dans Wiktionary. Tous les liens ont la même pondération : ∀e ∈
E, wW t(e) = 1.
V 0 = VW s est ici une duplication de V . Une arête relie v ∈ V à u ∈ VW s si v = u
ou si u (resp. v) est mentionné comme synonyme dans l’entrée de v (resp. u). Les
arêtes ont ici aussi toutes la même pondération : ∀e ∈ E, wW s(e) = 1.

• Graphe de synonymie GW s = (V, VW s, EW s, wW s)

• Graphe des gloses GW g = (V, VW g, EW g, wW g)

V 0 = VW g correspond ici à l’ensemble des mots lemmatisés trouvés dans les gloses
de toutes les entrées. Une arête relie v ∈ V à g ∈ VW g si g apparaît dans une
des gloses de v. Pour chaque mot, ses gloses ont été concaténées, lemmatisées et
étiquetées avec Treetagger, puis les mots vides ont été supprimés. Parmi plusieurs
pondérations envisageables, nous avons utilisé le nombre d’occurrences : le poids de
l’arête liant u ∈ V et g ∈ VW g est le nombre d’occurrences de g dans la glose de u.
Un score plus complexe de tf-idf est envisageable, ou encore il est possible d’utiliser
la position du terme dans la glose.

• Graphe des contextes syntaxiques GW pc = (V, VW pc, EW pc, wW pc)

Nous avons extrait de la version française de Wikipédia un corpus de 260 millions de
mots que nous avons analysé avec Syntex [Bourigault, 2007]. Cet analyseur produit
des relations de dépendance syntaxique que nous avons utilisées pour construire une
liste de paires <mot, contexte>, un contexte est ici un couple formé d’un (autre)
mot et d’une relation syntaxique. VW pc désigne l’ensemble de ces contextes et une
arête e = {v, c} ∈ EW pc signiﬁe qu’un mot v apparaît dans le contexte c. Nous
utilisons l’information mutuelle ponctuelle comme pondération de ces arêtes :

∀e = {v, c} ∈ E, wW pc(e = {v, c}) = log(f(v, c)f(∗,∗)
f(v,∗)f(∗, c))

(4.1)
où f(v, c) est le nombre d’occurrences du mot v dans le contexte c ; f(v,∗), f(∗, x)
et f(∗,∗) sont respectivement le nombre d’occurrences totale de v (tous contextes
confondus), le nombre d’occurrences totale de c et le nombre total de couples.

Les tailles des graphes ainsi construits et des graphes combinant les diﬀérentes
sources de données sont récapitulées dans le tableau 4.3. Par exemple, s + t + g
est le graphe résultant de l’agglomération des sommets et des arêtes des graphes de
synonymie, de traduction et de gloses :

V,

V 0 = VW s ∪ VW t ∪ VW g, E = EW t ∪ EW t ∪ EW g, w

G =(cid:16)

Deux sommets provenant d’ensembles diﬀérents (de diﬀérents V 0, e.g. un de VW t et
un de VW g) restent dupliqués même s’ils correspondent au même mot. Nous pondé-

(cid:17)

CHAPITRE 4. ENRICHISSEMENT SEMI-AUTOMATIQUE DE
RÉSEAUX LEXICAUX

124

Table 4.3 – Ordre et taille des graphes bipartis utilisés pour le calcul de similarité. n
et n0 : nombre de sommets de V et V 0 ayant au moins un voisin. m : nombre d’arêtes. s, t, g
et c : graphes de synonymie, traductions, gloses et contextes syntaxiques.

trad.
syn.
gloses
contextes
s + t
s + t + g
s + t + g + c
trad.
syn.
gloses
contextes
s + t
s + t + g
s + t + g + c
trad.
syn.
gloses
contextes
s + t
s + t + g
s + t + g + c

n
8 178
8 723
45 703
−
13 650
47 280
−
7 473
7 341
42 901
−
11 423
44 295
−
29 489
31 227
194 694
−
50 305
202 920
−

Anglais
n0
43 976
8 723
39 409
−
52 699
92 108
−
52 862
7 341
36 051
−
60 203
96 254
−
235 233
31 227
127 198
−
266 460
393 658
−

m
54 840
27 257
218 993
−
82 097
301 090
−
70 432
23 927
222 004
−
94 359
316 363
−
277 897
86 195
1 218 414
−
364 092
1 582 506

n
5 335
4 482
41 620
6 262
7 849
42 507
42 517
3 174
3 190
17 743
4 273
5 054
18 226
18 229
18 468
19 407
105 760
22 711
30 810
111 228
− 111 290

Français
n0
23 976
4 482
42 455
129 199
28 458
70 913
200 761
30 162
3 190
16 942
2 312 096
33 352
50 294
2 374 679
129 426
19 407
69 994
1 671 655
148 833
218 827
1 898 564

m
32 944
127 54
263 281
934 969
45 698
308 979
1 248 779
49 866
9 510
101 458
5 499 611
59 376
160 834
5 700 602
153 033
53 869
844 805
8 719 464
206 902
1 051 707
9 818 553

Adjectifs

Verbes

Noms

rons les arêtes du graphe combiné en multipliant le poids des arêtes initiales par un
coeﬃcient dépendant du type d’arête : un graphe noté αs.s + αt.t + αg.g + αc.c
aura la fonction de pondération suivante :
αs.wW s(e)
αt.wW t(e)
αg.wW g(e)
αc.wW c(e)

e ∈ EW s,
e ∈ EW t,
e ∈ EW g,
e ∈ EW c.

si
si
si
si



w(e) =

(4.2)

D’autres manières d’agréger les données et de pondérer les arêtes peuvent évidem-
ment être envisagées. À ce stade, les conﬁgurations que nous proposons permettent
déjà d’augmenter sensiblement le nombre de candidats pertinents proposés (voir
sous-section 4.3.3).

4.3.2 Calcul de similarités, calcul de candidats

Le calcul des candidats synonymes se base sur un calcul de similarité entre les
sommets de V dans les graphes bipartis que nous venons de présenter. Les candi-

2

avg(u, v) = prxt(u, v) + prxt(v, u)
P
qP
w∈V prxt(u, w)2qP
cos(u, v) =
dot(u, v) = X
ZKLΩ(u, v) = X

prxt(u, w).prxt(v, w)

prxt(u, w).

( log( prxt(u,w)

w∈V

w∈V

w∈V .prxt(u, w)prxt(v, w)

w∈V prxt(v, w)2

(4.3)

(4.4)

(4.5)

(4.6)

(4.7)

4.3. ENRICHISSEMENT SEMI-AUTOMATIQUE

125

dats synonymes pour un mot si ∈ V , sont simplement les sommets de V les plus
« proches » et n’étant pas déjà synonymes de v.

Nous avons expérimenté plusieurs mesures de similarité basées sur des marches
aléatoires en temps courts. Le fonctionnement de ces marches est déﬁni plus en détail
en section 2.2.1. L’idée est de considérer un marcheur parcourant aléatoirement le
graphe : à chaque pas ce marcheur passe du sommet courant à l’un des voisins choisi
aléatoirement mais proportionnellement au poids des arêtes correspondantes. Nous
notons prxt(u, v) la probabilité qu’un marcheur soit sur un sommet v apres t pas
depuis un sommet u. En se basant sur ces probabilités nous avons testé les mesures
de similarité entre sommets suivantes :

simple(u, v) = prxt(u, v)

prxt(v,w)) si prxt(v, w) 6= 0,

Ω

sinon.

La mesure « simple » est directement la probabilité d’atteindre u en partant de v,
« avg » est une version naïvement symétrisée de « simple ». Les autres similarités sont
des mesures classiques calculées sur les vecteurs de probabilité donnés par prxt(v,∗)
et prxt(u,∗). « cos » et « dot » sont respectivement le cosinus et le produit scalaire.
« ZKLΩ » est une variante de la mesure de divergence Kullback-Leibler introduite
par Hughes et Ramage [2007].

Nous avons vu en section 2.2.1 que sur un graphe biparti, le marcheur est ex-
clusivement sur l’un ou l’autre des ensembles de sommets en fonction du sommet
de départ et de la parité du nombre t de pas eﬀectués. Pour pouvoir mesurer une
similarité entre sommets d’un même groupe V nous utilisons donc des marches en
temps pairs. Les expériences présentées ensuite sont réalisées avec t = 2 (marches
de longueur 2). Nous avons eﬀectué des expériences préliminaires avec t = 4, mais
les résultats sont similaires (avec un coût de calcul plus important).

4.3.3 Évaluation
4.3.3.1 Mesure de la pertinence des candidats proposés

En vue d’un enrichissement semi-automatique dans lequel les contributeurs va-
lident ou invalident les candidats proposés, nous considérons qu’une courte liste de
suggestions est acceptable si elle contient au moins un candidat pertinent. Ainsi,

CHAPITRE 4. ENRICHISSEMENT SEMI-AUTOMATIQUE DE
RÉSEAUX LEXICAUX

126

notre évaluation consiste essentiellement à compter pour combien de mots le sys-
tème produit une liste acceptable. Nous examinons également pour combien de mots
sont proposées des listes contenant 2, 3 ou plusieurs candidats pertinents.
Soit GGS = (VGS, EGS) un réseau de synonymie étalon, VGS ses mots, et EGS ⊆
VGS × VGS ses relations de synonymie. Nous évaluons ci-après la pertinence des
relations proposées pour enrichir la ressource déﬁciente par rapport aux relations
de l’étalon. Nous eﬀectuons cette évaluation pour les mots communs à la ressource
à enrichir et l’étalon. Nous supprimons donc de la liste des candidats ceux qui ne
ﬁgurent pas dans l’étalon 8 et limitons cette liste à k = 5 candidats. Pour chaque
mot v ∈ V ∩ VGS, on note Ωk(v) la liste des meilleurs candidats « évaluables » :

Ωk(v) = [c1, c2, . . . , ck0] avec

∀i, ci ∈ V ∩ VGS
∀i, sim(v, ci) ≥ sim(v, ci+1)

(4.8)

 k0 ≤ k

où sim est l’une des mesures présentées en section 4.3.2.

N
proposés :

Ωk(v) contient un maximum de k candidats (mais peut en contenir moins ou être
vide). De plus, Ωk(v) dépend de la ressource étalon, car les candidats non présents
dans l’étalon sont éliminés. On note Ω+
k (v) l’ensemble des candidats corrects de
Ωk(v) :

(4.9)
On note Nk l’ensemble des mots pour lesquels sont proposés k candidats et
+p
le sous-ensemble des mots pour lesquels au moins p candidats pertinents sont
k

Ω+

o

k (v) =n
Nk =n
k =n

c+ ∈ Ωk(v).{v, c+} ∈ EGS
o
.|Ωk(v)| = k
o
.|Ω+
k (v)| ≥ p

v ∈ V ∩ VGS
v ∈ Nk

+p

N

(4.11)
Pour mesurer l’impact des sources de données utilisées sur le calcul des candidats,
nous mesurons le ratio Rk entre le nombre de listes suggérées 9 et le nombre de mots
évaluables, et Pk, le ratio entre le nombre de listes acceptables et le nombre de listes
suggérées :

(4.10)

Rk =

|Nk|

|VGS ∩ V |,

Pk = |N +1
k |
|Nk|

(4.12)

Nous utilisons PWN et DicoSyn comme ressources étalons respectivement pour l’an-
glais et le français. Nous renvoyons à la section 3.5 pour une description plus en
détail de ces ressources.

8. Il peut s’agir d’un néologisme ou d’un terme de spécialité, plus rarement d’une forme mal

orthographiée de Wiktionary, ou être dû à une couverture non exhaustive de l’étalon.

9. i.e. le nombre de mots pour lesquels au moins un candidat est proposé.

4.3. ENRICHISSEMENT SEMI-AUTOMATIQUE

127

Table 4.4 – Comparaison de diﬀérentes mesures de similarité sur la mesure P5 des
listes de candidats.

Synonymie

EN

FR

V

N

Traductions

EN

FR

Syn. + Trad.
EN
FR

N

V

V

V

N

N
simple 41.4 32.4 58.6 47.3 51.4 37.8 78.7 58.3 51.9 39.0 74.6 55.3
42.5 33.5 58.2 46.8 50.5 38.0 78.7 58.3 51.1 39.3 74.0 55.1
avg
43.4 34.6 60.2 47.9 51.8 38.5 78.3 58.6 51.3 39.4 73.1 54.2
cos
42.0 34.0 59.7 46.7 52.3 38.7 78.2 58.7 52.4 39.7 73.6 54.8
dot
43.2 34.0 60.1 48.2 51.8 38.6 78.7 58.8 51.9 39.8 74.0 54.5
ZKL10

N

V

N

V

4.3.3.2 Résultats de l’évaluation

Les tableaux 4.4 et 4.5 donnent les résultats de la méthode. Le tableau 4.4 pré-
sente les valeurs de la mesure P5, pour les cinq similarités et pour trois sources de
données. Le tableau 4.5 se limite à la mesure « simple », mais présente les résul-
tats pour toutes les sources de données et donne les valeurs des diﬀérentes mesures
d’évaluation.

Notons que pour une même langue, une même partie du discours, et la même
similarité, les résultats sont légèrement meilleurs dans le tableau 4.5 que dans le
tableau 4.4. Cela s’explique par le fait que le tableau 4.4 a été calculé sur des
données extraites de Wiktionary en 2007 [Sajous et al., 2010], alors que le tableau 4.5
a été calculé à partir de données de 2010 [Sajous et al., 2011a]. Cela montre un
autre intérêt de cette méthode endogène : plus la ressource est complète 10 plus les
propositions sont de qualité. Nous pouvons imaginer que cela restera vrai jusqu’à un
point où la plupart des candidats de fortes similarités seront déjà indiqués comme
synonymes.

On observe tout d’abord dans le tableau 4.4 qu’il n’y a pas de diﬀérence de per-
formance ﬂagrante entre les diﬀérentes mesures. C’est pourquoi nous nous limitons
par la suite à la mesure « simple » qui présente la complexité la plus faible.

On remarque que les graphes de traduction donnent de meilleures précisions que
ceux de synonymie. Ce résultat était prévisible car les entrées ont plus de liens de
traduction que de synonymie dans Wiktionary. De plus, ces liens sont répartis sur
un nombre important de langues, ce qui rend l’information relativement ﬁable. Pour
l’anglais, agglomérer les graphes de synonymie et de traduction améliore le rappel
sans détriment notable pour la précision. Pour le français, cela conduit à une baisse
de précision par rapport à l’utilisation du seul graphe de traduction.

Le meilleur rappel est obtenu avec les gloses et les contextes syntaxiques (ta-
bleau 4.5). Ce résultat était attendu également, l’information véhiculée par ces don-
10. voir section 4.2.2.3 pour une étude de l’évolution de Wiktionary.

CHAPITRE 4. ENRICHISSEMENT SEMI-AUTOMATIQUE DE
RÉSEAUX LEXICAUX

128

ANG

N.

A.

V.

A.

FR

R5

s + t 21.9 58.1

3096

syns
trads

8.7
34.6
8.5 51.0

3862
3759
gloses non pondérées 95.6 14.8 42337

1335
1916
6252
gloses pondérées 95.6 15.3 42337 6467
3063

s + t 14.5 47.6

6440

P5
syns 17.4 49.1
9.2 65.7
trads

|N5|
2456
1299
gloses non pondérées 93.5 25.9 13205

5 |
|N +1
1207
853
3421
gloses pondérées 93.5 26.6 13205 3510
1800

Table 4.5 – Impact des sources de données sur le calcul des candidats (similarité
« simple »).
5 |
|N +5
22
3
2
1
29
32
32
54
2
0
1
45
43
43
59
1
2
2
43
34
34
16
11
1
0
1
28
38
38
32
43
15
20
2
3
5
33
50
52
38
56
5
38
3
8
3
48
58
58
43
53

5 |
|N +2
439
406
774
794
805
10.s + 10.t + g 95.0 35.9 13417 4819 1567
102.s + 102.t + g 95.0 35.9 13417 4818 1567
483
655
926
933
1061
10.s + 10.t + g 96.4 23.3 42688 9944 2344
102.s + 102.t + g 96.4 23.3 42688
9942 2345
431
959
2153
609
1342
2223
8852
2389
518
8852 2490
548
3380
1962
918
8916 3655 1352
8917
3644
1351
361
224
480
184
222
243
3627
309
1167
337
3627 1220
3468
719
157
375
513
631
728
1594
3602
729
3668
1600
3913
1640
680
791
3915 1774
478
936
1722
472
730
916
gloses non pondérées 95.8 20.6 15828
3268
607
gloses pondérées 95.8 22.5 15828 3560
693
721
2898
1681
983
1956
5303
1952
5298
5394
1908
16273 5980 2240
172
280
412
544
710
785
3947
530
1628
638
3947 1773
426
1192
3378
669
1062
909
1161
2229
3989
1160
2226
3989
4053
1004
2158
4053 2368 1243

syns 23.9 44.5
trads 24.7 60.4
gloses non pondérées 98.5 27.0
gloses pondérées 98.5 28.1
s + t 37.6 58.0
10.s + 10.t + g 99.2 41.0
102.s + 102.t + g 99.2 40.9
syns 11.9 75.2
6.0 91.4
trads
gloses non pondérées 90.2 32.2
gloses pondérées 90.2 33.6
contextes 86.2 20.7
s + t 15.7 81.3
10.s + 10.t + g 89.5 44.3
102.s + 102.t + g 91.2 43.6
102.s + 102.t + 10.g + c 97.3 41.9
103.s + 103.t + 102.g + c 97.3 45.3
syns 10.4 54.4
5.5 79.7
trads

103.s + 103.t + 102.g + c 98.4 36.7
syns 10.0 68.0
trads 19.0 90.4
gloses non pondérées 95.6 41.2
gloses pondérées 95.6 44.9
contextes 81.8 35.3
s + t 25.7 85.6
10.s + 10.t + g 96.6 55.9
102.s + 102.t + g 96.6 55.8
102.s + 102.t + 10.g + c 98.1 53.2
103.s + 103.t + 102.g + c 98.1 58.4

5 |
|N +4
57
27
34
30
91
125
125
95
41
6
5
110
142
143
115
43
10
13
119
136
136
55
56
12
17
11
105
154
155
143
172
68
94
16
21
34
166
219
218
196
260
30
146
38
45
28
165
216
214
146
223

5 |
|N +3
165
144
154
158
283
455
455
200
178
106
114
348
561
561
216
187
98
100
358
448
448
139
117
91
100
40
243
371
370
347
408
194
245
116
127
181
480
735
736
649
825
86
352
149
198
126
418
580
580
433
604

contextes 84.0 20.9
s + t 15.2 66.9
10.s + 10.t + g 96.5 33.3
102.s + 102.t + g 96.5 33.2

13882
2511
15948
15948
102.s + 102.t + 10.g + c 98.5 33.1 16274

N.

V.

4.3. ENRICHISSEMENT SEMI-AUTOMATIQUE

129

nées étant moins spéciﬁque. Elle permet cependant de proposer des candidats pour
la quasi-totalité des mots. La pondération des gloses par les nombres d’occurrences
améliore légèrement les résultats et l’on peut penser qu’une pondération plus ﬁne
(e.g. en favorisant les termes situés à l’initiale des gloses) produirait une nouvelle
amélioration.

Plus étonnamment, les contextes syntaxiques donnent une précision inférieure,
ce qui ne va pas dans le sens des résultats de Van der Plas et Bouma [2005]. Filtrer
les contextes de basse fréquence permettrait probablement d’améliorer ce score : en
eﬀet, un mot apparaissant avec un unique contexte syntaxique tend à avoir avec
lui une information mutuelle élevée sans que le rapprochement avec un autre mot
apparaissant avec ce même contexte ne soit réellement signiﬁcatif.

Dès lors que l’on introduit les gloses, des candidats sont proposés pour quasi-
ment tous les mots. Cela est intéressant car l’objectif, en vue de l’application, est
d’eﬀectuer des suggestions certes les plus pertinentes possible mais surtout pour un
nombre maximal de mots. Il serait en eﬀet dommage de ne proposer des candidats
que pour un nombre restreint de mots, même si cela permettrait d’avoir un score
de conﬁance élevé (nous atteignons dans ce cas 80 à 90% de listes pertinentes en ne
recourant qu’aux graphes de traduction pour le français, mais alors des propositions
ne sont faites que pour seulement 6 à 20% des mots). Notons que l’utilisation des
gloses et contextes syntaxiques dans les graphes combinés ne fait chuter la précision
qu’au niveau global : au niveau local, les mots pour lesquels les liens de synonymie
et de traduction conduisent à proposer des candidats pertinents ne sont pas aﬀectés
par cette baisse (la proportion P5 baisse mais pas le cardinal N +i
k ).

Les meilleurs résultats sont obtenus en combinant les graphes de synonymie, de
traduction et des gloses pour l’anglais (graphe 10.s+10.t+ g) et celui de synonymie,
traductions et des contextes syntaxiques pour le français (graphe 103.s + 103.t +
102.g + c). L’utilisation de ces graphes nous permet de proposer 5 candidats pour
presque tous les mots et 35 à 60% des listes proposées comportent au moins un
candidat validé par un étalon. Ce résultat nous permet de considérer la méthode
comme valable en vue de l’application envisagée.

Notons enﬁn que les résultats sont meilleurs pour le français que pour l’anglais.
Cela pourrait s’expliquer par une diﬀérence de densité des réseaux initiaux, mais est
surtout dû aux étalons utilisés : DicoSyn, étant une compilation de 7 dictionnaires,
est plus dense que PWN.

4.3.3.3 Qualiﬁcation des relations proposées

Aﬁn de mieux cerner le type de relations capturées par notre mesure de simila-
rité, nous avons étudié la nature des synonymes candidats rejetés par l’étalon choisi.
En utilisant PWN, nous avons réparti les candidats anglais des listes Ωk(v) présentées
plus haut (k ≤ 5) suivant les relations synonymie, hyperonymie, hyponymie, cohypo-

CHAPITRE 4. ENRICHISSEMENT SEMI-AUTOMATIQUE DE
RÉSEAUX LEXICAUX

130

Figure 4.3 – Proportion des candidats par type de relation sémantique.

Table 4.6 – Exemples de candidats appartenant à une relation sémantique autre
que la synonymie.

Catégories Mots
hound
law
fool

Noms

Candidats
greyhound
rule
idiot, dummy

Relations
hyponymie
hyponymie
cohyponymie

Verbes

to represent
to negociate
to blame

to depict
to bargain
to incriminiate

(synset hyperonyme : {simpleton, simple})

troponymie
troponymie
cotroponymie

(synset troponymie : {to charge, to accuse})

nymie pour les noms et synonymie, hyperonymie, troponymie, cotroponymie pour les
verbes 11. Nous avons gardé dans cette évaluation la ﬁnesse de granularité de PWN
en respectant ses déﬁnitions strictes : deux mots sont synonymes s’ils apparaissent
dans un même synset, et nous considérons seulement les relations strictes d’hyper-
onymie/hyponymie/cohyponymie (e.g. w1 est en relation d’hyperonymie avec w2 si
w2 apparaît dans un synset qui est un ﬁls direct d’un synset dans lequel apparaît w1).
La ﬁgure 4.3 montre la répartition, en termes de type de relation, des candidats
calculés. Pour tous les noms et verbes communs à Wiktionary et PWN, nous avons
considéré des listes de 1 à 5 candidats, puis supprimé ceux absents de PWN. Nous
pouvons voir qu’une grande partie des candidats non pertinents pour la synonymie
recouvre une autre réalité sémantique. Le tableau 4.6 donne quelques exemples de
candidats appartenant à une relation sémantique autre que la synonymie. Sans que
11. Nous avons également considéré, puis écarté les relations d’antonymie, holonymie/méronymie

et causalité, qui concernaient entre 1%(cid:24) et 0.1%(cid:24) de cas.

123450%10%20%30%40%50%60%70%80%90%100%aucunerelationcohyponymiehyponymiesynonymie(graphe syns+trads)Nomsk23105195.65.123450%10%20%30%40%50%60%70%80%90%100%aucunerelationcotroponymietroponymiesynonymie(graphe trads)Verbes261161146khyperonymiehyperonymie4.4. UNE IMPLÉMENTATION FONCTIONNELLE : LE SYSTÈME
131
WISIGOTH
les résultats soient directement comparables (enrichissement endogène vs. extraction
en corpus, PWN vs. EuroWordNet néerlandais), nous pouvons noter que la similarité
de Heylen et al. [2008] produit autant de cohyponymes que de synonymes alors que
nos mesures de similarité sur les sources de données utilisées produisent deux fois
plus de synonymes que de cohyponymes. Ce dernier point indique qu’il est intéressant
que l’application ne permette pas seulement aux utilisateurs de valider ou d’invalider
un candidat synonyme, mais permette aussi de « réorienter » chaque candidat vers
une autre relation sémantique.

4.4 Une implémentation fonctionnelle : le système

Wisigoth

Pour mettre en œuvre le système d’enrichissement avec validation décrit plus
haut, nous avons implémenté le système Wisigoth (pour WIktionarieS Improvement
by Graph-Oriented meTHods), constitué en trois parties :

• une chaîne de traitement hors-ligne,
• un serveur de candidats en ligne,
• une extension Firefox.

La chaîne de traitement (hors-ligne) extrait, à partir des dumps de Wiktionary,
les relations de synonymie, de traduction, etc. et calcule les relations candidates
comme présenté dans les sections précédentes. Ces listes de candidats sont données
au serveur de candidats, qui les propose quand il reçoit une requête pour un mot
ciblé donné. Ce serveur est interrogé par l’extension Firefox qui est la seule partie
« visible » du système. On peut en voir une capture d’écran en ﬁgure 4.4.

Lorsqu’un internaute installe cette extension et visite une page française ou an-
glaise de Wiktionary, le serveur de candidats est interrogé et renvoie une liste or-
donnée de candidats pour le(s) mot(s) en cours de consultation. Ces candidats sont
alors présentés à l’internaute (cf. ﬁgure 4.4). S’il les valide (bouton « + »), l’exten-
sion prend en charge l’ajout du synonyme dans Wiktionary et l’édition du code wiki
correspondant. Un champ texte libre permet également de proposer un synonyme
non-suggéré. Indépendamment de notre méthode d’enrichissement, Wisigoth per-
met ainsi aux internautes non familiers avec le code wiki de contribuer. Pour rester
proche du principe wiki, nous n’avons pas mis en place de système de validation
croisée et les ajouts sont publiés immédiatement. Néanmoins, facilitant l’ajout de
synonymes, nous devons également en faciliter leur suppression : un bouton « - »
est ajouté à chaque synonyme présent, qui déclenche sa suppression. Enﬁn, un sys-
tème de liste noire permet à un contributeur de signaler une suggestion jugée non
pertinente (bouton « × »). Cette suggestion ne lui sera plus faite et permettra aux
autres candidats de remonter dans la liste. Au-delà d’un seuil (réglé pour l’instant

CHAPITRE 4. ENRICHISSEMENT SEMI-AUTOMATIQUE DE
RÉSEAUX LEXICAUX

132

Figure 4.4 – Exemple de propositions faites par Wisigoth sur le Wikitionary fran-
çais pour « enseignant ».

arbitrairement à 3) de mises sur listes noires individuelles, la proposition est insérée
dans une liste noire globale de manière à ne plus être proposée à aucun utilisateur.

4.5 Conclusion du chapitre

Nous avons dans ce chapitre proposé un système (en grande partie) endogène
d’enrichissement semi-automatique de ressources lexicales. Ce système se base sur
un calcul de similarités, par des marches aléatoires courtes, entre sommets d’un
graphe biparti construit à partir de la ressource.

Nous appliquons ce système sur Wiktionary. En eﬀet nous avons présenté une
étude qui montre que la couverture lexicale de cette ressource construite « par les
foules » croit plus vite que son nombre de relations. Un système d’aide à l’ajout
de relations lexicales est donc justiﬁé. Nous avons validé le système proposé grâce
à des ressources étalons. Ce système est capable de proposer des courtes listes de
candidats pour la quasi totalité des entrées de Wiktionary et de 30 à 60% de ces
listes permettent d’ajouter au moins une relation validée par une ressource étalon
(les résultats dépendent de la partie du discours et de la langue considérée).

Enﬁn nous avons présenté une implémentation sous forme d’une extension Fi-
refox. Celle-ci permet de proposer des candidats synonymes à l’utilisateur lorsqu’il
navigue sur Wiktionary. Ainsi il peut les marquer comme incorrects ou les ajouter
dans la ressource collaborative.

Cette extension Firefox est une preuve de concept qui a été développé en 2010 et
2011. Elle était alors pleinement fonctionnelle, malheureusement nous n’avons pu la

4.5. CONCLUSION DU CHAPITRE

133

maintenir et des modiﬁcations tant dans Firefox que dans les formats et structures
de données de Wiktionary font qu’elle n’est plus utilisable aujourd’hui. Nous sommes
néanmoins convaincus que ce type de solution d’aide à la construction de ressource
est nécessaire pour développer des ressources de qualité à moindre coût, que ce soit
« par les foules » ou par des lexicographes experts. Notamment le développement
d’outils (extension Firefox par exemple) d’assistance à la contribution sur Wiktio-
nary est, il nous semble, quasiment indispensable pour réellement voir la qualité et le
nombre de relations lexicales de Wiktionary atteindre un niveau comparable à celui
de Wikipedia. Il serait intéressant de développer un outil d’assistance à la contribu-
tion, similaire à l’extension que nous avons proposé, mais qui puisse se connecter à
diﬀérents serveurs de candidats. Ainsi chacun (particulier, entreprise ou équipe de
recherche) pourrait facilement proposer un serveur de candidats. Un contributeur
disposerait alors des diﬀérentes propositions de candidats. Il serait possible d’évaluer
a posteriori les serveurs ayant amené le plus de contributions.

Concernant la méthode de calcul des candidats, diﬀérentes améliorations sont
envisageables. Nous avons vu que la construction du graphe est primordiale. Il serait
possible d’explorer diﬀérentes méthodes de pondération, en particulier pour les gloses
et les contextes syntaxiques. Par exemple en pondérant les termes des gloses de
manière inversement proportionnelle à leur position dans la déﬁnition (les premiers
termes d’une déﬁnition étant souvent plus semblables au mot déﬁni que les derniers).
Il serait possible d’évaluer les diﬀérentes mesures de similarité introduites au cha-
pitre 2. Mais cette perspective n’apparaît pas comme prioritaire à la vue des faibles
diﬀérences observées ici entre les quelques méthodes utilisées. Ces faibles diﬀérences
sont par ailleurs conﬁrmées par les fortes corrélations constatées au chapitre 2 entre
beaucoup de mesures. De surcroît, la taille importante des graphes utilisés ici limite
l’utilisation de méthodes globales. Il faudrait aussi vériﬁer que ces méthodes peuvent
s’appliquer sur un graphe biparti. Il serait tout de même intéressant 12 d’évaluer la
conﬂuence (introduite en sous-section 2.2.2). Cela étant dit, le fait que la conﬂuence
soit déﬁnie entre 0 et 1 n’a aucun intérêt ici, seule la prise en compte du degré
d’arrivée peut être intéressante. Cela aurait comme eﬀet de pénaliser les candidats
de forts degrés par rapport à ceux de faibles degrés. Il n’est donc pas sûr que cela
améliore les résultats, car dans une optique de construction de ressource, le fait de
privilégier les candidats de forts degrés peut être souhaitable : un peu à la manière
de l’attachement préférentiel, les liens se forment certainement plus facilement avec
des sommets forts.

Notons aussi que l’application présentée ici peut être mise en parallèle avec le
problème dit « de recommandation ». Ce problème consiste typiquement à proposer
des produits à un client étant donné l’ensemble des achats passés [Ricci et al., 2011].
Aussi, la littérature s’intéressant à la prédiction de liens dans un graphe biparti
12. Au moins pour la cohérence de ce rapport.

CHAPITRE 4. ENRICHISSEMENT SEMI-AUTOMATIQUE DE
RÉSEAUX LEXICAUX

134

[Allali et al., 2013] peut proposer des méthodes (spéciﬁques aux graphes bipartis)
qui méritent d’être évaluées sur cette application.

Enﬁn l’évaluation pourrait proﬁter des propositions faites au chapitre précédent.
Notamment il pourrait être intéressant de mesurer la conﬂuence dans le graphe
étalon des candidats qui sont, pour le moment, rejetés par l’étalon.

135

Deuxième partie

Clustering de graphe biparti,

théories et applications

136

137

Chapitre 5
Clustering de graphe biparti

On s’intéresse dans ce chapitre au problème consistant « à regrouper intelli-
gemment les sommets d’un graphe biparti ». Cette formulation est volontairement
vague pour nous permettre de recouper les approches généralement proposées sous
diverses étiquettes : « détection de communautés », « conceptualisation » ou encore
« clustering ».

Rappelons qu’un graphe biparti est un graphe comportant deux types de som-
mets et des liens seulement entre des sommets de types diﬀérents. C’est une relation
(binaire) entre deux ensembles de sommets. Nous considérons dans ce chapitre que
l’un de ces ensembles correspond à des objets, alors que le second correspond à des
propriétés.

Diﬀérentes motivations peuvent amener à déﬁnir plus clairement ce problème
de « clustering », par exemple : comment diviser un graphe en diﬀérents groupes
de façon à minimiser le nombre de liens entre ces groupes ? comment, pour simpli-
ﬁer la représentation des objets, peut-on les classer, d’après leurs propriétés, dans
un nombre raisonnable de catégories « naturelles » ? quels sont les objets formant
des groupes indissociables et quelle peut être leur organisation hiérarchique ? Ces
diﬀérentes motivations ont amené plusieurs communautés de chercheurs à proposer
des approches diﬀérentes. Pour autant il est intéressant de mettre en parallèle ces
approches. C’est un travail auquel nous contribuons dans ce chapitre, en établissant
notamment une traduction de l’analyse formelle de concepts en termes de graphe.
Ce parallèle permet d’apporter un éclairage intéressant sur les méthodes de la
littérature, et nous amène en particulier vers une problématique du clustering de
graphe biparti souvent ignorée : Comment partitionner les objets sans imposer de
partition correspondante sur les propriétés ? Nous proposons une méthode simple
qui répond à cette question. Cette méthode est basée à la fois sur l’analyse formelle
de concepts et sur un algorithme de détection de communautés.

Enﬁn, en utilisant des mesures de similarité déﬁnies dans la première partie
de ce rapport, nous proposons une méthode de prétraitement d’un graphe biparti

138

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

éventuellement pondéré. Cette méthode permet de ﬁltrer un graphe biparti et de le
binariser s’il est pondéré, pour simpliﬁer les résultats obtenus par l’analyse formelle
de concepts.

Ces contributions sont organisées en quatre sections. En section 5.1 nous dres-
sons un panorama des diﬀérentes familles d’approches. En section 5.2 nous dressons
un parallèle entre l’analyse formelle de concepts et l’analyse de graphe biparti. Ce
parallèle nous permet d’apporter deux contributions dans les sections suivantes. En
section 5.3 nous proposons et évaluons une méthode pour catégoriser les objets, et
ce même si les propriétés sont fortement partagées entre les catégories. Enﬁn en
section 5.4, après avoir décrit les diﬀérentes extensions envisageables ou existantes
de l’analyse formelle de concepts pour des données graduelles ou empreintes d’in-
certitude, nous proposons une méthode basée sur des marches aléatoires permettant
de simpliﬁer et de binariser un graphe biparti. Nous proposons ensuite une première
évaluation de cette méthode sur des graphes artiﬁciels et sur des graphes réels, avant
de conclure en section 5.5.

5.1 Diﬀérentes familles d’approches

L’objectif de cette section est de dresser un panorama rapide des diﬀérents pro-
blèmes et diﬀérentes familles d’approches s’intéressant au « regroupement intelli-
gent » des sommets d’un graphe biparti. Nous partons volontairement de cette déﬁ-
nition vague pour essayer d’apporter une formulation plus claire à ce problème, en
explorant les approches existant dans la littérature.

Nous détaillons les méthodes citées seulement quand cela nous semble nécessaire
pour la suite du chapitre, ou pour comprendre les similarités ou diﬀérences existant
avec d’autres approches. Aussi nous nous eﬀorçons de citer les travaux ayant établi
ou utilisé des parallèles entre diﬀérentes approches ou familles d’approches.

Notons que beaucoup de méthodes d’apprentissage automatique supervisé ont
été appliquées sur ce type de données avec un objectif semblable. Ces méthodes né-
cessitent un jeu d’entraînement sur lequel on connaît le résultat escompté. Nous nous
intéressons ici uniquement aux méthodes dites « non supervisées », ne nécessitant
pas un tel jeu d’entraînement.

5.1.1 Partitionnement de graphe biparti

Le partitionnement de graphe est un problème classique de la théorie des graphes
[Bichot et Siarry, 2010]. Il consiste, sous sa forme première (c’est-à-dire le partition-
nement dit « contraint »), à découper un graphe en un certain nombre de parties de
taille semblable, en minimisant le « cut » , c’est-à-dire le nombre d’arêtes entre ces
parties. L’objectif n’est pas (a priori) de révéler une structure sous-jacente existante

5.1. DIFFÉRENTES FAMILLES D’APPROCHES

139

dans les données, mais plutôt d’optimiser un découpage en un nombre imposé de
parties de taille ﬁxée.

Bien que pensé sur des graphes uni-partis ce problème a aussi été posé pour
des graphes bipartis. Et certains algorithmes ont pu être adaptés. On peut citer en
particulier [Dhillon, 2001] et [Hu et al., 2006].

Le principal inconvénient de ce type d’approche est que le nombre de clusters est
un paramètre ﬁxé. Aussi ces méthodes cherchent à construire autant que possible
des clusters de taille semblable, ce qui ne correspond pas forcément à la réalité
des données. Notons qu’une comparaison de ce type d’approches (sur des graphes
uni-partis) avec des méthodes de détection de communautés (voir section 5.1.3) est
faite dans [Newman, 2006]. Dans ce papier le problème de la taille ﬁxée des clusters
est soulevé, mais aussi la pertinence des approches optimisant la mesure de cut est
remise en question, quand on s’intéresse à faire émerger des groupes « existant »
dans les données. L’idée défendue par Newman est que les groupes pertinents ne
sont pas ceux ayant le moins de liens possible entre eux, mais ceux ayant moins de
liens entre eux « qu’attendu ». Il faut chercher un découpage qui optimise le nombre
de liens entre les groupes, par rapport au nombre de liens qui existerait entre ces
mêmes groupes sur un graphe « neutre » partageant certaines des caractéristiques du
graphe étudié (null model). La mesure de modularité permet de mesurer ce rapport
(voir section 5.1.3).

L’approche proposée par Hu et al. [2006] consiste à appliquer une méthode de
partitionnement de graphe biparti sur un graphe reliant les objets et des hyperclique
patterns qui sont des bicliques maximales approchées. Cette méthode est très simi-
laire à celle que nous proposons en section 5.3. Elle souﬀre néanmoins des faiblesses
que nous venons d’évoquer concernant le partitionnement de graphe.

5.1.2 Bi-clustering

Le terme « bi-clustering » fait référence, en général, aux méthodes cherchant à
regrouper les objets similaires avec les propriétés qui les rapprochent. Les méthodes
en elles-même ne sont pas détaillées dans cette section, pour un état de l’art plus
complet nous renvoyons à [Busygin et al., 2008] ou à [Eren et al., 2012]. Notons que
le principal champ d’application des méthodes de bi-clustering concerne l’analyse de
données génétiques, en particulier l’exploitation des puces à ADN (ou microarray),
voir [Babu, 2004].

À l’inverse du partitionnement de graphe, ici le résultat ne doit pas simplement
être optimal pour une fonction de coût donnée, mais doit « faire sens » vis à vis des
données. On dit parfois que le regroupement ou le découpage doit être « naturel » 1.
1. Le terme « naturel » peut sembler bien vague, et c’est certainement là toute la diﬃculté du
clustering : il est très diﬃcile de déﬁnir précisément ce qui est recherché. Précisons tout de même
que l’idée derrière ce terme « naturel » est que les méthodes doivent rendre visible un découpage

140

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

Cela amène une plus grande liberté dans la formalisation de ce qu’est un bon dé-
coupage, mais aussi une diﬃculté accrue à évaluer les performances des diﬀérentes
méthodes.

L’autre diﬀérence évidente avec le partitionnement de graphe est que les données
ne sont plus représentées par un graphe. Souvent ces algorithmes travaillent sur une
matrice de nombres réels, où chaque ligne correspond à un objet et chaque colonne
correspond à une propriété. Mais cette diﬀérence n’est, a priori, qu’une question de
point de vue : une matrice de réels n’est rien d’autre qu’un graphe biparti pondéré.
Est-ce que ces diﬀérents points de vue sous-entendent des hypothèses diﬀérentes sur
la structure des données ? sur leur densité ? ou sur la signiﬁcation des valeurs réelles ?
Nous allons voir dans la section suivante que les méthodes que l’on nomme « dé-
tection de communautés » sont en quelque sorte le produit du partitionnement de
graphe et du clustering : c’est-à-dire la recherche d’un regroupement « naturel » des
sommets d’un graphe. Malheureusement il n’existe pas encore, à notre connaissance,
de comparaison théorique ou expérimentale entre ces méthodes (en particulier de
celles adaptées aux graphes bipartis) et les méthodes de bi-clustering. Une telle com-
paraison pourrait pourtant se montrer fertile. D’autant plus que certaines approches
de bi-clustering se basent sur une binarisation des données, citons par exemple [Serin
et Vingron, 2011], ce qui rend le parallèle avec le clustering de graphe encore plus
évident.

5.1.3 Détection de communautés sur un graphe biparti

Comme nous l’avons vu au chapitre d’introduction, le problème de détection de
communautés est l’un des problèmes maintenant classique de l’analyse des graphes
de terrain. Il consiste à rechercher les regroupements de sommets formant des « com-
munautés » intelligibles. Le terme « communauté » provient de l’application de pré-
dilection de ces méthodes aux réseaux sociaux. Nous préférons plus généralement
parler de clustering de graphe, en eﬀet la détection de communautés se diﬀérencie
du clustering principalement par la forme des données initiales. De la même façon
que pour le clustering, il n’existe pas une déﬁnition formelle universelle de ce que
doit être une « communauté ». Presque chaque algorithme propose, plus ou moins
implicitement, sa propre déﬁnition.

La plupart des méthodes de la littérature s’intéressent aux graphes uni-partis.
Quelques méthodes ont tout de même été proposées pour les graphes bipartis. En
particulier plusieurs adaptations de la modularité de Newman [Clauset et al., 2004]
ont été publiées. La modularité de Newman est une mesure de qualité d’un parti-
tionnement des sommets d’un graphe, plus complexe que la mesure de cut utilisée en
partitionnement de graphe (voir section 5.1.1). Une première adaptation aux graphes
qui existerait, de manière peu lisible, dans les données.

5.1. DIFFÉRENTES FAMILLES D’APPROCHES

141

bipartis a été proposée par Barber [2007]. Une autre adaptation a été développée
par Liu et Murata [2010], cette dernière autorise un partitionnement des objets et
un partitionnement des propriétés tels que ces deux partitionnements ne soient pas
en correspondance directe (à un cluster d’objets ne correspond pas directement un
cluster de propriétés).

D’autres approches utilisent les cliques comme noyaux des communautés. Ci-
tons en particulier la méthode CFinder de Palla et al. [2005] qui consiste, sur un
graphe uni-parti, à déﬁnir les clusters comme des chaînes de k-cliques adjacentes
(une k-clique est une clique de taille k, et deux k-cliques sont adjacentes si et seule-
ment si elles ont, au moins, k − 1 sommets en commun). Cette méthode a été
adaptée aux graphes bipartis par Lehmann et al. [2008] : les clusters deviennent des
chaînes de Ka,b-bicliques adjacentes. Une Ka,b-biclique étant une biclique comportant
a sommets-objets et b sommets-propriétés, et deux Ka,b-bicliques sont adjacentes si
et seulement si elles partagent a − 1 sommets-objets et b − 1 sommets-propriétés.
Dans [Navarro et al., 2011], nous avons proposé une adaptation simple de la
méthode Walktrap de Pons et Latapy [2006]. Cette méthode se base sur un algo-
rithme de clustering hiérarchique utilisant une distance entre groupes de sommets
calculée par des marches aléatoires en temps courts semblablement à celles que nous
présentons au chapitre 2.

Notons aussi que certaines méthodes prévues pour les graphes uni-partis peuvent
être appliquées directement sur des graphes bipartis. Le caractère biparti du graphe
est alors ignoré, les sommets étant regroupés indépendamment de leur type.

En particulier, nous utiliserons dans la suite de ce chapitre la méthode Infomap
[Rosvall et Bergstrom, 2008], qui est reconnue comme l’une des méthodes de cluste-
ring de graphe les plus performantes [Lancichinetti et Fortunato, 2009]. La méthode
consiste à rechercher les clusters qui compressent au mieux la description de la tra-
jectoire d’un marcheur aléatoire sur le graphe. Cette trajectoire étant décrite sur
deux niveaux en fonction des clusters : quand le marcheur entre dans un cluster, le
nom du cluster est noté, ensuite seuls les noms des sommets sont inscrits (avec une
notation locale au cluster). De cette manière, un même nom court peut être utilisé
pour nommer deux sommets s’ils sont dans des clusters diﬀérents. Une description
concise de la trajectoire est donc possible si les clusters sont tels que le marcheur
tend à rester à l’intérieur de ceux-ci. Cela correspond à l’idée que les marcheurs aléa-
toires sont « capturés » quand ils entrent dans un cluster, puisqu’un cluster n’est
que faiblement connecté aux autres. Cette idée a été utilisée de diﬀérentes manières
dans la littérature sur le clustering de graphe [Pons et Latapy, 2006; Schaeﬀer, 2007;
Delvenne et al., 2010].

142

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

5.1.4 Analyse formelle de concepts

L’analyse formelle de concepts (AFC dans la suite) est une méthode d’analyse de
données prenant la forme d’une relation binaire entre objets et propriétés. Une telle
relation peut être représentée comme un tableau où chaque ligne (resp. colonne)
correspond à un objet (resp. une propriété), et une croix entre une ligne et une
colonne indique que l’objet correspondant vériﬁe la propriété correspondante. Un
exemple est donné dans le tableau de gauche de la ﬁgure 5.1 (page 147).

L’AFC telle que nous la présentons ici a été introduite par Wille [1982]. Notons
que les constructions qui forment la base de l’AFC ont été introduites auparavant par
Barbut et Monjardet [1970, chap. V]. Nous présentons ici les concepts et déﬁnitions
de base utiles à l’AFC ainsi que certaines méthodes utilisées pour rendre plus robuste
l’AFC. Contrairement aux méthodes abordées jusque là, nous introduisons donc
assez précisément l’AFC. Cela nous sera utile pour la suite de ce chapitre. Nous
revenons en eﬀet sur l’AFC dans les sections suivantes, en particulier en section 5.2.1.
Pour une documentation de référence sur l’AFC nous renvoyons à [Ganter et Wille,
1999].

5.1.4.1 Cadre théorique de l’AFC

Soit R une relation binaire entre un ensemble d’objets O et un ensemble de
propriétés P. On note R = (O, P, R) le tuple formé par l’ensemble d’objets, l’en-
semble de propriétés et la relation binaire. On nomme contexte formel ce tuple R.
La notation (x, y) ∈ R signiﬁe que l’objet x vériﬁe la propriété y. Soit R(x) = {y ∈
P|(x, y) ∈ R} l’ensemble des propriétés vériﬁées par l’objet x et R−1(y) = {x ∈
O|(x, y) ∈ R} l’ensemble des objets vériﬁant la propriété y.
respectivement appelés intension et extension, tels que ∀Y ⊆ P et ∀X ⊆ O :

L’AFC déﬁnit deux opérateurs ensemblistes, que nous noterons ici (.)∆ et (.)−1∆,

X∆ = {y ∈ P|∀x ∈ X, (x, y) ∈ R}
Y −1∆ = {x ∈ O|∀y ∈ Y, (x, y) ∈ R}

(5.1)
(5.2)

L’intension de X, X∆ est l’ensemble des propriétés possédées par tous les objets de
X et l’extension de Y , Y −1∆ est l’ensemble des objets vériﬁant toutes les propriétés
de Y . Ces deux opérateurs induisent une connexion de Galois antitone entre 2O et
2P. C’est-à-dire que la propriété suivante est vériﬁée :
X ⊆ Y −1∆ ⇔ Y ⊆ X∆.

(5.3)
Une paire telle que X∆ = Y et Y −1∆ = X est appelée concept formel. X est
appelé l’extension et Y l’intension. Un concept formel est donc une paire (X, Y ) telle
que X est l’ensemble d’objets vériﬁant toutes les propriétés de Y et Y est l’ensemble

5.1. DIFFÉRENTES FAMILLES D’APPROCHES

143

de propriétés partagées par tous les objets de X. On montre qu’un concept formel
correspond à une paire (X, Y ) maximale telle que :

X × Y ⊆ R.

(5.4)

Il est possible de déﬁnir une relation d’ordre partiel sur l’ensemble des concepts

formels. Si (X1, Y1) et (X2, Y2) sont deux concepts, on a :

(X1, Y1) ≤ (X2, Y2) ⇔ X1 ⊆ X2 ⇔ Y2 ⊆ Y1

(5.5)

Avec cet ordre partiel, pour chaque paire de concepts ((X1, Y1), (X2, Y2)) il existe
un unique concept étant le plus grand des concepts plus petits. C’est le concept
déﬁni par les objets X1 ∩ X2 et toutes les propriétés vériﬁées par tous ces objets (on
parle d’opération meet). Symétriquement il existe, pour chaque paire de concepts,
un unique concept étant le plus petit des concepts plus grands. C’est le concept
déﬁni par les propriétés Y1 ∩ Y2 et tous les objets vériﬁant toutes ces propriétés (on
parle d’opération join). Ces deux opérations meet et join forment un treillis sur
l’ensemble des concepts, que l’on nomme treillis des concepts.

Il convient de noter que les notations utilisées ici ne sont pas celles utilisées
classiquement dans la littérature de l’AFC. Ces notations proviennent de la théorie
des possibilités, et sont introduites ici par souci de cohérence avec la suite de ce
chapitre. En eﬀet une extension possibiliste de l’AFC est présentée en section 5.2.1.

5.1.4.2 Approximation de l’AFC

Le principal problème de l’AFC est sa sensibilité au bruit. Le nombre de concepts
explose très vite lorsque les données sont bruitées, en particulier si les données
sont de grande taille (nous allons le voir dans l’évaluation en section 5.4.3). Cela
rend l’AFC quasiment inutilisable telle quelle dans la pratique. Diﬀérentes méthodes
d’approximation, de post-traitement ou de pré-traitement ont ainsi été proposées.
Notons qu’en section 5.4, nous proposons une méthode de pré-traitement permettant
de simpliﬁer un contexte pour réduire le nombre de concepts formels produits. Aussi
la méthode proposée en section 5.3 peut être vue comme une technique de post-
traitement pour simpliﬁer le résultat de l’AFC.

La principale approche permettant de simpliﬁer l’ensemble des concepts formels
consiste à sélectionner les concepts pertinents en utilisant diﬀérentes mesures [Kli-
mushkin et al., 2010]. La mesure de stabilité [Kuznetsov et al., 2007; Jay et al., 2008]
est la plus couramment utilisée. Plus un concept (X, Y ) a de sous-ensembles X0 de
X tels que (X0)∆ = Y plus sa stabilité est grande. Cela signiﬁe en pratique que ces
objets n’ont en général pas de propriété à l’extérieur de Y , ou en d’autres termes
que le concept a de grandes chances de rester « stable » si on lui retire quelques
objets.

144

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

Une telle approche ne fait que sélectionner des concepts formels particuliers parmi
les concepts existants. Cela ne permet donc pas de regrouper plusieurs contextes
formels séparés uniquement parce que quelques objets ne vériﬁent pas quelques pro-
priétés. Une autre famille d’approches consiste à regrouper les concepts formels sur
la base d’une mesure de similarité entre concepts. On parle de factorisation du
treillis des concepts [Belohlavek, 2000; Belohlavek et al., 2007]. Souvent la similarité
est basée sur les contextes eux-mêmes mais il est aussi envisageable de déﬁnir une
similarité utilisant une ressource externe. Formica [2008], par exemple, utilise une
ressource lexicale (WordNet) pour mesurer une similarité entre concepts.

Il est aussi possible de déﬁnir des concepts formels approchés. En eﬀet l’opérateur
X∆ peut être assoupli en considérant qu’il recouvre les propriétés partagées par « la
plupart » des objets de X plutôt que par tous [Dubois et Prade, 2012]. Cela revient
à déﬁnir un opérateur X∆,k qui autorise au plus k exceptions parmi les objets (en
supposant que X comporte plus de k éléments). De la même manière, on peut déﬁnir
un opérateur Y ∆,j qui autorise au plus j exceptions parmi les propriétés. Alors un
concept formel approché est une paire (X, Y ) telle que Y = X∆,k et X = Y ∆,j. Un
tel concept a au plus k « trous » par colonne et au plus j par ligne. Nous allons voir
dans la sous-section suivante que cette idée est utilisée par des méthodes cherchant
des itemset tolérants aux erreurs.

5.1.5 Recherche d’itemsets fréquents

Une autre approche qui doit être citée est la recherche d’itemsets fréquents. Ces
méthodes sont particulièrement utilisées pour analyser des bases de transactions :
l’ensemble des achats eﬀectués dans un magasin. Les méthodes d’extraction de règles
d’association cherchent des ensembles d’articles (itemsets) fréquemment présents
ensemble dans des bases de transactions [Agrawal et al., 1993; Tan et al., 2005].
C’est-à-dire, plus simplement des ensembles d’articles souvent achetés ensemble. La
recherche de tels itemsets peut être mise en relation avec l’AFC [Pasquier et al.,
1999]. La base de transaction est vue comme un contexte formel où chaque transac-
tion correspond à un objet et chaque article dans cette transaction correspond à une
propriété vériﬁée par l’objet. Alors l’intension de chaque concept formel correspond
à un itemset fermé. Ensuite les itemsets fréquents peuvent être trouvés en ﬁltrant
le treillis des concepts. En particulier la règle : Y0 ⇒ Y \ Y0 n’a pas d’exception si
et seulement si ((Y0)−1∆)∆ = Y avec Y0 ⊂ Y .

La présence de bruit dans les données a motivé le développement de diﬀérentes
méthodes pour rechercher des itemsets tolérants aux erreurs [Gupta et al., 2008].
Brièvement, l’idée est qu’il n’est plus nécessaire que chaque article d’un itemset ap-
paraisse absolument dans chaque transaction support. Cette idée est très proche de
l’idée de tolérer des « trous » dans les concepts formels (voir sous-section précédente).
Une remarque intéressante qui ressort de la littérature à propos des itemsets appro-

5.2. DE L’ANALYSE FORMELLE DE CONCEPTS AU
CLUSTERING DE GRAPHE BIPARTI

145

chés est qu’il est dangereux de considérer uniquement la proportion de « trous » dans
les concepts, il faut aussi considérer leurs « positions relatives », c’est-à-dire est-ce
que ces « trous » sont tous sur les mêmes objets ou propriétés. On distingue en eﬀet
les itemsets approchés faibles (weak error-tolerant itemsets) et les itemsets appro-
chés forts (strong error-tolerant itemsets) [Yang et al., 2001]. Dans le premier cas la
tolérance est globale, alors que dans le second cas le nombre d’articles manquants
est limité sur chaque transaction support, et le nombre de transactions manquantes
à un article particulier est, lui aussi, limité. De plus un itemset approché peut être
plus fortement contraint en imposant qu’une certaine proportion des transactions
supports inclue tous les articles de l’itemset [Cheng et al., 2006].

Par ailleurs, il est envisageable de regrouper les itemsets en se basant sur une
mesure de similarité. Cette similarité peut par exemple consister à mesurer à quel
point l’itemset approché résultant de l’union est dense [Cerf et al., 2009]. Ce type
d’approche est à mettre en lien avec la méthode que nous présentons en section 5.3
et avec les techniques de factorisation du treillis des concepts présentées dans la
sous-section précédente.

Notons enﬁn que les méthodes d’extraction d’itemsets fréquents ont inspiré des
méthodes de bi-clustering (voir section 5.1.2) appliquées en particulier à des données
issues de la biologie [Serin et Vingron, 2011].

5.2 De l’analyse formelle de concepts au cluste-

ring de graphe biparti

Cette section présente un parallèle formel entre une extension de l’AFC et l’ana-
lyse de graphe biparti. Nous présentons, en section 5.2.1, cette extension possibiliste
de l’AFC, avant de présenter, en section 5.2.2, comment cette extension de l’AFC
peut être comprise en termes de graphes bipartis.

Notons que ce travail a fait l’objet de trois publications : [Gaume et al., 2010],

[Gaume et al., 2013] et [Navarro et al., 2012b].

5.2.1 AFC, extension possibiliste

Nous présentons dans cette sous-section une extension de l’AFC qui a été intro-
duite dans [Dubois et al., 2007] et développée dans [Djouadi et al., 2010b; Dubois
et Prade, 2012].
Soit R = (O, P, R) un contexte formel comme déﬁni en section 5.1.4.1. Nous
avons vu dans cette même section 5.1.4.1 que l’AFC déﬁnit deux opérateurs ensem-

146

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

blistes, l’intension et l’extension, tels que ∀Y ⊆ P et ∀X ⊆ O :

X∆ = {y ∈ P|∀x ∈ X, (x, y) ∈ R}
Y −1∆ = {x ∈ O|∀y ∈ Y, (x, y) ∈ R}

(5.6)
(5.7)

Un parallèle entre l’AFC et la théorie des possibilités a mis en avant trois autres
opérateurs ensemblistes remarquables : (.)Π, (.)N et (.)∇. Ces trois opérateurs (et
l’opérateur d’intension déjà présenté) peuvent être déﬁnis de la manière suivante,
∀X ⊂ O :

XΠ = {y ∈ P|R−1(y) ∩ X 6= ∅}
X N = {y ∈ P|R−1(y) ⊆ X}
X∆ = {y ∈ P|R−1(y) ⊇ X}
X∇ = {y ∈ P|R−1(y) ∪ X 6= O}

(5.8)
(5.9)
(5.10)
(5.11)

Notons que la déﬁnition donnée par l’eq. (5.10) est équivalente à celle donné par
l’eq. (5.6). Ces opérateurs peuvent être compris de la manière suivante :

• XΠ est l’ensemble des propriétés possédées par au moins un objet de X,
• X N est l’ensemble des propriétés telles que chaque objet vériﬁant l’une d’elles
est nécessairement dans X ; ou, dit autrement, X N est l’ensemble des proprié-
tés vériﬁées seulement par les objets de X (mais pas forcément par tous),

• X∆ est, comme nous l’avons déjà vu, l’ensemble de propriétés partagées par

tous les objets de X,

• Et enﬁn, X∇ est l’ensemble des propriétés qui ne sont pas vériﬁées par certains

objets en dehors de X.

Les opérateurs (.)−1Π, (.)−1N, (.)−1∆ et (.)−1∇ sont déﬁnis similairement sur un
ensemble de propriétés Y en substituant R−1 à R et en remplaçant P par O et
inversement O par P.
Il est courant de considérer que la relation R est telle que pour toute propriété
y ∈ P : R−1(y) 6= ∅ et R−1(y) 6= O, ce qui signiﬁe qu’il n’y a pas de propriété
vériﬁée par aucun objet, ni de propriété vériﬁée par tous les objets. On parle de
bi-normalisation. Avec cette bi-normalisation on a :

(5.12)
Si cette même bi-normalisation est valable sur l’ensemble des objets (∀x ∈ O on a
R(x) 6= ∅ et R(x) 6= P) alors cette propriété tient pour les opérateurs inverses :

X N ⊆ XΠ et X∆ ⊆ X∇

X−1N ⊆ X−1Π et X−1∆ ⊆ X−1∇

(5.13)

5.2. DE L’ANALYSE FORMELLE DE CONCEPTS AU
CLUSTERING DE GRAPHE BIPARTI

147

1 2 3 4 5 6 7 8
× × × ×
× ×
× × × ×
× × × ×

a
b
c
d
e
f
g × × × ×
h × × × ×
×
i

×

× ×

×

Figure 5.1 – Premier exemple de contexte formel R1 avec le graphe biparti corres-
pondant

Ces nouveaux opérateurs nous amènent à considérer une nouvelle connexion qui
correspond aux paires (X, Y ) telles que XΠ = Y et Y −1Π = X ou, de manière
équivalente, telles que X N = Y et Y −1N = X. Notons que (.)∇ et (.)∆ induisent les
mêmes 2 paires remarquables correspondant aux concepts formels.

Les paires (X, Y ) telles que XΠ = Y et Y −1Π = X forment ce que nous pouvons
appeler des sous-contextes indépendants. Ces paires sont diﬀérentes des contextes
formels. En eﬀet, il a été montré [Djouadi et al., 2010b; Dubois et Prade, 2012] que
ces paires sont telles que :

(X × Y ) ∪ (X × Y ) ⊇ R,

(5.14)

X × Y ⊆ R.

de la même manière que les concepts formels sont des paires (X, Y ) maximales telles
que :

(5.15)
Par exemple, les paires ({1, 2, 3, 4},{g, h, i}) et ({5, 6, 7, 8},{a, b, c, d, e, f}) dans
la ﬁgure 5.1 sont deux sous-contextes indépendants, alors que, par exemple, les paires
({1, 2, 3, 4},{g, h}), ({5, 6},{a, b, c, d, f}) ou encore ({5, 6, 7, 8},{a, c, d}) sont des
concepts formels. Notons que, en général, il est possible qu’un sous-contexte indé-
pendant puisse être à nouveau découpé en plusieurs sous-contextes indépendants
plus petits.

5.2.2 Traduction en théorie des graphes

Nous présentons ici une lecture des diﬀérentes notions introduites dans la section
précédente en termes de théorie des graphes. Les notions de la théorie des graphes
que nous utilisons ici sont déﬁnies au premier chapitre, en section 1.2. En plus des

2. Si X∇ = Y et Y −1∇ = X alors (X, Y ) est un concept formel.

148

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

1 2 3 4 5 6 7 8 9 10 11

× ×
× ×
× ×
×
×

×

× × ×
× ×

a1 × × ×
× ×
a2
a3 × ×
a4 × × ×
× × × ×
b1
× ×
×
b2
× × ×
b3
× × × ×
b4
c1
c2
d1
d2

× × ×
×
× × × × ×

Figure 5.2 – Second exemple de contexte formel R2 avec le graphe biparti corres-
pondant

notions introduites dans ce premier chapitre, nous utilisons les notations suivantes :
dans un graphe G = (V, E) (éventuellement biparti), soit un sous-ensemble de som-
mets S ⊆ V , nous notons alors Sk l’ensemble des sommets connectés à au moins
un sommet de S par un chemin de longueur plus petite ou égale à k. Par déﬁnition
S0 = S. On peut alors observer que ∀k, Sk ⊆ Sk+1. On note S∗ l’ensemble des
sommets connectés à au moins un des sommets de S (par un chemin de longueur
k≥0 Sk. Enﬁn, il nous semble utile de rappeler que

quelconque). On a donc S∗ = S

pour un sommet x, Γ(x) représente l’ensemble des voisins de x.

5.2.2.1 Du contexte formel au graphe biparti

Pour chaque contexte formel R = (O, P, R), on peut construire un graphe bi-
parti non-dirigé G = (Vo, Vp, E) tel qu’il y ait une correspondance directe entre :
l’ensemble des objets O et le premier ensemble de sommets Vo, l’ensemble des pro-
priétés P et le second ensemble de sommets Vp, et entre la relation binaire R et
l’ensemble d’arêtes E. On a donc un sommet de Vo pour chaque objet, un sommet
de Vp pour chaque propriété, et une arête entre un sommet-objet et un sommet-
propriété si et seulement si l’objet correspondant possède la propriété correspon-
dante (d’après R).

Les quatre opérateurs (.)Π, (.)N, (.)∆ et (.)∇ peuvent alors être traduits sur des
ensembles de sommets en remplaçant, dans les équations (5.8) à (5.11), O par Vo,
P par Vp et R−1(y) par Γ(y). Les opérateurs (.)Π et (.)∆ peuvent aussi être réécrits
de la manière suivante :

XΠ = ∪x∈XΓ(x)
X∆ = ∩x∈XΓ(x)

(5.16)
(5.17)

a1a2a3a4b1b2b3b4c1c2d1d218327105469115.2. DE L’ANALYSE FORMELLE DE CONCEPTS AU
CLUSTERING DE GRAPHE BIPARTI

149

Ces dernières notations sont intéressantes car seul le voisinage des sommets de X
est utilisé. Cela permet de comprendre plus facilement ces opérateurs en termes de
voisinage dans le graphe biparti : XΠ est l’union des voisins des sommets de X alors
que X∆ en est l’intersection. Notons qu’avec cette écriture (et cette interprétation)
il n’y a pas de diﬀérence entre (.)Π et (.)−1Π ni entre (.)∆ et (.)−1∆.

Interpréter (.)N et (.)∇ en termes de voisinage est moins évident. Néanmoins X N
peut être compris comme l’union des voisins de X qui n’ont pas de voisin en dehors
de X. C’est l’ensemble des sommets connectés exclusivement avec les sommets de
X (mais pas nécessairement avec tous). X∇ est l’ensemble des sommets-propriétés
qui ne sont pas connectés à tous les sommets-objets de X.

5.2.2.2 Deux connections, deux regards sur les « clusters » dans un

graphe

Les connections induites par (.)∆ et (.)Π peuvent aussi se traduire en termes de

graphes. Sur le graphe biparti G = (Vo, Vp, E), avec X ⊆ Vo et Y ⊆ Vp on a :
Proposition 5. X = Y −1∆ et Y = X∆, si et seulement si X ∪ Y est une biclique
maximale.
Démonstration. Soit (X, Y ) une paire telle que X = Y −1∆ et Y = X∆. Pour tous
x ∈ X et y ∈ Y , tels que Y = ∩x∈XΓ(x) on a y ∈ Γ(x) donc {x, y} ∈ E. Cela signiﬁe
que le sous-graphe induit par X ∪ Y est complet. De plus il n’existe pas de sommet
adjacent à tous les sommets de X (resp. Y ) qui ne soit pas dans X∆ (resp. Y −1∆),
donc X ∪ Y est une biclique maximale.
Si X ∪ Y est une biclique maximale, chaque sommet de X (resp. Y ) est adjacent
à chaque sommet de Y (resp. X) et il n’existe pas de sommet adjacent à tous les les
sommets de X (resp. Y ) qui ne soit pas dans Y (resp. X), il est alors évident que
Y = X∆ (resp. X = Y −1∆).
Proposition 6. Pour une paire (X, Y ) les deux propositions suivantes sont équiva-
lentes :

1. X = Y −1Π et Y = XΠ.
2.

(a) (X ∪ Y )∗ = (X ∪ Y ) et
(b) ∀v ∈ (X ∪ Y ), Γ(v) 6= ∅.

Démonstration. 1. ⇒ 2.. Par déﬁnition (X ∪ Y ) ⊆ (X ∪ Y )∗. On montre par ré-
currence que (X ∪ Y )∗ ⊆ (X ∪ Y ). (X ∪ Y )0 ⊆ (X ∪ Y ) est donné par déﬁnition.
On suppose ensuite qu’il existe k tel que (X ∪ Y )k ⊆ (X ∪ Y ). On peut noter que
(X∪Y )k+1 ⊆ ((X∪Y )k)1, en considérant qu’un chemin de longueur k+1 est un che-
min de longueur k suivi par un chemin de longueur 1. Alors (X ∪ Y )k+1 ⊆ (X ∪ Y )1.

150

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

currence que ∀k ≥ 0, (X∪Y )k ⊆ (X∪Y ). Donc (X∪Y )∗ =S

De plus comme X = Y −1Π et Y = XΠ tous les sommets connectés à X∪Y par un che-
min de longueur 1 sont dans X∪Y . Donc (X∪Y )k+1 ⊆ (X∪Y ). Cela implique par ré-
k≥0(X∪Y )k ⊆ (X∪Y ).
Nous avons encore à montrer que chaque sommet v de X ∪ Y a au moins un voisin,
ce qui est immédiat si on considère que soit v ∈ XΠ soit v ∈ Y −1Π.
2. ⇒ 1.. Nous montrons que X = Y −1Π, la preuve est exactement la même
pour Y = XΠ. Y −1Π est l’ensemble des sommets adjacents à (un moins) un sommet
de Y , donc Y −1Π ⊂ Y ∗ et donc Y −1Π ⊂ (X ∪ Y )∗. Cela signiﬁe, d’après 2.a, que
Y −1Π ⊂ (X ∪ Y ), or le graphe est biparti, donc Y −1Π ⊂ X. Soit x un sommet de X,
d’après 2.b 3 x a au moins un voisin v, v est dans X∗ et donc dans (X ∪ Y )∗, donc
d’après 2.a : v ∈ X ∪ Y , mais le graphe est biparti, donc v ∈ Y . Il est alors évident
que X ⊂ Y −1Π et donc X = Y −1Π.

Un ensemble de sommets S tel que S∗ = S n’est pas exactement une compo-
sante connexe maximale mais est un ensemble de sommets déconnectés du reste du
graphe. C’est une union de composantes connexes maximales. S’il n’existe pas de
sous-ensemble strict S0 de S satisfaisant S0∗ = S0 cela signiﬁe qu’il n’y a pas de sous-
ensemble de S déconnecté des autres sommets de S. C’est-à-dire, S est connecté
et donc S est une composante connexe maximale. Alors la propriété suivante est
évidente :
Proposition 7. Pour une paire (X, Y ) les deux propositions suivantes sont équiva-
lentes :

1. X = Y −1Π, Y = XΠ et il n’existe pas de sous-ensembles strict X0 ⊂ X et

Y 0 ⊂ Y tels que X0 = Y 0−1Π, Y 0 = X0Π.

2. X ∪ Y est une composante connexe maximale (qui contient au moins deux

sommets).

D’après les propositions 5 et 7, il est intéressant de noter que les deux connections
correspondent aux deux déﬁnitions extrêmes de ce que peut être un cluster dans un
graphe :

1. un groupe (maximal) de sommets n’ayant aucun lien manquant à l’inté-

rieur,

2. un groupe de sommets n’ayant aucun lien avec l’extérieur.

D’un coté une biclique maximale est un sous-ensemble de sommets de densité
maximale : en un sens, ses sommets ne peuvent être plus proches, et donc on ne
3. Notons que la condition 2.b n’est pas requise si l’on suppose que la relation est bi-normalisée.

5.2. DE L’ANALYSE FORMELLE DE CONCEPTS AU
CLUSTERING DE GRAPHE BIPARTI

151

peut construire un cluster plus « solide ». De l’autre coté, un ensemble de som-
mets déconnectés du reste du graphe ne peut être plus clairement séparé des autres
sommets.

Ces deux déﬁnitions triviales et extrêmes ont déjà été mises en avant par Schaeﬀer

[2007], dans le cadre du clustering de graphe uni-parti.

5.2.2.3 Parallèle avec l’opérateur de diﬀusion des marches aléatoires

Les marches aléatoires sont un mécanisme fréquemment utilisé tant pour déﬁnir
des similarités entre sommets (voir chapitre 2) que pour construire des algorithmes
de clustering de graphe (voir section 5.1.3). Nous proposons ici un parallèle entre
l’opérateur de base utilisé par les marches aléatoires et des généralisations des opéra-
teurs de l’AFC. Ce parallèle n’est malheureusement pas exploité plus en profondeur
dans la suite de ce rapport. Cependant, il constitue le début d’une piste certaine-
ment fertile à explorer, notamment pour déﬁnir un cadre plus général au clustering
de graphe.

Une généralisation de l’opérateur de base de l’AFC pour des contextes formels

ﬂous a été proposée par Belohlavek [2002] :

(cid:16)

X(x) → R(x, y)(cid:17)

X∆(y) = min
x∈O

(5.18)

où R est maintenant une relation ﬂoue, c’est-à-dire qu’à chaque paire est associée
une valeur dans [0, 1], et X et X∆ sont des ensembles ﬂous d’objets et de propriétés.
L’opérateur (.)Π (induisant les sous-contextes indépendants) a été généralisé de la
même manière par Dubois et Prade [2009] :

XΠ(y) = max
x∈O

(5.19)
Un choix usuel pour ∗ est l’opérateur min, et l’implication résiduée de Gödel pour →
(a → b = 1 si a ≤ b et a → b = b sinon). Nous revenons sur ces généralisations en
sous-section 5.4.1.

D’un autre côté, étant donné la position initiale d’un marcheur aléatoire (fonc-

tionnant comme déﬁni en section 2.2.1) sa position après un pas peut s’écrire :

(cid:16)

X(x) ∗ R(x, y)(cid:17)

Y P(x) = X

y∈P

Y (y). [P]y,x

(5.20)

où Y un vecteur de probabilité donnant la position initiale (sur les propriétés) du
marcheur, et Y P sa position après un pas. [P]y,x est le coeﬃcient y, x de la matrice
de transition P associée au graphe, c’est-à-dire la probabilité de passer de y à x.

Dans les deux cas, il est intéressant de remarquer que l’on utilise un opérateur
de projection d’un ensemble ﬂou de propriétés vers un ensemble ﬂou d’objets (ou
d’un vecteur de probabilité sur les propriétés vers un vecteur de probabilité sur les

152

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

objets). Les opérateurs inverses (des objets vers les propriétés) n’ont pas été expo-
sés ici, car ils sont semblables. Aussi les ensembles remarquables (clusters, concepts
formels, sous-contextes indépendants) sont déﬁnis formellement ou approximative-
ment comme des points ﬁxes pour ces opérateurs. En eﬀet les marches aléatoires
sont souvent exploitées avec l’idée que les clusters doivent être tels qu’ils « cap-
turent » les marcheurs aléatoires. Un marcheur à l’intérieur d’un cluster doit avoir
une faible probabilité d’en sortir [Pons, 2007; Rosvall et Bergstrom, 2008; Delvenne
et al., 2010].

5.2.3 Conclusion : vers deux types de clustering ?

Nous avons établi dans cette section une traduction en termes de graphes, des
éléments de base de l’AFC étendue. Ce parallèle n’apporte pas d’application algo-
rithmique directe, en revanche il permet, comme nous venons de le voir, de mieux
comprendre ce qu’est un cluster dans un graphe biparti. En particulier si l’on consi-
dère les deux connections comme des modèles (trop) rigides qui demandent, en
pratique, à être assouplis ; alors deux familles d’approches apparaissent :

1. les méthodes recherchant des sous-ensembles d’objets et de propriétés les plus

larges et cohérents 4 possibles,

2. les méthodes recherchant des sous-ensembles d’objets et de propriétés les plus

« indépendants » possible les uns des autres.

Bien entendu, ces deux familles ne forment pas une dichotomie absolue, beau-
coup de méthodes se positionnant quelque part entre ces deux points de vue, en
évaluant les clusters à la fois sur leur densité et sur leur indépendance. Il est cepen-
dant intéressant de relire l’état de l’art établi en section 5.1 pour confronter à cette
dichotomie les principales approches de la littérature. Assez clairement l’AFC, ses
diﬀérentes variantes et la recherche d’itemset se classent dans la première famille ;
alors que les méthodes de partitionnement de graphe appartiennent à la seconde. Par
contre la classiﬁcation des méthodes regroupées sous les étiquettes « bi-clustering »
ou « détection de communautés » ne peut être automatique. Par exemple les mé-
thodes proposant un partitionnement des sommets en se basant sur la modularité
ou une métrique similaire appartiennent plutôt à la seconde famille alors que celles
proposant de déﬁnir les clusters à partir de noyaux formés de bicliques sont plutôt
de la première famille.

Il faut bien remarquer que ces deux classes d’approches produisent des structures
de clustering diﬀérentes. Tout d’abord concernant le nombre de clusters. La recherche
de groupes indépendants (la seconde famille) amène un nombre de groupes a priori

4. « cohérent » signiﬁant ici « densément connecté ».

5.3. PARTITIONNEMENT DES OBJETS, EN PASSANT PAR LES
CONCEPTS

153

beaucoup plus faible que le nombre de sommets alors que ce n’est pas forcément le
cas lorsqu’on recherche des groupes cohérents (première famille).

Notons ici qu’une « conceptualisation » amenant autant, ou plus, de groupes qu’il
y a de sommets n’est pas forcément inutile ou absurde. Bien sûr une telle méthode
ne simpliﬁe pas la représentation d’un jeu de données, mais le résultat peut tout de
même avoir du sens et se révéler utile. Sur un graphe lexical par exemple, où les
sommets sont des mots/formes, si un cluster (ou, justement, un « concept ») déﬁnit
un sens non ambigu, il est naturel d’avoir un nombre comparable de clusters et
de sommets (c’est le cas dans Wordnet 5 qui comporte par exemple, pour les noms :
117 798 formes et 82 115 synsets, ou pour les verbes : 11 529 formes et 13 767 synsets,
un synset étant une clique de formes synonymes.).

Une seconde diﬀérence concerne le recouvrement entre les clusters. La seconde
famille d’approches ne tolère pas ou peu le recouvrement. En eﬀet comment un
groupe de sommets peut-il paraître indépendant si chacun de ses membres appar-
tient à d’autres groupes ? À l’inverse les approches de la première famille peuvent
engendrer un recouvrement fort, où chaque sommet appartient à plusieurs groupes
(comme avec l’AFC par exemple).

Nous tenons toutes ces discussions pour le clustering de graphe biparti, pourtant
les mêmes commentaires pourraient être faits concernant le clustering de graphe
uni-parti.

On peut alors remarquer que sur les graphes bipartis une approche intermédiaire
est envisageable. En eﬀet la seconde famille de méthodes, cherchant les groupes de
sommets les plus « indépendants » possible, amène à un partitionnement des objets
en correspondance directe avec un partitionnement des propriétés. On cherche des
groupes d’objets et de propriétés liés « le moins possible » aux autres propriétés ou
autres objets. Il est pourtant envisageable de chercher un partitionnement des objets
(resp. des propriétés) sans imposer un partitionnement correspondant des propriétés
(resp. des objets). Il peut exister un découpage sans recouvrement des objets, sans
que celui-ci ne puisse être mis en correspondance directe avec un partitionnement des
propriétés. C’est ce cas intermédiaire que nous explorons dans la section suivante.

5.3 Partitionnement des objets, en passant par les

concepts

Nous nous intéressons dans cette section au cas particulier où l’on souhaite ob-
tenir un clustering sans recouvrement des objets, sans imposer un partitionnement
correspondant sur les propriétés. Nous proposons de passer par une phase d’analyse
formelle de concepts avant d’utiliser une méthode de clustering de graphe (pro-

5. Voir http://wordnet.princeton.edu/wordnet/man/wnstats.7WN.html

154

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

duisant un partitionnement). Nous montrons ainsi qu’un partitionnement pertinent
peut être obtenu sur les objets, alors même qu’il n’existe pas de partitionnement
correspondant sur les propriétés. Ce travail a fait l’objet de la publication [Navarro
et al., 2012b].

Comme nous l’avons vu précédemment, beaucoup de méthodes de clustering de
graphe biparti recherchent un partitionnement des objets qui soit en correspondance
avec un partitionnement des propriétés. Or ces méthodes échouent généralement
lorsque les propriétés sont fortement partagées entre plusieurs groupes d’objets, car
elles regroupent tous les objets dans un seul cluster.

Cela est illustré par le graphe de la ﬁgure 5.3a qui correspond au contexte formel
du tableau 5.1a. Sur cet exemple la méthode de clustering regroupe tous les sommets
dans un seul cluster. Les trois groupes d’animaux (mammifères, oiseaux et poissons)
ne sont pas retrouvés. Cet exemple est un petit extrait du jeu de données Zoo que
nous utiliserons en sous-section 5.3.3. On observe, sur cet exemple, que le passage au
graphe objets-concepts (ﬁgure 5.3b et tableau 5.1b) permet de résoudre le problème
convenablement.

5.3.1 Clustering du graphe objets-concepts

Nous décrivons ici en détail la méthode proposée. Tout d’abord, une étape pré-
liminaire consiste à construire le contexte formel associé au graphe biparti entre
objets et propriétés, et à utiliser une méthode d’extraction des concepts formels, par
exemple [Fu et Nguifo, 2004] ou [Krajca et al., 2008] 6.
Ensuite, un graphe biparti reliant objets et concepts est construit. Chaque objet
o ∈ O est connecté à un concept (X, Y ) si et seulement si o ∈ X. Chacune des
arêtes de ce graphe est pondérée par W(o, (X, Y )) = |Y |, c’est-à-dire le nombre
de propriétés du concept (X, Y ) extrémité de l’arête. Ce poids est introduit pour
favoriser les concepts les plus grands, que nous espérons plus pertinents. En eﬀet un
concept comportant peu de propriétés a plus facilement un nombre (trop) important
d’objets. Les concepts extrémités du treillis (c’est-à-dire le concept contenant tous
les objets et celui contenant toutes les propriétés) sont ignorés. Cette transformation
est illustrée par la table 5.1 et la ﬁgure 5.3.
Si le graphe de départ est pondéré par une fonction w : E → R+, alors le graphe
objets-concepts peut être pondéré de diﬀérentes manières en agrégeant les poids du
6. Nous avons utilisé la méthode présenté dans [Krajca et al., 2008] et dont une implémentation

est disponible à l’adresse suivante : http://fcalgs.sourceforge.net/

5.3. PARTITIONNEMENT DES OBJETS, EN PASSANT PAR LES
CONCEPTS

155

Table 5.1 – Contextes formels entre 9 animaux et 3 propriétés, et entre les 9
animaux et les 6 concepts formels issus du premier contexte. Ces contextes cor-
respondent aux graphes de la ﬁgure 5.3. Les couleurs indiquent le regroupement
attendu.

(a) Contexte formel objets-propriétés.

(b) Contexte formel objets-concepts.

s
e
h
t
a
e
r
b

s
g
g
e

d
e
h
t
o
o
t

}
s
e
h
t
a
e
r
b
{

}
s
g
g
e
{

}
d
e
h
t
o
o
t
{

×
×
×

elephant ×
×
×

goat
lion
sparrow × ×
vulture × ×
carp

× ×
× ×
× ×
× ×

piranha

sole
tuna

×
×
×

elephant ×
×
×

goat
lion
sparrow × ×
vulture × ×
carp

× ×
× ×
× ×
× ×

piranha

sole
tuna

}
s
g
g
e
&
s
e
h
t
a
e
r
b
{

×
×

}
d
e
h
t
o
o
t

&
s
g
g
e
{

}
d
e
h
t
o
o
t

&
s
e
h
t
a
e
r
b
{

×
×
×

×
×
×
×

(a) graphe objets-propriétés, tous les
sommets sont regroupés dans le même
cluster.

graphe objets-concepts,

(b)
les 3
groupes d’animaux sont bien retrouvés.

Figure 5.3 – Résultat d’Infomap [Rosvall et Bergstrom, 2008] sur un petit jeu
de données représentant 9 animaux avec 3 propriétés. Ces graphes correspondent
aux contextes formels de la table 5.1. La méthode de clustering est appliquée soit
directement sur le graphe objets-propriétés (a) soit sur le graphe objets-concepts
(b).

{},{}{}{}{},{},156

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

graphe de départ. Par exemple les pondérations suivantes sont imaginables :

Wsum(o, (X, Y )) = X

y∈Y
Wmin(o, (X, Y )) = min
y∈Y
Wmax(o, (X, Y )) = max
y∈Y

Wprod(o, (X, Y )) = Y
y∈Y
Wavg(o, (X, Y )) = 1
|Y |

w(o, y)

w(o, y)
w(o, y)
w(o, y)

X

y∈Y

w(o, y)

(5.21)

(5.22)
(5.23)
(5.24)

(5.25)

Notons que Wsum(o, (X, Y )) peut être compris comme une généralisation du poids
que nous proposons dans le cas où le graphe de départ n’est pas pondéré. Dans le
chapitre suivant (section 6.4.3) nous évaluons ces diﬀérentes pondérations.

Finalement, l’ensemble des sommets du graphe objets-concepts est partitionné
avec la méthode de clustering de graphe Infomap [Rosvall et Bergstrom, 2008]. Nous
décrivons cette méthode en section 5.1.3. Notons, à nouveau, que Infomap n’a pas
spécialement été conçu pour les graphes bipartis. La méthode ne prend simplement
pas en compte le fait que le graphe soit biparti.

Comme on peut le voir sur la ﬁgure 5.3b, Infomap appliqué sur le graphe objets-
concepts donne maintenant, si on considère les objets (les animaux), les trois clusters
attendus.

5.3.2 Étiquetage des clusters

Pour donner à chaque cluster d’objets un label fait d’un sous-ensemble de pro-

priétés, nous proposons la méthode suivante.

Soit X un cluster d’objets et l’on note T l’ensemble des concepts formels. Pour
chaque cluster d’objets on cherche deux ensembles de concepts particuliers : l’en-
semble T ∗, formé des concepts (X∗, Y ∗) composés des plus petits sur-ensembles
d’objets contenant le cluster, c’est-à-dire :

et l’ensemble T∗, formé des concepts (X∗, Y∗) composés des plus grands des sous-
ensembles d’objets contenus dans le cluster, c’est-à-dire :

X ⊆ X∗
(cid:64)(X0, Y 0) ∈ T tel que X ⊂ X0 ⊆ X∗
(

X∗ ⊆ X
(cid:64)(X0, Y 0) ∈ T t.q. X∗ ⊂ X0 ⊆ X

(

(X∗, Y ∗) ∈ T t.q.

(X∗, Y∗) ∈ T tel que

On peut vériﬁer que ∀(X∗, Y ∗) ∈ T ∗ et ∀(X∗, Y∗) ∈ T∗, on a :

X∗ ⊆ X ⊆ X∗

et Y ∗ ⊆ Y∗

(5.26)

(5.27)

(5.28)

5.3. PARTITIONNEMENT DES OBJETS, EN PASSANT PAR LES
CONCEPTS

157
Il est alors possible d’utiliser les propriétés des concepts de T ∗ et de T∗ pour
décrire le cluster. Notons que toutes les propriétés des ensembles Y ∗ des concepts
de T ∗ sont vériﬁées par tous les objets du cluster.

5.3.3 Évaluation

Nous proposons d’évaluer cette procédure de clustering sur deux jeux de tests :

sur des graphes artiﬁciels, puis sur un graphe « réel » issu de la littérature.

5.3.3.1 Évaluation sur des graphes artiﬁciels

Pour évaluer notre procédure nous construisons des contextes formels artiﬁciels

de la manière suivante :

• On pose n groupes de k objets,
• chaque groupe est associé à :

– mown propriétés spéciﬁques (c’est-à-dire seuls les objets de ce groupe

peuvent les vériﬁer),

– mshared propriétés partagées par s autres groupes (c’est-à-dire que les

objets de s + 1 groupes peuvent les vériﬁer) ;

• chaque objet du groupe satisfait chaque propriété du groupe (spéciﬁque ou

partagée) avec une probabilité µ.

Un exemple d’un tel contexte formel est donné par le tableau 5.2.

Table 5.2 – Exemple de contexte formel artiﬁciel construit pour l’évaluation par la
procédure décrite en sous-section 5.3.3, avec n = 3, k = 3, mown = 2, mshared = 4,
s = 1, µ = 0.8. Chaque ligne correspond à un objet (a*, . . ., c*) et chaque colonne
correspond à une propriété (A*, . . ., C* pour celles spéciﬁques et AB*, . . ., CA*
pour celles partagées).

A0 A1 B0 B1 C0 C1 AB0 AB1 BC0 BC1 CA0 CA1
×
×

×

×
×

×
× ×
×

a0 ×
a1
a2 ×
b0
b1
b2
c0
c1
c2

×
×
×

×
×
×
×
×

×
×

×
× ×
× ×

×

×
×

×
×

×
×
×
×
×
×

×
×
×
×
×
×

158

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

(b) Résultats en fonction de µ, avec mown = 5

(a) Résultats en fonction de nown, avec µ = 0.8
Figure 5.4 – Valeur de l’information mutuelle normalisée (NMI) [Danon et al.,
2005] de la méthode de clustering Infomap appliquée directement sur les graphes
objets-propriétés (courbe O ↔ P, triangles vers le haut en bleu) et sur les graphes
objets-concepts (courbe O ↔ C, triangle vers la gauche en rouge). Les contextes
(i.e. graphes objets-propriétés) sont tous construits avec les paramètres suivants :
n = 5, k = 10, mshared = 6, s = 2, les autres paramètres sont indiqués sur les
courbes. Chaque point indique la valeur moyenne obtenue sur 50 réalisations, l’écart
type est indiqué par les barres verticales sur chaque point.

Pour évaluer la qualité du résultat d’une méthode par rapport au partitionnement
correct (ici donné par construction) nous utilisons l’information mutuelle normalisée
(NMI, pour normalised mutual information en anglais). Une valeur de 0 indique que
les deux partitions sont complètement dissimilaires, alors qu’une valeur de 1 indique
que les deux partitions sont identiques. Cette mesure est très populaire dans la
littérature du clustering de graphe [Danon et al., 2005].
Les ﬁgures 5.4a et 5.4b présentent les résultats du clustering sur le graphe objets-
propriétés (la courbe O ↔ P) et sur le graphe objets-concepts (la courbe O ↔ C).
Comme le montre la ﬁgure 5.4a, les résultats restent stables avec notre approche
quand le nombre de propriétés partagées entre les diﬀérents groupes d’objets aug-
mente. On voit à l’inverse que ce n’est pas le cas quand on travaille directement sur
le graphe objets-propriétés.

5.3.3.2 Jeu de données UCI Zoo

Le jeu de données Zoo décrit 101 animaux sur 16 propriétés booléennes et un
attribut numérique (le nombre de pattes). Nous avons transformé cet attribut nu-
mérique en 7 propriétés booléennes (pas de patte, une patte, deux pattes, etc.). Pour
chaque animal une classe est indiquée, il y a 7 classes d’animaux : mammifères (mam-
mal), oiseaux (bird), reptiles (reptile), poissons (ﬁshes), amphibiens (amphibians),

0246810number of non overlapping prop.0.00.20.40.60.81.0NMIO↔PO↔C0.00.20.40.60.81.0µ0.00.20.40.60.81.0NMIO↔PO↔C5.3. PARTITIONNEMENT DES OBJETS, EN PASSANT PAR LES
CONCEPTS

159

insectes (insects), invertébrés (invertebrates). Ce jeu de données est disponible sur
le dépôt « UCI Machine Learning Repository 7 ».

La Table 5.4 montre les résultats du clustering sur le graphe objets-propriétés,
alors que la Table 5.3 montre les résultats sur le graphe objets-concepts. On peut
voir que le clustering échoue sur le graphe objets-propriétés (NMI proche de zero),
alors que la partition calculée sur le graphe objets-concepts correspond presque
exactement aux classes des animaux (N M I = 0.81).

5.3.4 Conclusions et perspectives

Nous avons dans cette section présenté une méthode permettant de partitionner
un ensemble d’objets dans un graphe biparti sans imposer de partitionnement des
propriétés. Cette méthode repose sur l’utilisation de l’AFC pour remplacer dans le
graphe l’ensemble des propriétés par l’ensemble des concepts. Ce nouveau graphe
objets-concepts est ensuite partitionné avec une méthode de clustering de graphe
récente. Nous avons montré sur deux évaluations l’intérêt de la méthode, en parti-
culier quand il existe un fort recouvrement des propriétés entre diﬀérents groupes
d’objets.

Il serait intéressant de poursuivre cette évaluation en particulier sur des graphes
réels plus grands que celui utilisé ici. Notons que nous présentons au chapitre suivant
une utilisation de cette méthode sur des graphes bipartis documents/termes.

Aussi, il serait souhaitable de comparer expérimentalement notre approche avec
les quelques méthodes de l’état de l’art proposant de partitionner un groupe de
sommets sans imposer de partitionnement correspondant sur l’autre groupe. En
particulier les méthodes basées sur l’adaptation de la modularité de Liu et Murata
[2010], ou la méthode utilisant les hypercliques de Hu et al. [2006]. Malheureusement,
aujourd’hui, aucune de ces méthodes ne semble avoir d’implémentation disponible.
Nous présentons dans la section suivante une méthode de prétraitement d’un
graphe biparti (ou d’un contexte formel) qui peut être utilisée en amont de la mé-
thode que nous venons de présenter. C’est ce que nous ferons sur une application en
recherche d’information au chapitre suivant (section 6.4).

7. http://archive.ics.uci.edu/ml/datasets/Zoo

160

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

Table 5.3 – Résultats du clustering avec Infomap du graphe objets-concepts pour
le jeu de données Zoo (on a 10 clusters et un score N M I = 0.81). Les animaux sont
regroupés, dans chaque cluster, par leur catégorie dans le jeu de données.

Y ∗ = {backbone, breathes, hair, milk, toothed}
Y∗ = {backbone, breathes, hair, milk, tail, toothed}

Mammals :

aardvark, lynx, leopard, bear, boar, puma, lion, cheetah, raccoon, mink, pussy-
cat, mongoose, wolf, polecat, antelope, calf, elephant, oryx, goat, deer, reindeer,
buﬀalo, pony, giraﬀe, vole, mole, hare, cavy, hamster, opossum, sealion, girl,
wallaby, gorilla, fruitbat, squirrel, vampire

Y ∗ = {0legs}
Y∗ = {0legs, aquatic, eggs}

Fishes :

Invertebrates :
Reptiles :

stingray, pike, piranha, catﬁsh, herring, dogﬁsh, tuna, chub, bass, sole, seahorse,
carp, haddock
clam, seawasp
seasnake

Y ∗ = {2legs, backbone, breathes, eggs, f eathers, tail}
Y∗ = {2legs, backbone, breathes, eggs, f eathers, predator, tail}
Birds :

ﬂamingo, gull, skimmer, sparrow, wren, skua, hawk, crow, duck, vulture, lark,
swan, pheasant, kiwi, rhea, ostrich, penguin

Y ∗ = Y∗ = {4legs, eggs}

Amphibians :
Reptiles :
Mammals :
Invertebrates :

newt, frog2, frog1, toad
tortoise, tuatara
platypus
crab

Y ∗ = Y∗ = {0legs, aquatic, backbone, breathes, catsize, f ins, milk, predator, toothed}
porpoise, dolphin, seal
Mammals :
Y ∗ = Y∗ = {6legs, breathes, eggs}

Insects :

ﬂea, ladybird, moth, gnat, wasp, honeybee, houseﬂy, termite

Y ∗ = Y∗ = {2legs, airborne, backbone, breathes, domestic, eggs, f eathers, tail}

Y ∗ = Y∗ = {0legs, breathes, eggs}
Reptiles :

slowworm, pitviper

Invertebrates : worm, slug

chicken, parakeet, dove
Birds :
Y ∗ = {aquatic, eggs, predator}
Y∗ = {6legs, aquatic, eggs, predator}

Invertebrates :

crayﬁsh, starﬁsh, lobster

Y ∗ = Y∗ = {8legs, breathes, predator, tail, venomous}

Invertebrates :

scorpion

Y ∗ = Y∗ = {8legs, aquatic, catsize, eggs, predator}

Invertebrates :

octopus

5.3. PARTITIONNEMENT DES OBJETS, EN PASSANT PAR LES
CONCEPTS

161

Table 5.4 – Résultats du clustering du graphe objets-propriétés pour le jeu de
données Zoo (seulement 2 clusters et N M I = 0.02). Seuls les objets des clusters (les
animaux) sont présentés. Les animaux sont regroupés, dans chaque cluster, par leur
catégorie dans le jeu de données.

Mammals :

Birds :

Fishes :

Invertebrates :
Insects :
Reptiles :
Amphibians :
Invertebrates :

aardvark, antelope, bear, boar, buﬀalo, calf, cavy, cheetah, deer, dolphin, ele-
phant, fruitbat, giraﬀe, girl, goat, gorilla, hamster, hare, leopard, lion, lynx,
mink, mole, mongoose, opossum, oryx, platypus, polecat, pony, porpoise, puma,
pussycat, raccoon, reindeer, seal, sealion, squirrel, vampire, vole, wallaby, wolf
chicken, crow, dove, duck, ﬂamingo, gull, hawk, kiwi, lark, ostrich, parakeet,
penguin, pheasant, rhea, skimmer, skua, sparrow, swan, vulture, wren
bass, carp, catﬁsh, chub, dogﬁsh, haddock, herring, pike, piranha, seahorse,
sole, stingray, tuna
clam, crab, crayﬁsh, lobster, octopus, scorpion, seawasp, slug, worm
ﬂea, gnat, honeybee, houseﬂy, ladybird, moth, termite, wasp
pitviper, seasnake, slowworm, tortoise, tuatara
frog1, frog2, newt, toad
starﬁsh

162

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

5.4 Pré-traitement par marches aléatoires

Nous présentons dans cette section une méthode basée sur une mesure de si-
milarité pour simpliﬁer un graphe biparti en vue d’une AFC. Avant de présenter
cette méthode en sous-section 5.4.2, nous proposons dans la sous-section 5.4.1 un
rapide état de l’art des extensions de l’AFC aux données pondérées. Enﬁn la sous-
section 5.4.3 montre l’intérêt de la méthode avec une évaluation sur des graphes
artiﬁciels et sur des graphes réels.

5.4.1 Généralisations de l’AFC sur données pondérées ou

incomplètes

Il existe diﬀérentes motivations pour gérer des contextes formels pondérés, c’est-
à-dire un contexte formel où les liens entre objets et propriétés sont pondérés. En
eﬀet ces poids peuvent être compris de diﬀérentes manières. Tout d’abord ils peuvent
représenter l’idée que les propriétés sont graduelles. Un autre type de pondération
peut correspondre à la situation où les propriétés sont toujours binaires mais connues
avec incertitude. Dans les deux cas, on suppose que cette information graduelle ou
incertaine est disponible.

La première approche, qui a été la plus explorée jusqu’à présent, consiste à
considérer que les objets vériﬁent les propriétés jusqu’à un certain degré. Dans cette
optique, des généralisations de l’AFC ont été proposées par Belohlavek [2002], elles
sont basées sur l’opérateur :

(5.29)

X∆(y) = ^

x∈O

(cid:16)

X(x) → R(x, y)(cid:17)

et de propriétés.V est l’opérateur de conjonction min et → un opérateur d’implica-

où R est maintenant une relation ﬂoue, et X et X∆ sont des ensembles ﬂous d’objets
tion. Un choix possible de connecteur (l’implication résiduée de Gödel : a → b = 1 si
a ≤ b, et a → b = b si a > b) permet toujours de voir un concept formel ﬂou à travers
ses diﬀérents niveaux de coupe Xα, Yα tels que (Xα×Yα) ⊆ Rα avec Xα×Yα maximal
et Rα = {(x, y)|R(x, y) ≥ α}, Xα = {x ∈ O|X(x) ≥ α}, Yα = {y ∈ P|Y (y) ≥ α}.

La seconde approche correspond à l’idée d’incertitude [Djouadi et al., 2010a],
[Djouadi et al., 2010b]. La représentation possibiliste de l’incertitude consiste à as-
socier à chaque lien (x, y) une paire de nombres (α, β) tels que α, β ∈ [0, 1] et tels
que min(α, β) = 0. Ces nombres indiquent respectivement à quel point il est certain
que ce lien existe (α) ou n’existe pas (β). Un lien dans l’analyse formelle de concepts
classique correspond à une paire (1, 0), l’absence de lien à une paire (0, 1), et une
paire (0, 0) modélise l’ignorance complète à propos de l’existence ou non d’un lien.
Sur cette base, plus α (resp. β) est grand, plus un lien peut être facilement ajouté
(resp. supprimé).

5.4. PRÉ-TRAITEMENT PAR MARCHES ALÉATOIRES

163

Table 5.5 – Exemple de pré-traitement en utilisant la conﬂuence. Le contexte (b)
est le résultat du ﬁltrage du contexte (a) par la méthode décrite par l’eq. (5.32) avec
t = 3, τ + = 0.5 et τ− = 0.4.

(a) Contexte formel original
1 2 3 4 5 6 7 8 9 10 11

×

a1 × × × × ×
× ×
a2 × ×
a3 ×
× × ×
× × × ×
a4
b1
b2
b3
c1
c2
c3

×

× × ×
×
× × × × ×
× × × ×
×

×

× × ×
× ×
× × ×

(b) Après le pré-traitement
1 2 3 4 5 6 7 8 9 10 11

a1 × × × × ×
a2 × × × × ×
a3 × × × × ×
a4 × × × × ×
b1
b2
b3
c1
c2
c3

× × × × ×
× × × × ×
× × × × ×

× × ×
× × ×
× × ×

On peut aussi considérer que certaines propriétés sont moins importantes, ou que
certains objets sont plus typiques [Djouadi et al., 2010a]. Les poids ne sont plus sur
les liens mais sur les sommets. Oublier une propriété non primordiale (par exemple
la capacité de voler pour un oiseau) peut alors aider à construire un concept plus
large (par exemple la classe des oiseaux, bien que les oiseaux typiques volent).

Ces trois vues nécessitent diﬀérents types de données additionnelles qui ne sont
pas toujours disponibles. De plus les deux dernières vues ont été introduites récem-
ment et n’ont pas été explorées en détail.

5.4.2 Filtrage et binarisation d’un graphe biparti éventuel-

lement pondéré

Nous proposons dans cette section une méthode qui consiste à calculer une si-
milarité pour chaque paire (objet, propriété) dans le graphe biparti (i.e. contexte
formel) éventuellement pondéré. Nous utilisons pour cela une mesure de similarité
entre sommets comme celles déﬁnies au chapitre 2, mais adaptée aux graphes bi-
partis. Nous proposons de supprimer les arêtes dont la similarité (entre les sommets
extrémités) est inférieure à un certain seuil et d’ajouter les paires non-adjacentes
ayant une similarité supérieure à un second seuil. Un exemple est donné par la
table 5.5 : la méthode transforme le contexte formel 5.5a en un contexte formel plus
simple présenté dans le tableau 5.5b.

L’objectif est donc de transformer le graphe (ou contexte formel) initial pour
réduire le nombre de concepts formels associé. En eﬀet, un lien manquant dans
une zone dense (a priori de forte similarité) amène à diviser ce qui pourrait être un
concept clair en plusieurs petits concepts se recouvrant fortement. À l’inverse un lien
dans une zone peu dense (faible similarité) crée plusieurs concepts non pertinents.

164

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

Notons que ce pré-traitement peut être vu de deux manières : soit comme une
tentative de correction d’une donnée bruitée, soit comme une modiﬁcation de don-
nées pourtant exactes en vue d’en simpliﬁer la représentation. Lors d’une première
exploration, il peut par exemple être pertinent de considérer que le kiwi vole pour
simpliﬁer la représentation de la classe des oiseaux, même si cela est faux.

La méthode permet aussi de prendre en considération une pondération disponible
sur les liens (pondération indiquant à quel degré un objet vériﬁe une propriété). Il est
possible d’appliquer un seuil directement sur le graphe pondéré, mais notre approche
permet de normaliser et de lisser ces poids et de rendre ainsi la coupe cohérente 8.

5.4.2.1 Calcul des conﬂuences

Soit un graphe biparti G = (Vo ∪ Vp, E), avec Vo un ensemble d’objets et Vp un
ensemble de propriétés. Ce graphe peut être pondéré par une fonction w : E → R+.
Pour calculer une similarité entre les sommets de Vo et de Vp nous utilisons
une adaptation aux graphes bipartis de la conﬂuence présentée au chapitre 2 (sous-
section 2.2.2). Comme nous souhaitons calculer une similarité entre objets et pro-
priétés, nous utilisons des marches en temps impairs t = 2l + 1 (nous renvoyons à
la sous-section 2.2.1.3 pour plus de détails concernant les particularités des marches
aléatoires sur des graphes bipartis.). Aussi, pour éviter que les valeurs des paires
adjacentes soient biaisées, nous utilisons l’astuce consistant à supprimer du graphe
l’arête reliant deux sommets adjacents avant de calculer leur similarité. La conﬂuence
entre un objet o et une propriété p non adjacents vaut donc :

{o, p} 6∈ E :

conﬂ2l+1(o, p) =

P 2l+1i

h
[P 2l+1]o,p + π(p)

o,p

P étant la matrice d’adjacence (éventuellement pondérée) du graphe, et π(p) la
probabilité limite d’atteindre le sommet p en un nombre impair de pas avec une
probabilité de départ quelconque mais limitée à l’ensemble des objets. Cette limite
vaut, si le graphe est pondéré :

(5.30)

(5.31)

(5.32)

P

P

P

i∈Γ(p) w(p, i)

q∈Vp

a∈Γ(q) w(q, a)

π(p) =

Pour un objet o et une propriété p adjacents, la conﬂuence vaut :

{o, p} ∈ E :

conﬂ2l+1(o, p) =

i

P 2l+1
{o,p}

o,p

h
i

h

P 2l+1
{o,p}

+ π{o,p}(p)

o,p

8. La normalisation que nous proposons n’est cependant pas la seule imaginable et n’est certai-

nement pas celle à adopter dans tous les cas.

5.4. PRÉ-TRAITEMENT PAR MARCHES ALÉATOIRES

165

avec P{o,p} la matrice d’adjacence de G0 = (Vo ∪ Vp, E \ {o, p}). La limite π{o,p}(p)
correspond à π(p) mais calculée sur le graphe G0. Pour le calcul des limites, lorsque
le graphe n’est pas pondéré, il suﬃt de considérer que w est une fonction constante
égale à 1. Remarquons que les équations de cette section utilisent « 2l +1 » ; or pour
présenter les résultats nous indiquons t plutôt que l. On a toujours t = 2l + 1.

D’autres similarités peuvent être utilisées (voir chapitre 2) mais cette mesure a
l’avantage d’être symétrique, comprise entre 0 et 1 et d’avoir un seuil naturel à 0.5.
En eﬀet une valeur au-dessus de 0.5 indique que la probabilité qu’un marcheur passe
de l’un des sommets à l’autre en t = 2l + 1 pas est plus forte que la probabilité qu’a
le marcheur d’atteindre l’un de ces deux sommets après une marche de temps inﬁni.
Une valeur de conﬂuence supérieure à 0.5 indique donc que les deux sommets se
trouvent dans une même zone dense en arêtes. À l’inverse une valeur inférieure à 0.5
indique qu’il est moins probable de passer de l’un des sommets à l’autre en temps
court qu’en temps long, et donc que ces deux sommets ne se trouvent pas dans
une même zone de sur-densité. Cela indépendamment du fait que les deux sommets
soient reliés ou non par une arête.

5.4.2.2 Ajout et suppression d’arêtes

Plusieurs options sont possibles pour obtenir un nouveau graphe biparti à partir
de ces valeurs de conﬂuence. Une première solution consiste à ne garder comme arête
que les paires ayant une conﬂuence supérieure à un certain seuil :

Ecut− = {(o, p) ∈ E | conﬂ2l+1(o, p) > τ−}

(5.33)

Cette méthode ne fait que supprimer des arêtes.

Il est aussi possible d’ajouter toutes les paires ayant une conﬂuence au-dessus

d’un certain seuil :

Ecut+ = E ∪ { (o, p) ∈ Vo × Vp | conﬂ2l+1(o, p) > τ +}

(5.34)

On ne fait alors qu’ajouter des arêtes. Enﬁn, il est aussi possible d’ajouter et de
supprimer des arêtes :

Ecut+− = {(o, p) ∈ E | conﬂ2l+1(o, p) > τ−}

∪ { (o, p) ∈ Vo × Vp \ E | conﬂ2l+1(o, p) > τ +}

(5.35)

Et ce éventuellement avec un seuil diﬀérent pour l’ajout et la suppression.

5.4.3 Évaluation

Nous présentons ici une première évaluation de la méthode, sur des graphes ar-
tiﬁciels et sur des graphes réels. Nous étudions d’abord, sur deux séries de graphes

166

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

artiﬁciels, l’eﬀet d’un ajout d’arêtes aléatoires (ﬁgure 5.5 et 5.6) puis d’une suppres-
sion aléatoire des arêtes (ﬁgure 5.7 et 5.8). Avant d’être ainsi bruités, les graphes
sont construits par la méthode présentée en sous-section 5.3.3.1 : une série « a »
de graphes présentent 5 groupes de 10 sommets (ﬁgures « a ») et une série « b »
présentent 15 groupes de 10 sommets (ﬁgures « b »). Les autres paramètres sont les
mêmes dans les deux cas et sont donnés en légende de la ﬁgure 5.5. Notons que dans
les deux séries de graphes les groupes de sommets ont la même taille et sont reliés
au même nombre de propriétés. Aussi le degré moyen des sommets est le même. La
seule diﬀérence est que les graphes « b » sont plus grands (et donc moins denses)
que les graphes « a ».

Sur chacune des ﬁgures présentées (ﬁgure 5.5 à 5.8), le premier tracé représente
le score de Jaccard de l’ensemble des arêtes du graphe de départ calculé par rapport
aux arêtes du graphe bruité (courbe en triangles rouges) et calculé par rapport à
l’ensemble d’arêtes du graphe ﬁltré (courbe en ronds verts). Le second tracé repré-
sente le nombre d’arêtes du graphe de départ (courbe de carrés bleus), du graphe
bruité (courbe en triangles rouges) et du graphe ﬁltré (courbe en ronds verts). Enﬁn
le dernier tracé indique le nombre de concepts formels (|T|) issus du graphe original,
du graphe bruité et du graphe ﬁltré. Chacun des points indique la moyenne pour 20
réalisations, les barres verticales autour des points indiquant l’écart type.

Dans un second temps, nous utilisons des graphes mots-documents issus de l’ap-
plication présentée au chapitre suivant. Sur 25 graphes construits à partir de diﬀé-
rentes requêtes, nous comparons le nombre de concepts obtenus sans pré-traitement,
ou avec un pré-traitement consistant uniquement en une suppression des arêtes, uni-
quement en un ajout, ou en un ajout et une suppression.

5.4.3.1 Supprimer des arêtes bruits

Nous évaluons dans un premier temps la méthode de ﬁltrage face à des graphes
bruités par un ajout d’arêtes aléatoires. Les graphes sont bruités par un ajout
d’arêtes aléatoires, avant d’être ﬁltrés par notre méthode. Le ﬁltrage est conﬁguré
pour supprimer les arêtes ayant un score de conﬂuence (calculé avec t = 3) inférieur
à un certain seuil. Ici, aucune arête ne peut être ajoutée par la procédure de ﬁltrage.
Dans une première expérience le seuil est ﬁxe (τ− = 0.5) et le bruit varie (ﬁgure 5.5),
dans une seconde expérience le bruit est ﬁxe et c’est le seuil qui varie (ﬁgure 5.6).
Sur la ﬁgure 5.5, on observe tout d’abord (sur le dernier tracé) que le nombre de

concepts explose 9 dès lors qu’un peu de bruit aléatoire est présent.

On remarque ensuite que le Jaccard entre l’ensemble des arêtes au départ et
après ﬁltrage reste assez haut et cela même quand ont été ajoutées autant d’arêtes
« bruit » qu’il y a d’arêtes au départ. La méthode de ﬁltrage fonctionne donc bien
sur ces exemples. On note toutefois que le ﬁltrage ne supprime pas toutes les arêtes

9. Notons que les échelles des ordonnées de ces derniers tracés sont logarithmiques.

5.4. PRÉ-TRAITEMENT PAR MARCHES ALÉATOIRES

167

(a) 5 groupes de 10 sommets (n = 5)

(b) 15 groupes de 10 sommets (n = 15)

Figure 5.5 – Résultats de la méthode de ﬁltrage sur des graphes artiﬁciels bruités
par un ajout d’arêtes aléatoires. Le ﬁltrage consiste à supprimer les arêtes ayant une
conﬂuence (calculée pour t = 3) strictement inférieure à τ− = 0.5. Le premier tracé
indique le score de Jaccard entre les arêtes originales et les arêtes bruitées (triangles rouges),
et les arêtes ﬁltrées (ronds verts). Les tracés suivants indiquent respectivement le nombre
d’arêtes et le nombre de concepts, sur le graphe original (carrés bleus), le graphe bruité
(triangles rouges), le graphe ﬁltré (ronds verts). L’axe des abscisses indique la proportion
d’arêtes aléatoirement ajoutées au graphe de départ. Chaque point indique la moyenne pour
20 réalisations, l’écart type étant indiqué par les barres verticales. Les graphes sont construits
avec la méthode présentée en section 5.3.3.1 avec les paramètres suivants : k = 10, mown = 6,
mshared = 6, s = 2, µ = 1.0, avant un ajout d’arêtes aléatoires.

« bruit » : il y a presque toujours plus d’arêtes sur le graphe après ﬁltrage que sur le
graphe de départ. Cela fait que le nombre de concepts reste plus important même
après le ﬁltrage, en étant néanmoins beaucoup plus faible que sur le graphe bruité.
Sur les petits graphes (ﬁgure 5.5a), lorsque le nombre d’arêtes ajoutées est im-
portant, on observe que les graphes ﬁltrés comportent moins d’arêtes que les graphes
de départ. Le phénomène se produit lorsque le nombre d’arêtes a doublé (bruit= 1),
la densité dépasse alors 0.6. Le même phénomène se produit quand on ajoute suf-
ﬁsamment d’arêtes aux graphes de la ﬁgure 5.5b pour que leurs densités dépassent
0.5 (cela n’est pas visible sur la courbe car il faut ajouter 5 fois le nombre d’arêtes
présentes au départ). L’ajout d’arêtes fait que le score de conﬂuence de certaines
arêtes du graphe original se retrouve inférieur à 0.5.

Choisir un seuil plus élevé pourrait permettre de mieux nettoyer le graphe. La
ﬁgure 5.6 présente les mêmes mesures que celles de la ﬁgure 5.5 mais cette fois avec

168

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

(a) 5 groupes de 10 sommets (n = 5)

(b) 15 groupes de 10 sommets (n = 15)

Figure 5.6 – Résultats de la méthode de ﬁltrage en fonction du seuil sur des
graphes artiﬁciels bruités par un ajout d’arêtes aléatoires. La conﬂuence est calculée
avec t = 3. Le seuil sur la conﬂuence est indiqué sur l’axe des abscisses, les arêtes ayant un
score inférieur sont ﬁltrées. Le bruit ajouté sur les graphes est ici constant : 40% du nombre
d’arêtes de départ est ajouté aléatoirement. Pour une description en détail de la signiﬁcation
de ces courbes voir la légende de la ﬁgure 5.5.

un seuil τ− variant et un bruit ﬁxe de 0.4. Là encore, aucune arête n’est ajoutée par
la méthode de ﬁltrage : seules les arêtes ayant un score strictement inférieur au seuil
sont supprimées.

On observe en ﬁgure 5.6a qu’un seuil supérieur à 0.5 amène à retirer trop d’arêtes.
En eﬀet, on remarque que les scores de conﬂuence des arêtes (non issues du bruit)
sont proches de 0.5 sur les graphes « a ». À l’inverse en ﬁgure 5.6b, il apparaît
qu’un seuil de 0.6 permet de revenir presque exactement au graphe de départ
(Jaccard ≈ 1). La conﬂuence des arêtes des graphes de départ étant supérieure
à 0.5, on peut supprimer les arêtes ayant un score supérieur à 0.5 sans supprimer
les arêtes originales.

5.4.3.2 Ajouter des arêtes manquantes

Voici une seconde évaluation sur les mêmes séries de graphes artiﬁciels, mais cette
fois face à un bruit consistant en une suppression aléatoire des arêtes. La méthode
de ﬁltrage est conﬁgurée ici uniquement pour ajouter comme arêtes les paires de
sommets ayant un score de conﬂuence supérieur à un certain seuil τ +. Aucune arête
ne peut être supprimée par le ﬁltrage.

5.4. PRÉ-TRAITEMENT PAR MARCHES ALÉATOIRES

169

(a) 5 groupes de 10 sommets (n = 5)

(b) 15 groupes de 10 sommets (n = 15)

Figure 5.7 – Résultats de la méthode de ﬁltrage sur des graphes artiﬁciels bruités
par une suppression aléatoire des arêtes. Le ﬁltrage consiste à ajouter les paires
ayant une conﬂuence (calculée pour t = 2) strictement supérieure à τ + = 0.5. Pour
une description en détail de la signiﬁcation de ces courbes voir la légende de la ﬁgure 5.5.

La ﬁgure 5.7 donne les résultats de la méthode de ﬁltrage en fonction de la
quantité d’arêtes supprimées aléatoirement. Le score de conﬂuence, pour qu’une
paire soit ajoutée comme arête, est ﬁxé à τ + = 0.5. La conﬂuence est toujours
calculée avec t = 3.

On observe que les résultats sont très diﬀérents pour les deux tailles de graphe.
Sur les petits graphes, en ﬁgure 5.7a, le nombre d’arêtes des graphes ﬁltrés reste
similaire au nombre d’arêtes de départ, et jusqu’à une suppression de 30% des arêtes,
le ﬁltrage récupère ces arêtes supprimées (Jaccard≈ 1). À l’inverse sur les grands
graphes, en ﬁgure 5.7b, la procédure de ﬁltrage engendre un ajout d’arêtes dès le
départ lorsqu’il n’y a pas encore de bruit. Dans les graphes « b », il y a donc un
nombre important de paires de sommets, non arêtes, ayant un score de conﬂuence
supérieur à τ + = 0.5.

La ﬁgure 5.8 donne les résultats lorsque le seuil τ + varie. Le bruit est ﬁxé à 0.4,

c’est-à-dire 40% des arêtes ont été supprimées aléatoirement.

On observe sur la ﬁgure 5.8a qu’un seuil entre 0.45 et 0.6 permet de réduire
convenablement le bruit, alors que sur la ﬁgure 5.8b il faut un seuil entre 0.65 et 0.8.
En ﬁgure 5.8a les paires non-adjacentes ont toutes un score de conﬂuence inférieur
à 0.5, alors que les arêtes ont un score supérieur à 0.5. Cette limite autour de 0.5 se
déplace vers 0.7 sur la ﬁgure 5.8b.

170

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

(a) 5 groupes de 10 sommets (n = 5)

(b) 15 groupes de 10 sommets (n = 15)

Figure 5.8 – Résultats de la méthode de ﬁltrage par ajout d’arêtes, en fonction
du seuil sur des graphes artiﬁciels bruités par une suppression aléatoire des arêtes.
La conﬂuence est calculée avec t = 3. Le seuil sur la conﬂuence est indiqué sur l’axe des
abscisses, les paires de sommets (non-arêtes) ayant un score supérieur au seuil sont ajoutées.
Le bruit est ici constant : 40% des d’arêtes de départ sont supprimées aléatoirement. Pour
une description en détail de la signiﬁcation de ces courbes voir la légende de la ﬁgure 5.5.

5.4.3.3 Conclusion des évaluations sur les graphes artiﬁciels

Il est possible de résumer les remarques faites sur ces deux premières séries

d’évaluations de la manière suivante. Pour les graphes des ﬁgures « a » :

• le score de conﬂuence des arêtes est supérieur à 0.5 (et proche de 0.5),
• et celui des non-arêtes est inférieur à 0.5.
Alors que pour les graphes des ﬁgures « b » :

• le score de conﬂuence des arêtes est supérieur à 0.7.
• et celui des non-arêtes est inférieur à 0.7 (et souvent supérieur à 0.5).

Sur ce type de graphe, la valeur de la conﬂuence dépend donc du nombre de groupes
de sommets.

Pour les arêtes cela se comprend bien avec le raisonnement suivant. Plus un
graphe comporte de groupes de sommets plus la probabilité limite d’une marche
aléatoire (π(v), voir eq. (5.31)) tend à être faible. En revanche, la valeur des marches
aléatoires à temps courts entre deux sommets du même groupe ne varie pas énormé-
ment entre les deux séries de graphes. En eﬀet la taille (et la densité) des groupes de

5.4. PRÉ-TRAITEMENT PAR MARCHES ALÉATOIRES

171

sommets est la même dans les deux cas. Les probabilités à temps courts ne varient
donc pas ou peu, alors que les valeurs limites changent : on comprend donc bien ces
variations des valeurs de conﬂuence.

Le fait qu’un certain nombre de paires de sommets non-adjacents aient quand
même un score supérieur à 0.5 sur les graphes des ﬁgures « b », s’explique par le
fait que les graphes utilisés présentent un fort recouvrement (des propriétés) entre
les groupes. En eﬀet chaque groupe de sommets a autant de propriétés propres que
de propriétés partagées avec un autre groupe. Imaginons par exemple une paire de
sommets (a, b) constituée d’un objet a appartenant à un groupe A et d’une propriété
b étant une propriété propre à un groupe B. Il est possible que les groupes A et B
partagent un ensemble de propriétés. Il existe alors un nombre important de chemins
courts entre a et b, et donc une forte conﬂuence entre a et b. Le recouvrement
important entre les groupes de sommets dans ces graphes fait que des paires de
sommets peuvent avoir une conﬂuence assez forte sans appartenir au même groupe.
Ces paires de sommets existent dans les graphes des ﬁgures « a », mais la valeur
limite des marches aléatoires étant forte, leur conﬂuence reste en dessous de 0.5.

La suppression des arêtes ayant une conﬂuence inférieure à 0.5 est donc eﬃcace,
sur ces graphes. Utiliser un seuil plus élevé peut être intéressant, mais au risque de
supprimer aussi de « bonnes » arêtes. Aussi, l’utilisation de la méthode pour ajouter
des arêtes est possible. Mais le choix du seuil est plus compliqué, un seuil de 0.5
engendrant un nombre trop important d’ajouts. Cela est dû au recouvrement fort
existant dans les graphes utilisés, en eﬀet la moitié des propriétés appartient à deux
groupes de sommets.

5.4.3.4 Graphes réels documents-mots

Le tableau 5.6 présente le résultat de notre méthode de ﬁltrage sur 25 graphes
reliant des documents à des termes. Ces graphes sont issus d’une application pré-
sentée au chapitre suivant (section 6.4). Chacun comporte 100 sommets documents
(sauf le graphe #20 qui n’en comporte que 40). Ces documents sont les 100 premiers
retournés par un moteur de recherche pour une requête donnée. Un lien entre un
document et un terme indique que le terme est présent dans le document. Chacune
des arêtes est pondérée par un score de BM25, mesurant à quel point un terme
est signiﬁcatif pour un document. Tous les termes des documents ne sont pas pré-
sents dans le graphe, car diﬀérent ﬁltres sont appliqués. Pour plus de détails sur ces
graphes nous renvoyons à la sous-section 6.4.2 du chapitre suivant.

La suppression des arêtes ayant une conﬂuence inférieure à τ− = 0.5 semble
particulièrement eﬃcace. En eﬀet en moyenne 27% des arêtes sont supprimées, et
cela résulte en moyenne en une diminution de 64% des concepts. Supprimer ces
arêtes semble donc particulièrement eﬃcace pour réduire le nombre de concepts.

Par contre le ﬁltrage par ajout d’arêtes engendre une augmentation importante

172

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

du nombre d’arêtes et une augmentation très importante du nombre de concepts.
Ces deux observations sont à mettre en parallèle avec les remarques faites précé-
demment sur des graphes artiﬁciels. En particulier le fait que beaucoup de paires non-
adjacentes aient un score supérieur à 0.5 pourrait résulter, comme sur les graphes
artiﬁciels, d’un fort recouvrement des termes entre les groupes de documents. Il se-
rait intéressant de voir si un seuil plus contraignant pour l’ajout d’arêtes permet de
réduire le nombre de concepts.

5.4.4 Conclusion et perspectives

Nous avons présenté ici une méthode de ﬁltrage de graphe biparti basée sur une
mesure de similarité entre sommets. Une première série d’évaluations nous a permis
de montrer que cette méthode est eﬃcace pour retirer des arêtes résultant d’un bruit
aléatoire sur des graphes artiﬁciels. En revanche l’utilisation de cette méthode pour
ajouter des liens semble plus délicate. Des résultats compatibles avec ces conclusions
sont obtenus sur des graphes réels : la suppression des arêtes de faible conﬂuence
permet de réduire le nombre de concepts, en revanche l’ajout des paires de sommets
(non-arêtes) engendre une augmentation importante du nombre de concepts.

Le choix du seuil mériterait une évaluation plus approfondie. En particulier : est-
ce que, sur ces graphes réels, un seuil plus restrictif permet de réduire le nombre de
concepts ? On pourrait alors imaginer une méthode qui choisisse le seuil de manière à
minimiser le nombre de concepts. Toutefois cela poserait un problème de complexité.
De plus il suﬃt d’ajouter toutes les arêtes possibles pour qu’il n’y ait plus qu’un
seul concept.

Aussi plutôt que d’utiliser naïvement la conﬂuence pour ajouter toutes les paires
ayant un score supérieur à un certain seuil, une approche intéressante pourrait être
d’utiliser la conﬂuence (ou une mesure similaire) dans un algorithme de regroupe-
ment a posteriori des concepts formels. Par exemple en regroupant toutes les paires
de concepts telles que la conﬂuence minimale des liens manquants dans leur union
soit inférieure à un certain seuil. Notons que le défaut principal de ces méthodes de
regroupement des concepts est qu’elles ne peuvent pas faire disparaître des concepts
créés par des arêtes erronées. Dans l’optique d’un usage combiné de notre méthode de
pré-traitement et d’une telle méthode de post-traitement par agrégation de concepts,
les résultats de la méthode proposée ici sont donc tout à fait encourageants.

Table 5.6 – Résultat de la méthode de ﬁltrage sur les 25 graphes documents-mots. |Vdoc|, |Vterm|, |E| et |T| correspondent
respectivement aux nombres de documents, de termes, d’arêtes et de concepts dans les graphes. |Ecut−| et |Tcut−| sont respectivement les
nombres d’arêtes et de concepts après un ﬁltrage supprimant les arêtes de conﬂuence inférieure à 0.5. |Ecut+| et |Tcut+| donnent ces valeurs
après un ﬁltrage ajoutant les paires de sommets de conﬂuence supérieure à 0.5. Enﬁn |Ecut+−| et |Tcut+−| donnent ces valeurs pour un
ﬁltrage permettant l’ajout (conﬂuence > 0.5) et la suppression (conﬂuence < 0.5). Les conﬂuences sont calculées avec t = 3.
requête
#0 : « mykonos site »
#1 : « creation d’objets en ﬁl de fer »
#2 : « taux TSH »
#3 : « risques auditifs »
#4 : « abeilles et gaucho »
#5 : « lichens bioindicateurs »
#6 : « alsace nature »
#7 : « Gingko »
#8 : « protection des mains »
#9 : « grues mobiles d’atelier »
#10 : « noirmoutier camping »
#11 : « camping car occasion traﬁc »
#12 : « crampes aux pieds nocturne »
#13 : « tarte aux fruits de mer »
#14 : « lactation regurgitation »
#15 : « poème sur combattre la maladie »
#16 : « le gouﬀre de poudrey »
#17 : « meringues fondantes a l’interieur »
#18 : « greﬀe en ecusson »
#19 : « bruyeres le chatel »
#20 : « Hypertyroidie »
#21 : « Mercilon »
#22 : « refuge du chatelleret »
#23 : « le bourg d’oisans hotel »
#24 : « resultats templiers 2008 »

|T|
|E|
982
328 1 603
7 872
410 2 719
7 125
468 2 684
361 1 745
1 311
469 3 048 11 008
2 297
421 1 955
2 673
418 2 165
302
246
854
403 1 788
965
1 472
370 1 991
2 345
373 2 073
492 3 110
9 089
461 2 399
3 286
539 4 466 32 254
807
284 1 410
287
800
252
327 1 622
1 205
438 2 985 17 779
257 1 131
1 201
4 513
567 3 964
204
821
726
2 608
476 2 164
440 2 170
3 935
1 383
426 1 908
386 1 511
945

|Tcut+|
|Ecut+|
187 567
5 985
9 411
464 550
10 478 1 014 688
7 680
187 246
10 928 1 555 274
211 401
8 232
143 373
7 768
16 259
3 193
6 702
117 398
269 642
7 925
226 140
7 881
11 189
454 372
9 573 2 063 929
15 138 4 099 879
45 770
5 367
3 537
7 705
7 466
225 820
10 951 2 487 591
4 896
109 024
11 360 1 610 967
2 072
13 034
9 265 2 698 881
9 463 4 342 913
51 068
7 635
6 583
206 621

|Tcut+−|
|Ecut+−|
181 116
5 699
304 753
8 649
428 663
9 396
181 917
7 299
665 568
9 821
168 019
7 664
76 922
7 049
10 128
3 100
100 452
6 451
188 852
7 492
134 199
7 233
246 769
10 257
8 907 1 602 896
13 470 1 631 665
46 068
5 153
3 469
7 369
141 242
7 030
897 536
9 545
4 783
116 021
11 315 1 543 676
1 748
5 417
8 966 2 254 140
8 859 4 344 610
41 021
7 283
6 468
210 226

|Vterm|

|Vdoc|
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
41
100
100
100
100

|Ecut−|
1 285
1 910
1 583
1 302
1 938
1 330
1 384
712
1 468
1 540
1 406
2 162
1 698
2 792
1 133
601
1 143
1 571
960
3 891
468
1 847
1 532
1 512
1 285

|Tcut−|
374
2 285
856
594
1 153
635
567
188
433
531
598
1 043
936
1 919
419
111
346
1 106
907
4 467
158
1 387
1 045
489
648

5
.
4
.

P
R
É
-
T
R
A
I
T
E
M
E
N
T
P
A
R
M
A
R
C
H
E
S
A
L
É
A
T
O
R
E
S

I

1
7
3

174

CHAPITRE 5. CLUSTERING DE GRAPHE BIPARTI

5.5 Conclusion du chapitre

Nous avons présenté dans ce chapitre trois contributions au problème de cluste-
ring de graphe bipartis. Tout d’abord nous avons présenté un parallèle entre l’analyse
formelle de concepts (étendue) et l’analyse de graphe biparti, ce parallèle apporte
un éclairage intéressant sur ce que doit être un cluster dans un graphe. Nous avons
ensuite proposé une méthode permettant d’obtenir une partition des objets sans
imposer de partition correspondante sur les propriétés. Nous avons montré l’intérêt
de l’approche sur un jeu de graphes artiﬁciels et sur un jeu de données réelles. Et
enﬁn nous avons présenté une méthode de pré-traitement d’un graphe biparti en vue
de simpliﬁer le résultat de l’analyse formelle de concepts, cette méthode permettant
aussi de binariser un graphe biparti pondéré. Nous avons étudié les résultats de cette
méthode sur des graphes artiﬁciels et sur des graphes réels. Résultats qui montrent
les limites de cette approche, mais aussi son intérêt quand ce pré-traitement ne fait
que supprimer des arêtes.

Ce travail de comparaison des diﬀérentes méthodes de clustering de graphe est à
poursuivre. En particulier, il nous semblerait important d’eﬀectuer une comparaison
tant théorique qu’expérimentale des méthodes de bi-clustering et des méthodes dites
de détection de communautés (appliquées au graphes bipartis). Cela aﬁn de mieux
comprendre les hypothèses implicites faites sur les données, ainsi que les diﬀérentes
déﬁnitions (souvent implicites) données aux clusters. Bien sûr cela pourrait aider
à imaginer des nouvelles méthodes plus eﬃcaces. Mais ces diﬀérentes familles de
méthodes étant souvent utilisées dans des champs d’application diﬀérents, un tel
parallèle pourrait se montrer productif, même s’il ne débouche pas sur la création
de nouvelles méthodes.

Aussi une évaluation plus poussée des méthodes que nous avons proposées se-
rait souhaitable. En particulier une évaluation comparative avec les méthodes de la
littérature les plus semblables. Notons que cette évaluation est toutefois prolongée,
sur une application réelle, dans le chapitre suivant.

En eﬀet le chapitre suivant présente une application possible du problème de
clustering de graphe biparti vu dans ce chapitre. Cette application consiste à caté-
goriser automatiquement, de manière non supervisée, les résultats d’une recherche
de documents.

175

Chapitre 6
Application à l’organisation
des résultats d’une recherche
d’information

L’objectif de ce chapitre est de présenter Kodex, un système de recherche d’in-
formation modulable permettant une classiﬁcation automatique des résultats d’une
recherche de documents.

Ce travail apporte une contribution double : à la fois logicielle et scientiﬁque.
En eﬀet Kodex est un cadre logiciel modulable, et donc largement réutilisable, pour
enrichir d’une classiﬁcation automatique les résultats d’une recherche. Cette mo-
dularité permet de construire facilement tant des applications « réelles », que des
scripts d’évaluation. Enﬁn l’application de méthodes récentes de clustering de graphe
biparti (voir chapitre 5) au problème de clustering des résultats d’une recherche d’in-
formation est nouvelle et l’évaluation menée montre l’intérêt de l’approche.

L’approche générale de Kodex est introduite en section 6.1. Nous présentons
ensuite, en section 6.2 un état de l’art des méthodes de clustering appliquées à la
recherche de documents. Avant de présenter plusieurs applications, les grandes lignes
de l’architecture du cadre applicatif mis en place sont décrite en section 6.3. En sec-
tion 6.4, une application sur une collection de 2,6 millions de pages web est présentée.
Cette application permet de proposer une première évaluation de l’approche, ainsi
qu’une comparaison de diﬀérentes méthodes de clustering de graphe. En section 6.5,
nous présentons deux applications fonctionnelles construites l’une sur la collection
en ligne du journal le Guardian, l’autre sur la base de publication DBLP. Enﬁn la
section 6.6 conclut le chapitre en présentant les pistes d’améliorations envisagées.

176
6.1

CHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION
Introduction, clustering de graphe documents-
termes

Nous présentons ici les grandes lignes de l’approche que nous développons avec
le cadre logiciel Kodex. L’idée est de proposer à l’utilisateur une organisation en
diﬀérents groupes des documents retournés pour une recherche. Cela doit permettre
de rendre visible la polysémie de la requête par rapport à la collection de documents,
et d’aider l’utilisateur à raﬃner sa recherche. Un exemple de cas d’utilisation est
donné en ﬁgure 6.1. Dans cet exemple, un utilisateur cherche « japan » et le système
lui indique une liste de documents classés en trois clusters : l’un concernant plutôt
les aspects historiques et culturels, un autre à propos d’informations touristiques, et
un dernier relevant de l’actualité. Kodex est basé sur une méthode de clustering de
graphe biparti entre documents et termes. La ﬁgure 6.2 résume schématiquement le
fonctionnement du système en réponse à une requête.

Figure 6.1 – Cas d’utilisation de Kodex

6.1. INTRODUCTION, CLUSTERING DE GRAPHE
DOCUMENTS-TERMES

177

Figure 6.2 – Schéma général de fonctionnement de Kodex

Kodex fonctionne par dessus un système de recherche d’information classique
(SRI dans la suite) qui retourne une liste ordonnée de résultats pour une requête.
C’est la phase Search sur la ﬁgure 6.2. Étant donnée une telle liste de résultats, Ko-
dex dans une première étape (la phase Graph Building sur la ﬁgure 6.2) construit un
graphe biparti entre documents et termes. La méthode de construction du graphe,
c’est-à-dire la méthode d’extraction des termes de chaque document, peut varier en
fonction des informations disponibles sur les documents. Nous appelons « modélisa-
tion » d’un document l’ensemble des termes pondérés ainsi associé à celui-ci. Cette
modélisation est la seule représentation des documents utilisée pour construire les
clusters. Ce graphe est ensuite découpé par une méthode de clustering de graphe
biparti (phase Clustering sur la ﬁgure 6.2). Enﬁn, chaque cluster se voit attribuer un
certain nombre d’étiquettes (labels), c’est la phase Cluster labelling sur le schéma.
Le fonctionnement de Kodex peut donc être résumé par ces quatre étapes princi-
pales : recherche « classique », construction d’un graphe biparti, clustering de celui-
ci, et calcul d’étiquettes pour chacun des clusters.

Notons que la chaîne de traitement d’une requête de Kodex permet de conﬁgurer
des étapes intermédiaires de transformation, d’analyse ou de ﬁltrage des données
entre chacune de ces quatre étapes principales. Ces étapes intermédiaires permettent
par exemple d’interroger un index pour récupérer une modélisation des documents

(online)Search- Terrier, SOLR,lucene, ...- online API(Yahoo, Guardian, ...)GraphBuildingClusteringClusterlabellingdnumtitle... terms  + stats- label 1- label 2- label 3- label 1- label 2- label 3- label 4- label 1- label 2- label 3- label 4QUERYtermtermtermtermtermtermtermtermCHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION
178
calculée hors-ligne. Ou encore de ﬁltrer les clusters pour regrouper les –trop– petits
clusters dans un cluster « divers ». Cette chaîne de traitement est présentée plus en
détail en section 6.3.5.

Pour la construction du graphe, diﬀérentes techniques de modélisation des do-
cuments sont envisageables. La plus simple est d’utiliser les informations retournées
par le SRI. Malheureusement, ces informations se limitent souvent aux titres et à
de courts extraits des documents. Parfois des mots-clés peuvent être fournis par le
SRI et se révéler pertinents pour le clustering. C’est ce que nous utilisons dans une
application mise en place sur le SRI du journal anglais le Guardian (présentée en
section 6.5.1). Une autre solution consiste à extraire certains termes des documents
complets. Pour cela, une méthode simple consiste en trois étapes : extraire tous
les termes, les pondérer puis les ﬁltrer. Kodex permet de prendre en charge un tel
traitement hors-ligne. Nous revenons sur ce point en section 6.3.6.

Les diﬀérentes méthodes de clustering de graphe biparti présentées au chapitre
précédent peuvent être utilisées ici. Nous utilisons en particulier la méthode intro-
duite au chapitre précédent en section 5.3.

Notons enﬁn que ce travail, et en particulier la conception et le développement

logiciel, a été fait en collaboration avec Yannick Chudy et Benoit Gaillard.

6.2 État de l’art : le clustering appliqué à la re-

cherche d’information

Nous proposons dans cette section un état de l’art des diﬀérentes méthodes
utilisées pour classiﬁer automatiquement les résultats d’une recherche. Cet état de
l’art permet de mettre en exergue les diﬃcultés spéciﬁques au clustering des résultats
d’une recherche de documents par rapport au problème général de clustering. Pour
un état de l’art plus complet, nous renvoyons à l’article de Carpineto et al. [2009] 1.
Le clustering des résultats d’un SRI a été introduit pour la première fois par Cut-
ting et al. [1992]. Les clusters y étaient construits en utilisant Fractionation. C’est un
algorithme de clustering hiérarchique basé sur la modélisation vectorielle des docu-
ments [Salton et al., 1975b], introduit avec le système Scatter/Gather. Son principal
avantage résulte de sa complexité linéaire. En revanche, il ne permet pas de traiter
le problème de la polysémie, chaque document étant associé à un seul cluster. De
plus, le nombre de clusters produits est un paramètre ﬁxé par l’utilisateur, ce qui
nous semble peu réaliste. Ce nombre de clusters ne doit-il pas plutôt dépendre de
la polysémie de la requête ? N’existe-t-il pas des requêtes pour lesquelles une orga-
nisation des résultats en plusieurs groupes n’a aucun sens ? Ce sont là des points
1. Notons tout de même que les travaux [Boley et al., 1999],[Dhillon, 2001],[Mecca et al.,

2007],[Chen et al., 2008] ne sont pas cités dans cette revue.

6.2. ÉTAT DE L’ART : LE CLUSTERING APPLIQUÉ À LA
179
RECHERCHE D’INFORMATION
importants qui ont souvent été mis de côté dans les techniques proposées dans l’état
de l’art. Enﬁn c’est un algorithme centré sur les données (data-centric 2, pour re-
prendre la classiﬁcation de Carpineto et al. [2009]) et l’étiquetage des clusters n’est
pas forcément pertinent et interprétable par l’utilisateur (pour chaque cluster, les
termes les plus fréquents sont utilisés comme étiquettes).

Une approche très diﬀérente a ensuite été proposée avec Grouper [Zamir et Et-
zioni, 1998, 1999]. Ce système repose sur l’algorithme STC (Suﬃx Tree Clustering)
qui rassemble les documents sur la base d’une seule propriété bien choisie : STC re-
groupe les documents qui partagent des expressions suﬃsamment fréquentes (dans
l’ensemble des extraits de documents). L’astuce principale de STC repose sur l’uti-
lisation d’un arbre de suﬃxe généralisé pour rechercher les séquences de termes les
plus fréquentes en un temps linéaire. L’intérêt d’une telle approche est que les clus-
ters ont une justiﬁcation clairement lisible pour l’utilisateur ﬁnal (description-aware
selon Carpineto et al. [2009]). En revanche, nous doutons de la robustesse de telles
méthodes, il suﬃt en eﬀet de changer l’ordre de deux mots ou de remplacer un mot
par un synonyme pour que les séquences les plus fréquentes changent complètement.
Depuis ces deux travaux de référence, de nombreuses autres contributions ont
été proposées, dont : SHOC [Zhang et Dong, 2004], SnakeT [Ferragina et Gulli,
2005], Lingo [Osinski et Weiss, 2005] et Noodles [Mecca et al., 2007]. Il est assez
remarquable que la plupart de ces méthodes (toutes celles citées ici sauf SnakeT) se
basent sur la technique de Latent Semantic Indexing (LSI) introduite par Deerwester
et al. [1990]. Ceci est compréhensible car, d’une part, en prenant en considération
la co-occurrence des termes, la LSI répond au problème de la synonymie (c’est-à-
dire : rapprocher deux documents même s’ils n’ont aucun mot en commun mais
seulement des mots synonymes, voir [Manning et al., 2008, page 378]). D’autre part,
la complexité importante de la LSI est peu gênante car la taille des données est
faible car réduite au sous-ensemble de documents restitués par un SRI.

La majeure partie des approches (toutes sauf Scatter/Gather Cutting et al. [1992]
et Noodles [Mecca et al., 2007]) sont basées sur les extraits de documents plutôt que
sur les textes complets. C’est un intérêt évident au regard de la complexité, et cela
permet de faire facilement fonctionner les systèmes au-dessus d’un SRI existant en
analysant uniquement (et en ligne) les extraits restitués par celui-ci. Par ailleurs,
Zamir et Etzioni [1998] n’ont pas observé d’augmentation de la qualité des clusters en
utilisant les documents complets plutôt que les simples extraits. On peut cependant
penser que ce résultat est dû à un comportement particulier de l’algorithme STC.
En eﬀet, Mecca et al. [2007] montrent qu’avec une méthode basée sur la LSI, les
résultats sont signiﬁcativement meilleurs quand les documents complets sont utilisés.
2. Selon la dénomination de Carpineto et al. [2009], les approches de clustering data-centric sont
centrées sur la qualité des clusters ne se préoccupant pas de l’interprétabilité des clusters, alors
qu’au contraire les méthodes description-aware sont centrées sur l’interprétabilité des clusters par
l’utilisateur.

180

CHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION
Alors que les approches précédentes reposent principalement sur des modélisa-
tions vectorielles des documents, quelques auteurs ont traité le problème de cluste-
ring des résultats d’un SRI comme un problème de partitionnement de graphe [Boley
et al., 1999; Dhillon, 2001]. Le partitionnement d’un graphe consiste à trouver une
partition de l’ensemble des sommets qui minimise la taille de la coupe induite (c’est-
à-dire le nombre total d’arêtes entre deux groupes). Nous avons vu au chapitre pré-
cédent (section 5.1) que ce type d’approche pose problème quand on souhaite faire
émerger un découpage pré-existant dans les données.

Le travail récent de Chen et al. [2008] exploite une technique de clustering de
graphe (par optimisation de la modularité) sur un graphe de termes construit à
partir des co-occurrences de ceux-ci dans les extraits de documents restitués. Le but
est de trouver les word sense communities qui permettent ensuite de construire des
clusters de documents. Comme le signalent les auteurs, la principale originalité de
cette approche par détection de communautés est que le nombre de clusters n’est pas
un paramètre ﬁxé mais dépend des données. Les méthodes de clustering que nous
utilisons conservent cet avantage. Seulement notre approche est diﬀérente puisque
nous proposons de travailler sur un graphe biparti documents-termes, et non sur un
graphe uni-parti de termes.

Nous pouvons remarquer que beaucoup des méthodes de l’état de l’art construisent
d’abord des clusters de termes, et y attachent ensuite les documents (c’est par
exemple le cas de Lingo et de la méthode de Chen et al. [2008]). Nous retrouvons
ici, en un sens, la dichotomie proposé par Carpineto et al. [2009] entre approches
description-aware et approches data-centric. Les premières ayant pour avantage des
descriptions a priori plus pertinentes, alors que les secondes proposent des clusters,
a priori, correspondant mieux aux documents. Bien sûr la description des clusters
est primordiale quand on cherche à utiliser une méthode de clustering pour guider
un utilisateur dans les résultats d’une recherche. Cela étant dit, la pertinence des
clusters vis à vis du jeu de documents concernés n’est pas à minimiser. Imaginons
par exemple un résultat comportant deux clusters qui font sens par rapport à leurs
étiquettes, mais tels que tous les documents appartiennent à ces deux clusters :
l’intérêt d’un tel découpage est limité. On peut aussi se demander si le problème
consistant à attacher un ensemble de documents à un cluster de termes n’est pas
similaire au problème d’étiquetage, consistant à attacher un ensemble de termes à
un cluster de documents ?

L’approche que nous adoptons dans Kodex est intermédiaire, puisque nous cons-
truisons des clusters contenant à la fois documents et termes. Dans la pratique,
utiliser directement les termes des clusters comme étiquettes de ceux-ci pose plu-
sieurs problèmes. Tout d’abord le nombre de termes est souvent important, une
technique de sélection est donc nécessaire. De plus les termes les plus centraux des
clusters ne sont pas forcément les plus lisibles. Dans la pratique l’approche de Kodex

6.3. IMPLÉMENTATION LOGICIELLE D’UNE CHAÎNE DE
TRAITEMENT MODULABLE (KODEX)
est donc plus data-centric.

Notons enﬁn que le cadre logiciel de Kodex permet de développer une approche
qui passe d’abord par des clusters de termes, nous avons par exemple pu intégrer
l’implémentation de Lingo proposée par le logiciel Carrot.

181

6.3

Implémentation logicielle d’une chaîne de trai-
tement modulable (Kodex)

Cette section a pour objectif de décrire les grandes lignes de l’architecture du
cadre logiciel Kodex. Nous présentons tout d’abord les motivations et les besoins
auxquels répond ce logiciel. Nous introduisons ensuite les types de données manipu-
lées (sous-section 6.3.2), et le système de composant de traitement et de chaînage de
ces composants (sous-sections 6.3.3 et 6.3.4). Enﬁn les fonctionnements des chaînes
de traitement en-ligne (sous-section 6.3.5) et hors-ligne (sous-section 6.3.6) sont ex-
posés.

6.3.1 Objectifs, motivations et choix techniques

Kodex permet d’organiser automatiquement les résultats d’une recherche. En ce
sens Kodex n’est pas un SRI à proprement parler, mais un « méta-moteur » : il se
place au-dessus un SRI existant. Seulement une contrainte supplémentaire s’applique
à Kodex : le contenu textuel des documents doit être connu pour construire le graphe
et construire des groupes de documents. Il faut donc que Kodex puisse analyser les
documents et construire son propre index en se basant éventuellement sur un SRI.
Kodex doit donc fournir, en plus d’une chaîne de traitement en-ligne, des outils pour
analyser et indexer des documents hors-ligne.

Dès le départ Kodex a été conçu pour réaliser des évaluations, mais aussi pour
mettre en place des applications fonctionnelles. L’objectif est d’être utilisable tant
pour la recherche que pour une « mise en production ».

Aﬁn de mener à bien diﬀérentes évaluations, et d’arriver à un prototype fonction-
nel performant, Kodex a été construit de manière à être extrêmement modulable.
Il est important de pouvoir facilement modiﬁer ou changer chacune des parties du
traitement, pour pouvoir facilement comparer diﬀérentes approches. Aussi chacune
des étapes doit pouvoir être conﬁée à un service externe. Kodex doit pouvoir être
facilement « branché » à un logiciel tiers. En particulier les parties concernant la re-
cherche d’information « classique » n’étant pas faites par Kodex, il faut que diﬀérents
SRI puissent être utilisés.

Cette modularité des chaînes de traitement doit être accompagnée d’outils pour
faciliter, voire automatiser, la construction des applications d’évaluations et de dé-
monstrations. Il est nécessaire que la modiﬁcation d’un composant d’une chaîne de

CHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION
182
traitement ne demande pas de modiﬁcation des applications qui l’utilisent. Sans cela
la modularité de Kodex serait toute relative.

Enﬁn Kodex a été conçu pour permettre un prototypage rapide. C’est-à-dire
pouvoir rapidement mettre en place et expérimenter de nouvelles méthodes pour
chacune des phases du traitement.

Toutes ces motivations nous ont amené au choix de l’architecture présentée dans
les sous-sections suivantes. Ces besoins nous ont aussi amené à choisir de développer
Kodex en Python. En eﬀet Python permet un développement rapide (donc un pro-
totypage rapide) tout en étant suﬃsamment solide pour construire des applications
de production (par exemple une application web). Aussi la souplesse de Python aide
à construire facilement une solution modulable et facilement interfaçable avec des
logiciels tiers. Le dernier avantage de taille de Python est le vaste écosystème de
bibliothèques disponibles. En particulier, Kodex utilise intensément la bibliothèque
igraph [Csardi et Nepusz, 2006].

Les deux inconvénients majeurs de Python sont ses faibles performances (face à
des langages compilés), et le typage dynamique (qui est aussi l’un de ses avantages)
et donc le manque de vériﬁcation faite à la compilation. On peut éviter les pièges
de ce second problème par une rigueur dans le développement, la création de tests
unitaires et une bonne documentation construite au fur et à mesure. Le premier
problème n’est pas critique pour nous car les parties les plus coûteuses en temps de
calcul sont soit eﬀectuées par des logiciels tiers (codés dans des langages compilés),
soit eﬀectuées par des fonctions codées en C et appelées depuis Python, en particulier
avec les bibliothèques numpy et igraph.

6.3.2 Types de données principaux

Nous présentons ici les principaux types de données complexes manipulées par

Kodex.

Documents. Dans les chaînes de traitement de Kodex, les documents sont des
objets KodexDoc. Ces KodexDoc comportent des champs libres, et des listes d’élé-
ments. L’ensemble des champs utilisés n’est pas déﬁni par avance, c’est une approche
« sans-schéma » (schema free). Seulement le champ docnum est toujours renseigné,
il indique l’identiﬁant du document dans la collection. Un KodexDoc est en fait un
dictionnaire Python (une table d’association) gérant en plus un mécanisme de listes
d’éléments. Ces listes d’éléments sont utilisées typiquement pour indiquer la liste des
termes associés au document. Elles peuvent être multiples (par exemple les termes
du titre et les termes du corps de texte) et des données peuvent être associées à
chacun de ces éléments (par exemple la fréquence dans le document, ou encore la
catégorie lexicale).

6.3. IMPLÉMENTATION LOGICIELLE D’UNE CHAÎNE DE
TRAITEMENT MODULABLE (KODEX)
183
Graphes. Pour la gestion des graphes, Kodex utilise intensément la bibliothèque
igraph 3 [Csardi et Nepusz, 2006]. C’est une bibliothèque écrite en C et proposant
une interface en Python. Les graphes construits par Kodex sont donc des objets
Graph déﬁnis dans cette bibliothèque.

Clustering. Le résultat des composants de clustering utilise le type GraphCover de
la bibliothèque igraph, mais lors du traitement en ligne, un clustering est convertit
en un objet KodexCover. Une objet KodexCover est une liste de Cluster, chaque Cluster
comportant une liste de documents, une liste de termes et une liste d’étiquettes. En
eﬀet cela permet d’indiquer les étiquettes directement dans l’objet clustering.

6.3.3 Chaînes de traitement, chaînes de composants

Les diﬀérentes étapes de traitement des données sont eﬀectuées par un chaînage
conﬁgurable de composants élémentaires. Les données passent successivement dans
chacun des composants pour y être traitées. Cela permet une grande modularité
et une factorisation eﬃcace des diﬀérentes étapes de traitement des données. Nous
présentons ici les grandes lignes du fonctionnement de ce système de chaînes de com-
posants. Notons que cette architecture est largement inspirée du SRI libre Whoosh
[Chaput, 2009].

# construction de la mé thode de clustering
2 fca_infomap = GraphOfConcepts () | Infomap ()

# exemple d’utilisation

4 gb = igraph . Graph . Formula ("A:B:C--a:b:c, D:E:F--b:c:d:e")

clusters = fca_infomap (gb)

6 goc = clusters . graph # récupé ration du graphe objets / concepts

Listing 6.1 – Exemple simple de chaîne de traitement

Le listing 6.1 présente un exemple simple de chaîne de traitement correspon-
dant à la méthode de clustering de graphe biparti présentée au chapitre précédent
(voir section 5.3). L’objet fca_infomap est une chaîne combinant deux composants
de base : GraphOfConcepts() et Infomap(). Le composant GraphOfConcepts() transforme
un graphe biparti en un graphe objets/concepts et Infomap() applique la méthode de
clustering Infomap et retourne un partitionnement des sommets du graphe. Cette
chaîne est exécutée, à la ligne 5, sur le graphe gb créé à la ligne précédente. Notons
que le partitionnement retourné (clusters) est un partitionnement des sommets du
graphe objets/concepts et non un partitionnement de gb. Le graphe objets/concepts
est accessible depuis le clustering (goc en ligne 6) ainsi il est possible d’en déduire
un partitionnement des sommets de gb, cette étape n’est pas montrée ici.

3. http://igraph.sourceforge.net/

184

CHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION
Un traitement complexe peut donc, avec ce système, être découpé en une succes-
sion de composants élémentaires. Cela permet de construire facilement les chaînes
de traitement, et de factoriser eﬃcacement le code de certaines opérations entre
diﬀérentes chaînes de traitement.

2

4

6

8

cla ss ToUndirected ( Composable ):

def __init__ ( self ):

Composable . __init__ ( self )

def __call__ (self , graph ):

graph_undir = graph . copy ()
graph_undir . to_undirected ()
return graph_undir

Listing 6.2 – Exemple de composant élémentaire

Un composant élémentaire est une classe qui hérite de la classe abstraite Composable
et qui implémente la méthode __call__. Un exemple simple est donnée par le lis-
ting 6.2, ce composant d’exemple transforme un graphe dirigé en un graphe non-
dirigé.

La méthode __call__ 4 eﬀectue le traitement en lui même. Elle prend en premier
paramètre la donnée d’entrée et retourne la donnée traitée. Les autres paramètres
de cette méthode __call__ correspondent à des options éventuelles, nous allons le
voir dans la section suivante.

6.3.4 Gestion des options des composants

Chaque composant peut avoir des options dont les valeurs sont données à l’exécu-
tion. Kodex propose un système de déclaration de ces options. Ce système s’occupe
de dispatcher les valeurs de chaque option aux bons composants dans une chaîne
de traitement. Il permet aussi une découverte automatique des options d’un com-
posant ou d’une chaîne de composants. Cela est particulièrement utile pour générer
des formulaires html de conﬁguration des options dans une application web. De la
même manière, cela permet de produire automatiquement un analyseur d’arguments
de lignes de commandes pour exposer et conﬁgurer facilement ces options dans un
programme s’exécutant en ligne de commande.

Le listing 6.3 est une modiﬁcation du listing 6.1, la chaîne de traitement fca_infomap
est maintenant appelée avec deux options : wgt_method et nb_trials. La première op-
tion wgt_method déﬁnit la méthode à utiliser pour agréger les poids du graphe biparti
4. En Python la méthode __call__ est une méthode spéciale appelée quand l’objet est uti-
lisé comme une fonction, par exemple en ligne 5 du listing 6.1 la méthode __call__ de l’objet
fca_infomap est appelée, implicitement.

6.3. IMPLÉMENTATION LOGICIELLE D’UNE CHAÎNE DE
TRAITEMENT MODULABLE (KODEX)

185

# construction de la mé thode de clustering
2 fca_infomap = GraphOfConcepts () | Infomap ()

# exemple d’utilisation

4 gb = igraph . Graph . Formula ("A:B:C--a:b:c, D:E:F--b:c:d:e")
clusters = fca_infomap (gb , wgt_method ="sum", nb_trials =50)

Listing 6.3 – Exemple simple de chaîne de traitement avec options

sur le graphe objets/concepts, c’est une option du composant GraphOfConcepts(). La
seconde option nb_trials est une option du composant Infomap(), elle indique le
nombre de tentatives d’optimisation qui doivent être eﬀectuées. Ces options sont
dispatchées automatiquement aux bons composants élémentaires, sous réserve que
deux composants n’aient pas une option de même nom (cela arrive rarement dans
la pratique et est vériﬁé automatiquement lors de la construction d’une chaîne).

2

4

6

8

10

12

14

16

cla ss ToUndirected ( Composable , Optionable ):

def __init__ (self , name =" to_undirected "):

Composable . __init__ ( self )
Optionable . __init__ (self , name = name )
self . add_enum_option (

" mode ",
[" collapse ", " each ", " mutual "],
" collapse ",
" What to do with multiple edges going between same

vertex pair ",

str

)

def __call__ (self , graph , mode =" collapse "):

graph_undir = graph . copy ()
graph_undir . to_undirected ( mode = mode )
return graph_undir
Listing 6.4 – Exemple de composant élémentaire avec une option

Le listing 6.4 donne un exemple simple de déclaration d’option pour le composant
introduit au listing 6.2. C’est une option à choix multiples permettant de déﬁnir
comment plusieurs arcs entre deux sommets doivent être agrégés.

Il convient ici de faire une distinction entre les options des composants, et ce que
nous appellons les paramètres. Les paramètres sont donnés lors de la construction
du composant (comme arguments de la méthode __init__). Alors que les options
ne sont données que lors de l’exécution du composant (comme arguments de la
méthode __call__). Les paramètres permettent donc une conﬁguration « statique »
des composants (par exemple l’url du serveur à interroger, pour une phase de search).
Alors que les options permettent une conﬁguration « dynamique », à l’exécution

CHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION
186
(par exemple le nombre de documents à retourner). Notons enﬁn qu’il est possible
de forcer la valeur d’une option de manière statique, l’option n’est alors plus exposée
et la valeur indiquée est forcée à l’exécution. Cela est utile pour disposer de toutes
les options lors d’une phase de mise au point et d’expérimentation, puis de « faire
disparaître » certaines de ces options sur une application en production.

6.3.5 Conﬁguration d’une chaîne de traitement en ligne

La chaîne de traitement de requête (en-ligne) de Kodex est composée de cinq

blocs de traitement :
Searcher : eﬀectue la recherche (en faisant appel à un SRI externe) ;
Expander : traite les documents ; par exemple : enrichit les documents en inter-
rogeant un index ou une base de données, ﬁltre les termes en fonction de
diﬀérents scores, ou encore extraits les termes du titre avec un tokeniser 5 ;

GraphBuilder : construit le graphe ;
Clustering : regroupe les sommets du graphe en diﬀérents clusters ;
Labelling : appose des étiquettes à chaque cluster.

Chacun de ces blocs de traitement est réalisé par un composant ou par une chaîne
de composants.

Cette chaîne de traitement est construite par un objet KodexEngineBuilder. Cet
objet permet d’indiquer le ou les composants utilisables pour chacune des étapes, et
ensuite de gérer le traitement d’une requête étant donné le nom des composants à
utiliser et l’ensemble des options. Le KodexEngineBuilder permet aussi à une applica-
tion l’utilisant de découvrir les composants disponibles et leurs options respectives.
Le listing 6.5 donne un exemple de construction d’un KodexEngineBuilder. L’ob-
jet keb permet d’eﬀectuer une recherche par dessus le SRI Terrier [Ounis et al.,
2005]. Par soucis de lisibilité cet exemple est simpliﬁé pour ne présenter que les
concepts les plus importants. Dans cet exemple un seul composant est disponible
pour les phases search, expander et graph builder. Par contre, trois méthodes de
clustering et deux de labelling sont disponibles. Le composant le plus complexe
est certainement la chaîne expander. Il est d’abord composé de BulkIndexExpand(idx)
qui importe, pour chaque document, la liste des termes enregistrés dans l’index
de Terrier ainsi qu’un certain nombre de statistiques et de scores associés. En-
suite un composant TermSetBuildExpand permet de regrouper l’ensemble des termes
5. La tokenisation consiste à découper un texte en diﬀérents tokens, c’est-à-dire à séparer les

mots.

6.3. IMPLÉMENTATION LOGICIELLE D’UNE CHAÎNE DE
TRAITEMENT MODULABLE (KODEX)

187

terrier_server = " localhost :8081 "

2 index_name = " index_test "

4 keb = KodexEngineBuilder ()

6 # Search

from kodex . extra import terrier

8 search = terrier . TerrierSearch ( terrier_server , index_name )

keb. add_searching ( search )

# Expander

12 idx = terrier . TerrierIndex ( terrier_server , index_name )

expander = BulkIndexExpand (idx)
14 expander |= TermSetBuildExpand (

fields_to_merge =[

(" terms_tf ", " tf_RD ", lambda x, y : x+y),
(" terms_tf ", " df_RD ", lambda x, y : x+1) ,

]

10

16

18

)

22 )

)

20 expander |= OnlineTermsFilter (

output_field =" terms_filtered "

expander |= CacheUrlExpand ()
24 keb. add_expanding ( expander )

26 # Graph Builder

from kodex . gbuilder import DocsTermsGraphBuilder

28 gbuilder = DocsTermsGraphBuilder (
terms_field =" terms_filtered ",
terms_scores_field =" terms_bm25 "

30

32 keb. add_graph_building ( gbuilder )

34 # Clustering

from kodex . clustering import WalktrapBigraph , Infomap ,

ConnectedComponents

36 keb. add_clustering ( Infomap ())

keb. add_clustering ( WalktrapBigraph ())

38 keb. add_clustering ( ConnectedComponents ())

40 # Labelling

from kodex . labelling import AllClusterTerms , BaseLineLabelling

42 keb. add_labelling ( AllClusterTerms ())

keb. add_labelling ( BaseLineLabelling ())

Listing 6.5 – Exemple de construction d’un KodexEngineBuilder

CHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION
188
présents dans les documents retournés, et d’agréger certaines statistiques. Ce com-
posant calcule pour chaque terme sa fréquence totale dans les documents retournés
("tf_RD") et le nombre de documents retournés le contenant ("df_RD"). Le compo-
sant suivant, OnlineTermsFilter(), permet de calculer des scores pour chaque couple
terme-document et d’appliquer certains ﬁltres. Enﬁn le composant CacheUrlExpand()
permet d’ajouter aux documents un champ "chache_url" correspondant à l’adresse
permettant d’accéder à une copie locale du document en question.

6.3.6 Analyse des documents hors-ligne

La construction du graphe lors d’une réponse à une requête demande une modé-
lisation des documents. C’est-à-dire, une liste de termes (éventuellement pondérés)
associée à chaque document. Cette modélisation peut être construite entièrement en
ligne à partir des données retournées par le SRI. Seulement la quantité et la qualité
des informations sont souvent limitées. Les SRI ne retournent, la plupart du temps,
que le titre et un court extrait du document. De plus, aucune statistique n’est alors
disponible sur les termes extraits, si ce n’est la fréquence « locale ».

Une autre solution consiste à utiliser le texte complet des documents. Nous utili-
sons pour cela une approche simple, consistant en trois étapes : i) analyse des textes,
c’est-à-dire en extraire des termes (des tokens), ii) calcul des fréquences des tokens
sur la collection complète et iii) sélection, pour chaque document, des meilleurs to-
kens grâce à un score calculé à partir de ces fréquences. Les deux premières étapes
doivent être faites hors-ligne, la troisième étape peut être faite tant en-ligne que
hors-ligne. Un SRI doit nécessairement eﬀectuer les deux premières étapes pour
construire un index inversé et calculer des scores pour les documents. Kodex va
donc chercher à s’appuyer le plus possible sur un SRI pour gérer ces deux étapes.
Notons que de nombreuses autres solutions seraient envisageables pour extraire
des termes, ou mots-clés des documents. Par exemple en utilisant des méthodes
d’apprentissage automatique [Boniface, 2011] ou encore des méthodes cherchant à
relier les documents aux concepts d’une ontologie [Bouidghaghen et al., 2009]. Ces
approches plus complexe de « text mining » (ou « keywords extraction ») n’ont pas été
explorées. Nous y reviendrons dans la conclusion, mais ce problème de modélisation
des documents est certainement un point clé pour améliorer le système.

6.3.6.1 Tokenisation et analyse hors-ligne des documents

L’analyse des documents par Kodex repose sur le système de composants et de
chaînes de composants présenté dans les sous-sections précédentes. Kodex analyse
les documents grâce à une chaîne de trois composants :

Reader()| Analyser()| Writer()

6.3. IMPLÉMENTATION LOGICIELLE D’UNE CHAÎNE DE
TRAITEMENT MODULABLE (KODEX)
189
À partir d’une liste d’identiﬁants ou d’adresses, le reader lit les documents depuis
des ﬁchiers ou depuis une base de données. Son rôle est de construire des KodexDoc
contenant des champs correspondant au plus près à la structure des documents bruts.
Le minimum de transformation des données est fait dans cette phase. L’analyser a
ensuite pour rôle de découper les champs de textes des documents en tokens et
de compter les fréquences de ces tokens (fréquences dans chaque document). Cet
analyser peut lui même être une chaîne de composants élémentaires. Enﬁn, le writer
enregistre les documents analysés dans des ﬁchiers, dans l’index d’un SRI, ou dans
une base de données. Il est possible de chaîner plusieurs writer pour enregistrer
diﬀérentes parties des documents sur diﬀérents supports.

Notons que les analysers ne se limitent pas forcément à une tokenisation. Toute
méthode d’analyse ou d’enrichissement des documents est envisageable. Par exemple,
il est possible d’extraire tous les mots indiqués en gras dans un texte en html.

Notons encore que cette phase d’analyse peut être faite en parallèle. La parallé-
lisation est simple car chaque document est traité indépendamment. Nous utilisons
en particulier l’outil gnu parallel 6 pour cela.

6.3.6.2 Calcul des fréquences

Certains SRI permettent d’indexer des champs de textes des documents dont le
découpage en tokens est déjà fait. Le SRI calcule alors les fréquences totales de ces
termes. Le plus simple pour cette étape est donc de se reposer sur un SRI permettant
cela. En eﬀet aucune adaptation intéressante et spéciﬁque à Kodex n’est envisageable
ici. Il s’agit simplement de compter, pour chaque terme, le nombre de documents
qui l’utilisent et sa fréquence totale dans la collection.

Pour utiliser ainsi un SRI, il suﬃt que le writer de l’étape précédente donne le

document à indexer au SRI.

6.3.6.3 Calcul de score et sélection des termes

Cette étape peut se faire en-ligne ou hors-ligne. Une approche entièrement en-
ligne pose un problème de performances. Le principal goulot d’étranglement est le
chargement en mémoire des termes associés aux n documents retournés, ces infor-
mations étant sur le disque dur. À l’inverse une sélection en-ligne à l’avantage de
permettre de choisir les termes en connaissant la requête et les autres documents
retournés. Nous nous basons donc sur une approche mixte : une première sélection
est faite hors-ligne, et une seconde est faite en ligne.

La sélection des termes (ou token) hors-ligne est faite avec la chaîne de traitement

suivante :

MinimalReader()| TermExpander()| TermScoring()| Cut()| Writer()

6. http://www.gnu.org/software/parallel/

190

CHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION
À partir d’une liste d’identiﬁants (de documents), le composant MinimalReader()
construit des KodexDoc vides ou ne contenant que le minimum d’information néces-
saire. Dans chacun de ces KodexDoc « vides », le composant TermExpander() charge
les termes et leurs statistiques associés depuis l’index du SRI.

Un score pour chaque couple document-terme est ensuite calculé par le compo-
sant TermScoring(). Ces scores sont stockés dans les KodexDoc. Le composant Cut()
permet ensuite d’appliquer un seuil pour ﬁltrer les termes en fonction de leurs scores.
Notons que plusieurs scores peuvent être calculés (par un ou diﬀérents composants)
et plusieurs seuils sont applicables sur chacun de ces scores.

Enﬁn un composant Writer() sauvegarde les termes sélectionnés (et éventuelle-
ment les scores associés) pour chaque document. Ce stockage est fait soit dans une
base de données spéciﬁque, soit en modiﬁant les données enregistrées par le SRI
utilisé à l’étape précédente.

La seconde sélection, en-ligne, se fait avec une chaîne de composants semblable
à la chaîne TermScoring()| Cut(). La diﬀérence est que ces composants peuvent alors
avoir accès à l’ensemble des documents retournés, et éventuellement à la requête.

Notons ici l’un des avantages de la structure modulable de Kodex : pour une
phase de mise au point ou d’expérimentation, il est possible d’utiliser en-ligne la
partie centrale de la chaîne de traitement prévue pour être utilisée hors-ligne. C’est-
à-dire la chaîne :

TermExpander()| TermScoring()| Cut()

Le temps de réponse pour une requête devient alors trop important pour une applica-
tion réelle. Mais cela permet, par exemple, de lancer des jeux de tests et d’évaluations
sans avoir à eﬀectuer une passe complète sur la collection et sans avoir à stocker les
sélections intermédiaires.

6.4 Évaluation sur une collection de deux millions

de documents

Nous présentons dans cette section une application de Kodex sur une collection
de 2,6 millions de pages web. Cette application a été mise en place dans le cadre
du programme de recherche européen Quaero 7 aﬁn de proposer une évaluation de
Kodex. Après avoir présenté la collection et la conﬁguration de Kodex mise en place,
nous présentons les résultats de cette évaluation.

7. http://www.quaero.org/

6.4. ÉVALUATION SUR UNE COLLECTION DE DEUX MILLIONS
DE DOCUMENTS
191
6.4.1 Collection et jeu d’évaluation

La collection utilisée est composée de 2,6 millions de pages Web issues du domaine
français (.fr) et aspirées en 2008 par le moteur de recherche Exalead 8. Chacune de
ces pages a été transcrite au format TREC [Harman, 2005], en extrayant uniquement
le texte brut (suppression de toutes les balises html), l’url et le titre.

Pour l’évaluation, un corpus de 25 sujets (topics) a été constitué. Un sujet com-
prend une requête textuelle (Title) et un texte explicitant le besoin en information
(Narrative). Ces derniers sont des besoins réels qui ont été soumis par des utilisa-
teurs d’Exalead (extraits du log de ce moteur de recherche). Le listing 6.6 donne
un exemple de topic codé en XML. Pour chacun de ces 25 topics, des jugements
de pertinence ont été recueillis en suivant une procédure similaire à celle utilisée
dans TREC [Harman, 2005]. Il a été constitué un pool de résultats issus de 144
conﬁgurations diﬀérentes du SRI Terrier [Ounis et al., 2005]. Ces conﬁgurations
ont été construites en utilisant diﬀérentes formes d’indexation, diﬀérents modèles
de recherche, et en réalisant ou non l’expansion de requêtes. Ce pool a ensuite été
évalué manuellement pour identiﬁer les documents pertinents de chaque requête.

<Topic Topic_Id ="17">

<Title >le gouffre de poudrey </ Title >
<Narrative >On souhaite un minimum d’ information sur la

localisation , ou la taille , ou les caract é ristiques du
gouffre de Poudrey . La seule comparaison de ce gouffre à un
monument (par exemple ) n’est pas pertinente .

2

4

</ Narrative >

</ Topic >

Listing 6.6 – Exemple de requête d’évaluation en XML

6.4.2 Conﬁguration de Kodex

Nous décrivons ici la conﬁguration de Kodex mise en place sur cette collection.

6.4.2.1 Collection, indexation et recherche

Nous avons utilisé le SRI Terrier [Ounis et al., 2005] pour indexer les documents
et eﬀectuer la phase de recherche. Ce SRI a l’avantage de proposer beaucoup des
modèles de recherche de l’état de l’art, et de fournir un système d’indexation pour
les documents au format TREC.

Pour les évaluations de Kodex, nous avons choisi la conﬁguration de Terrier
suivante. Les documents ont été indexés en appliquant une radicalisation (stemming)

8. http://www.exalead.fr/

CHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION
192
par l’algorithme « Snowball » de Porter [2001] pour le français. Les mots vides ont
été éliminés. Pour la recherche le modèle de pondération Okapi BM25 a été utilisé
avec des paramètres par défaut [Manning et al., 2008, chap. 11]. Le nombre de
documents retournés pour chaque requête a été ﬁxé à : nD = 100.

6.4.2.2 Modélisation des documents et construction du graphe

La modélisation des documents pour la construction du graphe a été faite à
partir des termes extraits par Terrier pour la phase de recherche. C’est-à-dire les
termes ont été extraits des documents par une tokenisation et une radicalisation
par l’algorithme de Porter [2001] pour le français. Les termes ayant une fréquence
documentaire 9 inférieure à fdf = 100 sont éliminés. Lors d’une requête, pour chaque
terme est calculé le score maximale de BM25 qu’il obtient parmi les documents
retournés. Les termes ayant un tel score inférieur à fBM25 = 5 sont éliminés. Enﬁn
le nombre de termes total du graphe est limité à nT = 1000, en utilisant ce score
de BM25 maximal pour ordonner les termes. Notons que le premier ﬁltre peut être
appliqué hors-ligne (voir sous-section 6.3.6) alors que les deux dernières sélections
sont forcément faites en-ligne.

Le graphe biparti construit peut être pondéré de diﬀérentes manières. Nous avons

évalué les pondérations entre documents et termes suivantes :
one, le graphe n’est pas pondéré, chaque lien a un poids de 1,
tf, le nombre d’occurrence du terme dans le document (tfdoc,term) est utilisée,
tﬁdf, mesure de tf-idf classique :

tf idf(doc, term) = tfdoc,term ∗ log

  N

!

dfterm

où N indique le nombre de documents dans la collection et dfterm est la fré-
quence documentaire du terme (c’est à dire le nombre de documents contenant
le terme),

tﬁdfRD, variante de la mesure de tf-idf, la partie idf est calculée sur les documents

retournés (RD) :

tf idfRD(doc, term) = tfdoc,term ∗ log

nD

dfRD(term)

 

!

où nD indique le nombre de documents retournés, et dfRD(term) le nombre de
documents retournés contenant le terme,

9. Nombre de documents contenant le terme (document frequency en anglais).

6.4. ÉVALUATION SUR UNE COLLECTION DE DEUX MILLIONS
DE DOCUMENTS
193
logfratio, score naïf :

logf ratio(doc, term) = log( tfdoc,term

T F(term)) − log( len(doc)

N

)

où T F(term) est la fréquence totale du terme dans la collection, logf ratio
est donc le ratio entre la fréquence du terme dans le document par rapport à
sa fréquence totale et la taille du document par rapport au nombre total de
documents,

bm25, score de BM25 avec paramètres par défaut, voir [Manning et al., 2008, chap.

11].

6.4.2.3 Clustering du graphe biparti

Pour cette évaluation, diﬀérentes méthodes de clustering de graphe biparti ont été
utilisées. Ces méthodes sont présentées dans le chapitre précédent (voir chapitre 5),
nous les résumons ici rapidement.

WalktrapB : adaptation de la méthode Walktrap de Pons et Latapy [2006] pour

des graphes bipartis, voir [Navarro et al., 2011] pour plus de détails,

Infomap : méthode de clustering basée sur la compression de la description de
la trajectoire d’un marcheur aléatoire [Rosvall et Bergstrom, 2008], Pour une
description plus en détail voir section 5.1.3,

fca.Infomap : méthode Infomap appliquée sur le graphe documents-concepts for-

mels, c’est la méthode présentée au chapitre précédent en section 5.3.

conﬂ.fca.Infomap, conﬂ.fca.WalktrapB : méthodes Infomap et WalktrapB ap-
pliquées sur le graphe documents-concepts formels, mais cette fois construit
après un ﬁltrage du graphe documents-mots par la méthode présentée en sec-
tion 5.4.

Pour chacune des méthodes, le post-traitement suivant est appliqué : tous les
clusters comportant moins de ndoc−min = 5 documents sont regroupés dans un seul
cluster « divers ». Cela permet d’éviter d’avoir des clusters trop petits ce qui est
souhaitable sur une application réelle, mais aussi pour l’évaluation que nous présen-
tons dans la sous-section suivante. En eﬀet il est « facile » pour un petit cluster (un
ou deux documents par exemple) d’avoir une précision très forte. Comme nous al-
lons le voir, l’évaluation choisit le meilleur cluster grâce à la précision, l’un des biais
possible de l’évaluation est donc de ne prendre en compte que des « mini »-clusters.
Regrouper ainsi ces petits clusters permet d’éviter ce biais.

CHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION

194
6.4.3 Évaluation

L’évaluation que nous présentons ici permet de mesurer dans quelle mesure un des
clusters retourné par Kodex regroupe les documents pertinents. L’hypothèse est que
si les clusters sont « corrects », ils correspondent chacun à une « interprétation » de
la requête (ou à un « sous-sens » de celle-ci) par rapport à la collection de documents.
Le sens de la requête entendu par l’utilisateur doit donc correspondre à l’un de ces
clusters.

Plus pragmatiquement, Kodex peut avoir de l’intérêt pour un utilisateur seule-
ment si cet utilisateur peut trouver plus de documents pertinents dans un cluster
bien choisi que dans la liste brute retournée par le SRI.

Une première version de cette évaluation a été présentée dans [Navarro et al.,
2011]. Nous présentons ici une évaluation plus complète, basée sur le même jeu de
données. Il faut noter que les valeurs des résultats présentés ici pour la méthode
WalktrapB diﬀèrent légèrement de celles indiquées dans ce papier. Cela est dû à
une conﬁguration diﬀérente du SRI utilisé. Nous utilisons ici la version 3 de Terrier
alors que la version 2 était utilisée pour ce premier article. De plus les termes sont
ici radicalisés alors qu’ils étaient simplement tronqués lors de la première évaluation.
Notons que les nouveaux résultats obtenus avec ces changements sont compatibles
avec les conclusions que nous tirions de cette première évaluation.

6.4.3.1 Méthode d’évaluation

Comme nous l’avons vu, cette évaluation a pour but de valider l’hypothèse sui-
vante : à partir d’un ensemble de documents restitués par un moteur de recherche
pour une requête, Kodex est capable de construire un cluster contenant un maximum
de documents pertinents. Pour ce faire, nous exploitons la collection présentée en
section 6.4.1.

Aﬁn d’évaluer l’intérêt du clustering de Kodex par rapport à une organisation
sous forme de liste de documents, nous nous basons sur la procédure MK1-k dé-
crite par Tombros et al. [2002]. Cette évaluation se base sur les deux ensembles de
documents ekodex et eSRI provenant respectivement de Kodex et du SRI utilisé. L’en-
semble ekodex contient les documents du « meilleur cluster » de Kodex (contenant m
documents) et eSRI contient les m premiers documents de la liste de Terrier. Nous
décrivons plus bas comment ce « meilleur cluster » est déterminé. Par exemple, si le
« meilleur cluster » de Kodex ekodex contient 18 documents alors ekodex est comparé
à esri qui contient les 18 documents les mieux classés par Terrier. Les ensembles
ekodex et eSRI sont alors comparables grâce aux mesures rappel (R), précision (P) et
F1 déﬁnies comme suit, pour un système s (∈ {Kodex, SRI}) et un topic q [Jardine
et van Rijsbergen, 1971].

6.4. ÉVALUATION SUR UNE COLLECTION DE DEUX MILLIONS
DE DOCUMENTS
195

R(s, q) =

Nombre de documents pertinents dans es pour q

Nombre de documents pertinents dans le top-100 du SRI pour q
P(s, q) = Nombre de documents pertinents dans es pour q

Nombre de documents dans es pour q
F1(s, q) = 2 · R(s, q) · P(s, q)
R(s, q) + P(s, q)

(6.1)

(6.2)

(6.3)

Pour identiﬁer le « meilleur » cluster (ekodex) une possibilité serait de montrer
tous les clusters à des utilisateurs pour qu’ils sélectionnent celui qui leur semble le
plus pertinent. On retiendrait alors le cluster le plus consensuel.

Pour ne pas introduire un facteur (humain) externe à l’évaluation et aﬁn de
garantir sa reproductibilité, nous avons plutôt opté pour un critère objectif de sélec-
tion de ce meilleur cluster. La mesure de précision nous est apparue la plus adaptée
car le « meilleur » cluster est celui ayant la plus forte concentration de documents
pertinents.

Pour chaque requête la procédure d’évaluation consiste donc à calculer les scores
de rappel, précision et F-score pour le cluster de meilleure précision, ainsi que sur
la liste de documents retournés par le SRI coupée à la même taille. La moyenne de
ces scores est ensuite calculée pour les 25 requêtes.

6.4.3.2 Kodex par rapport au SRI seul, et par rapport à Carrot

Le tableau 6.1 présente les résultats pour quatre méthodes de clustering diﬀé-
rentes. La signiﬁcation de chaque colonne est indiquée dans la légende. Les deux
premières lignes correspondent à la chaîne Kodex conﬁgurée comme indiqué précé-
demment, avec les méthodes de clustering Infomap et WalktrapB. Dans ces deux
cas le graphe est pondéré par le score de BM25. Les deux lignes suivantes indiquent
les résultats avec un clustering eﬀectué par le logiciel Carrot (voir section 6.2) qui
implémente deux algorithmes (k-means et lingo). Notons qu’avec Carrot la même
chaîne de traitement des documents est appliquée pour le ﬁltrage des termes, par
contre la pondération des termes n’est pas prise en compte. Le regroupement des
petits clusters dans un cluster « divers » est aussi eﬀectué lors du clustering par
Carrot.

Notons que <F> est la moyenne des F-scores des meilleurs clusters, il ne corres-

pond donc pas au F-score que l’on pourrait calculer à partir de <R> et <P>.

On observe, par exemple sur la conﬁguration Infomap que les meilleurs clusters
produits contiennent en moyenne 48% de bons documents (précision), et que cela
représente en moyenne 45% de l’ensemble des bons documents retournés par le
SRI (rappel). Sur cette conﬁguration, ces mêmes meilleurs clusters comportent en

196

CHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION

Table 6.1 – Résultats de Kodex et de Carrot par rapport à la liste ordonnée.
<|C|> indique le nombre moyen de clusters, <|es|> la taille moyenne du meilleur cluster.
<P>, <R> et <F> sont respectivement la précision, le rappel et la F-mesure moyenne du
meilleur cluster. Entre parenthèses est indiqué l’écart-type. Enﬁn +P, +R et +F indiquent
le pourcentage d’amélioration sur <P>, <R> et <F> par rapport à une liste de même taille
retournée par le SRI.

Infomap
WalktrapB
Carrot (k-means)
Carrot (lingo)

<|C|> <|es|>
3.56
4.52
9.64
11.04

29.76 0.48 (0.33) 0.45 (0.37) 0.27 (0.14) +30.5% +17.4% +24.8%
25.84 0.44 (0.28) 0.51 (0.32) 0.33 (0.17) +18.2% +30.8% +21.7%
17.60 0.51 (0.27) 0.38 (0.25) 0.34 (0.13) +20.7% +24.4% +20.4%
10.48 0.53 (0.30) 0.29 (0.24) 0.27 (0.10) +15.2% +29.5% +12.8%

<P>

<R>

<F>

+P

+R

+F

moyenne un peu moins de 30 documents. Cela représente une amélioration moyenne
de 30% pour la précision, de 17% pour le rappel et de 24% pour le F-score par
rapport aux premiers documents retournés par le SRI.

Pour les diﬀérentes mesures, les résultats comparatifs (+P, +R et +F) montrent
clairement l’amélioration apportée par rapport aux top-k documents de la liste du
SRI. Ce premier résultat valide donc l’intérêt de la méthode.

On observe que les deux algorithmes de Carrot tendent à produire plus de clus-
ters, plus petits, que Infomap et WalktrapB. Il en résulte que Carrot produit des
clusters de meilleures précisions mais avec de moins bons rappels que ceux construits
par Kodex. Dans une moindre mesure, on observe aussi que Infomap tend à avoir
de meilleures précisions, alors que WalktrapB privilégie le rappel.

Notons que la précision et le rappel moyen de ces quatre systèmes sont aussi

indiqués sur la ﬁgure 6.3.

6.4.3.3 Comparaison de la méthode de pondération du graphe

Nous comparons ici les résultats des méthodes de clustering Infomap et Walk-
trapB. Les résultats sont aussi indiqués dans le tableau 6.2 et les performances de
chaque conﬁguration, en termes de précision et rappel moyen, sont représentées de
manière plus lisible sur la ﬁgure 6.3. Chaque point indique la précision (ordonnée)
et le rappel moyen (abscisse) d’une conﬁguration donnée. Les carrés (bleus) cor-
respondent à la méthode de clustering Infomap (avec diﬀérentes pondérations), les
losanges (rouges) la méthode WalktrapB (là aussi avec diﬀérentes pondérations) et
les triangles (violets) indiquent que le clustering est fait avec Carrot. Enﬁn les croix
noires reliées par une ligne indiquent les performances de Terrier sur les k premiers
documents. A chaque croix un document de plus est pris en compte, et les croix
correspondant à une liste de 20 puis de 30 documents sont indiquées.

La remarque faite précédemment est clairement observable sur la ﬁgure 6.3 :
lingo et k-means privilégient la précision par rapport au rappel. Et cela plus que ne
le fait Infomap. Infomap à son tour est dans le même cas par rapport à WalktrapB.

6.4. ÉVALUATION SUR UNE COLLECTION DE DEUX MILLIONS
DE DOCUMENTS
197

Figure 6.3 – Précision et rappel moyen du meilleur cluster pour diﬀérentes pon-
dérations du graphe. I indique l’utilisation de Infomap, W celle de WaltrapB. La ligne
noire indique les performances de la liste ordonnée du SRI, chaque point correspondant à
la prise en compte d’un nouveau document, les points correspondant à une liste de 20 et 30
documents sont indiqués.

198

CHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION

Table 6.2 – Resultats de Kodex en fonction de la pondération du graphe <|C|>
indique le nombre moyen de clusters, <|es|> la taille moyenne du meilleur cluster. <P>,
<R> et <F> sont respectivement la précision, le rappel et la F-mesure moyenne du meilleur
cluster. Entre parenthèses est indiqué l’écart-type.

Infomap

WalktrapB

|C| <|e_s|>
31.24
3.88
one
26.60
3.40
tf
27.92
3.28
tﬁdf
77.60
tﬁdfRD
1.88
logfratio 3.64
32.44
29.76
3.56
BM25
26.40
4.64
one
64.20
3.04
tf
tﬁdf
4.96
20.92
17.96
tﬁdfRD
4.40
27.40
logfratio 4.60
BM25
4.52
25.84

<P>

<R>

<F>

0.48 (0.33) 0.49 (0.39) 0.26 (0.13)
0.51 (0.34) 0.43 (0.37) 0.25 (0.14)
0.49 (0.31) 0.42 (0.37) 0.26 (0.14)
0.29 (0.25) 0.86 (0.32) 0.31 (0.21)
0.47 (0.33) 0.48 (0.39) 0.26 (0.14)
0.48 (0.33) 0.45 (0.37) 0.27 (0.14)
0.44 (0.26) 0.53 (0.30) 0.35 (0.18)
0.33 (0.25) 0.81 (0.30) 0.37 (0.23)
0.49 (0.30) 0.47 (0.26) 0.36 (0.18)
0.45 (0.30) 0.33 (0.26) 0.25 (0.13)
0.43 (0.25) 0.55 (0.30) 0.36 (0.17)
0.44 (0.28) 0.51 (0.32) 0.33 (0.17)

Concernant le choix des pondérations, on observe tout d’abord que la mesure
tf idfRD ne fonctionne ni pour walktrap ni pour Infomap. Avec Infomap cela tend
à créer un seul gros cluster (77,6 sommets en moyenne, sur 100) et WalktrapB les
performances sont clairement moins bonnes qu’avec les autres pondérations.

On remarque aussi que la pondération tf se comporte sur WalktrapB comme
tf idfRD sur Infomap : un cluster regroupant la quasi totalité des documents est
construit. Par contre sur Infomap, tf produit des résultats comparables aux autres
pondérations.

Notons ensuite que pour Walktrap la pondération tf idf est remarquable dans
le sens où elle amène une meilleure précision et un moins bon rappel que les trois
autres pondérations (one, logf ratio et bm25).

Mis à part ces diﬀérences, les mesures observées ici ne permettent pas de mettre
en évidence des comportements diﬀérents entre ces pondérations sur les deux mé-
thodes de clustering utilisées. Le plus surprenant est certainement que la pondération
one, c’est-à-dire une absence de pondération, produit un résultat comparable par
exemple à la mesure BM25. Cela s’explique très certainement à cause du ﬁltrage
des termes eﬀectué en amont (voir sous-section 6.4.2). Seul les termes ayant des
scores élevés ont été gardés et donc la pondération utilisée lors du clustering peut
diﬃcilement améliorer les résultats. Nous avons vu, par contre, que la pondération
peut se retrouver contre-productive. Enﬁn Infomap et WalktrapB ne se comportent
pas de la même manière avec ces pondérations.

6.4. ÉVALUATION SUR UNE COLLECTION DE DEUX MILLIONS
DE DOCUMENTS
199

Table 6.3 – Résultat de l’utilisation de l’AFC en amont du clustering, La pondération
du graphe est faite par le score de BM25. <|C|> indique le nombre moyen de clusters, <|es|>
la taille moyenne du meilleur cluster. <P>, <R> et <F> sont respectivement la précision,
le rappel et la F-mesure moyenne du meilleur cluster. Entre parenthèses est indiqué l’écart-
type.

|C| <|e_s|>
26.48
one
4.32
28.36
min 4.56
30.36
max 4.12
sum 3.44
39.08
26.72
avg
4.40
97.16
prod 1.08
15.52
one
5.56
min 5.56
14.92
16.72
max 5.56
27.00
sum 4.68
17.16
avg
5.72
prod 5.92
16.96
13.32
one
6.64
15.80
min 6.84
avg
6.96
15.60

fca.Infomap

conﬂ.fca.Infomap

conﬂ.fca.WalktrapB

<P>

<R>

<F>

0.50 (0.34) 0.49 (0.34) 0.31 (0.16)
0.51 (0.34) 0.54 (0.34) 0.33 (0.17)
0.47 (0.32) 0.54 (0.35) 0.32 (0.15)
0.44 (0.34) 0.59 (0.37) 0.30 (0.15)
0.52 (0.34) 0.50 (0.33) 0.31 (0.15)
0.24 (0.18) 1.00 (0.00) 0.35 (0.22)
0.54 (0.33) 0.37 (0.32) 0.29 (0.15)
0.55 (0.31) 0.36 (0.32) 0.29 (0.14)
0.52 (0.32) 0.38 (0.33) 0.28 (0.15)
0.49 (0.35) 0.45 (0.38) 0.28 (0.17)
0.56 (0.33) 0.38 (0.33) 0.28 (0.15)
0.52 (0.32) 0.40 (0.33) 0.30 (0.16)
0.53 (0.28) 0.37 (0.26) 0.32 (0.14)
0.51 (0.31) 0.39 (0.27) 0.32 (0.15)
0.51 (0.29) 0.41 (0.29) 0.33 (0.17)

6.4.3.4 Passage par l’AFC et la conﬂuence

Nous évaluons ici les méthodes de clustering passant par le graphe objets-concepts
(ici documents-concepts) introduites au chapitre précédent (sections 5.3 et 5.4). La
ﬁgure 6.4 présente de manière identique à la ﬁgure 6.3 les performances de diﬀé-
rentes méthodes utilisant l’Analyse Formelle de Concepts (AFC). Ces résultats sont
aussi indiqués dans le tableau 6.3. Pour toutes les méthodes présentées ici, le graphe
documents-termes est pondéré par BM25. Les symboles pleins indiquent l’usage des
méthodes de base, sans passer par l’AFC (WalktrapB et Infomap). Les symboles
grisés correspondent à un clustering fait sur le graphe documents-concepts construit
directement depuis le graphe documents-termes, enﬁn les symboles blancs corres-
pondent à un clustering fait sur un graphe documents-concepts construit après un
ﬁltrage par la conﬂuence du graphe documents-termes. Ce ﬁltrage consiste à sup-
primer tous les liens ayant une conﬂuence à temps t = 3 inférieure à 0.5.

On observe tout d’abord que le passage par le graphe des concepts (sans ﬁltrage
préalable) permet d’améliorer assez clairement les résultats d’Infomap. En eﬀet, par
rapport à Infomap.bm25 (tableau 6.2), le rappel moyen et la précision moyenne
augmentent pour les méthodes fca.Infomap avec les pondérations avg, one et min,
Le passage par le graphe objets-concepts semble donc être intéressant ici.

Ce sont les méthodes d’agrégation des poids min et avg qui sont les plus perfor-
mantes. Le méthode prod ne fonctionne pas : tous les sommets se retrouvant dans

200

CHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION

Figure 6.4 – Précision et rappel moyen du meilleur cluster, par un clustering
du graphe documents-concepts. Pour améliorer la lisibilité, les noms des méthodes ont
été abrégés : I pour Infomap, f-I pour fca.Infomap, cf-Infomap pour conﬂ.fca.Infomap, de
même W indique WalktrapB. La ligne noire indique les performances de la liste ordonnée du
SRI, chaque point correspondant à la prise en compte d’un nouveau document, les points
correspondant à une liste de 20 et 30 documents sont indiqués.

6.4. ÉVALUATION SUR UNE COLLECTION DE DEUX MILLIONS
DE DOCUMENTS
201
un seul cluster (|C| = 1.08).

Notons aussi que ce passage par l’AFC engendre davantage de clusters plus
petits (4.56 clusters en moyenne pour fca.Infomap.bm25.min contre 3.56 pour Info-
map.bm25).

Le ﬁltrage du graphe par la conﬂuence, comme nous l’avons vu au chapitre
précédent, permet de réduire considérablement le nombre de concepts (voir le ta-
bleau 5.6 en section 5.4.3). Nous voyons ici que ce ﬁltrage du graphe permet d’amé-
liorer clairement la précision moyenne du meilleur cluster, mais au prix d’une dé-
gradation du rappel. Le gain n’est donc pas évident. Notons tout de même que
conﬂ.fca.Infomap.avg donne un rappel semblable à la méthode k-means de Carrot
mais avec une précision clairement supérieure. Nous observons ici que ce ﬁltrage
amène Infomap à construire plus de clusters, plus petits (par exemple 5.56 clusters
en moyenne pour conﬂ.fca.Infomap.min contre 4.56 pour fca.Infomap.min). Le ﬁl-
trage coupe des liens « faibles » dans le graphe de départ, cela permet visiblement
de mieux séparer certains groupes, et d’avoir ainsi davantage de clusters.

6.4.4 Conclusions et perspectives

Nous avons présenté dans cette section une évaluation de Kodex sur une collection
de 2,6 millions de pages web. Nous avons pu montrer la capacité de l’approche à
regrouper dans un cluster plus de documents pertinents qu’il y en a dans la liste
ordonnée de même taille. Cette évaluation ne constitue toutefois qu’une première
étape, elle présente en eﬀet plusieurs limites.

Tout d’abord, cette évaluation se base sur l’hypothèse selon laquelle les docu-
ments pertinents doivent être regroupés dans un seul cluster. Cette hypothèse a
l’avantage de permettre une évaluation en ne connaissant « que » la liste des docu-
ments pertinents pour un certain nombre de requêtes. Seulement il est envisageable
qu’un clustering soit correct et informatif alors que les documents pertinents sont
répartis dans plusieurs clusters. Typiquement dans le cas où l’utilisateur cherche à
faire une recherche « en largeur » et donc cherche des documents (de qualité) dans
diﬀérents clusters. Il est aussi possible que le découpage proposé par le système soit
pertinent sans qu’il ne corresponde au découpage « attendu » par l’utilisateur. Ima-
ginons par exemple une recherche sur un médicament, l’utilisateur cherchant des
informations sur les eﬀets secondaires. Un clustering classant les résultats en fonc-
tion du type de publication (forums médicaux, sites gouvernementaux, publications
scientiﬁques, sites de laboratoires, etc.) peut être intéressant, mais des documents
pertinents pour l’utilisateur peuvent alors être présents dans chacun de ces groupes.
Cette évaluation présente aussi une limitation plus pragmatique. En eﬀet nous
mesurons à quel point un cluster regroupe bien un nombre important de documents
pertinents, mais cela ne nous dit rien sur les autres clusters. Est-ce que ces autres
clusters sont pertinents ?

202

CHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION
Le problème soulevé ici est que cette évaluation est indirecte par rapport à l’ob-
jectif visé par le système. Une évaluation plus directe consisterait « simplement » à
vériﬁer pour un ensemble de documents donnés si les groupes construits automati-
quement forment un découpage intelligible.

Un autre défaut de cette évaluation est dû à la moyenne faite sur les 25 requêtes.
Nous avons pu observer que l’écart-type, associé à ces moyennes, est assez élevé.
Cela indique que les résultats varient beaucoup entre les diﬀérentes requêtes. Il est
alors naturel de se demander si toutes les conﬁgurations évaluées se comportent de
la même manière sur les diﬀérentes requêtes. Indépendamment de la performance
moyenne, est-ce que certaines conﬁgurations ne sont pas plus eﬃcaces pour certains
types de requêtes ? Le problème étant ensuite de pouvoir caractériser ces diﬀérents
« types » de requêtes.

Enﬁn un des enjeux majeurs d’un tel système de classiﬁcation automatique de
résultats est l’étiquetage des clusters. Il faut en eﬀet que la signiﬁcation des groupes
puisse être rapidement identiﬁée par l’utilisateur. Un travail, non présenté ici, a été
entamé 10 sur ce point dans Kodex par Benoit Gaillard. La principale diﬃculté étant
que cette évaluation ne permet pas d’évaluer la qualité d’un étiquetage des clusters.

6.5 Deux applications de Kodex

Nous présentons ici deux implémentations de Kodex qui ont été mises en place 11
en utilisant les interfaces de programmation (API) des moteurs de recherche du
journal Guardian et de la base de publication DBLP.

Pour chacune de ces applications nous présentons la chaîne de traitement mise
en place ainsi qu’une capture d’écran donnant une idée des résultats. Pour le mo-
ment, aucune évaluation n’a été construite sur ces applications. Notons que ces deux
applications doivent être considérées comme des preuves de concept. Leur intérêt
principal est de montrer le potentiel de Kodex sur une application réelle.

6.5.1 Application sur l’API de recherche du Guardian

Le site internet du journal anglais le Guardian propose une interface de dévelop-
pement (API) permettant d’interroger le moteur de recherche du journal 12. Nous
avons mis en place une application Kodex en utilisant cette API.

Pour cela nous avons développé un composant Kodex qui interroge l’API du
Guardian et transforme les résultats obtenus en objets KodexDoc (voir section 6.3).
Ce composant exploite quelques options disponibles sur cette API (la méthode de
10. Travail non encore publié.
11. Ces applications sont accessibles à l’adresse : http://www.irit.fr/kodex.
12. http://www.guardian.co.uk/open-platform/content-api-content-search-reference-guide

6.5. DEUX APPLICATIONS DE KODEX

203

Figure 6.5 – Exemple d’application de Kodex par dessus le Guardian.
Pour la requête « euro », On observe que deux clusters sont crées : un premier en
rapport avec la monnaie, un second avec le championnat de football.

eurosearchSearchingGuardian_searchClusteringwalktrap_bigrap...NewsKodex over some newspapers20 results for "euro": 2 clusters in 0.46 sec1 TalkSport owner UTV Media looks to recovery after riding out ad slump  April 14, 2013: TalkSport owner UTV Media looks to recovery after riding out ad slumphttp://www.guardian.co.uk/media/media-blog/2013/apr/14/talksport-owner-utv-media2 Leading German economist calls for dissolution of eurozone to save EU  April 14, 2013: Leading German economist calls for dissolution of eurozone to save EUhttp://www.guardian.co.uk/world/2013/apr/14/german-economist-eurozone-eu3 How Do We Fix This Mess? by Robert Peston – review  April 14, 2013: How Do We Fix This Mess? by Robert Peston – reviewhttp://www.guardian.co.uk/books/2013/apr/14/robert-peston-fix-mess-review4 Drive to get more women on the board seems to be Petering out  April 13, 2013: Drive to get more women on the board seems to be Petering outhttp://www.guardian.co.uk/business/2013/apr/14/slow-progress-to-more-women-in-boardroom5 IMF frets on sidelines while global economic divides widen  April 13, 2013: IMF frets on sidelines while global economic divides widenhttp://www.guardian.co.uk/business/2013/apr/14/imf-frets-as-global-economic-divides-widen6 British widow: 'I face ruin from Cyprus crisis'  April 13, 2013: British widow: 'I face ruin from Cyprus crisis'http://www.guardian.co.uk/world/2013/apr/14/briton-cyprus-financial-ruin7 Why David Cameron won't confront Ukip | Nick Cohen  April 13, 2013: Why David Cameron won't confront Ukiphttp://www.guardian.co.uk/commentisfree/2013/apr/14/cameron-ukip-nigel-farage8 Uefa's racism rhetoric alone won't be enough to help Kick It Out | Daniel Taylor  April 13, 2013: Uefa's racism rhetoric alone won't be enough to help Kick It Outhttp://www.guardian.co.uk/football/blog/2013/apr/13/uefa-racism-kick-it-out9 Simon Cowell banks on Britain's Got Talent in face-off with The Voice  April 12, 2013: Simon Cowell banks on Britain's Got Talent in face-off with The Voicehttp://www.guardian.co.uk/media/2013/apr/13/simon-cowell-britains-got-talent10 The Cypriot 'poison' is spreading  April 12, 2013: The Cypriot 'poison' is spreadinghttp://www.guardian.co.uk/business/nils-pratley-on-finance/2013/apr/12/cyprus-eurozone-bp-hbos-lord-green11 Cyprus sell-off fears send gold price tumbling  April 12, 2013: Cyprus sell-off fears send gold price tumblinghttp://www.guardian.co.uk/business/2013/apr/12/gold-selloff-cyprus-eurozone-crisis12 Eurozone crisis live: Cyprus tops agenda as finance ministers meet  April 12, 2013: Eurozone crisis live: Cyprus tops agenda as finance ministers meethttp://www.guardian.co.uk/business/2013/apr/12/eurozone-crisis-live-cameron-merkel-eurogroup13 Tottenham Hotspur v Basel – as it happened | Jacob Steinberg  April 11, 2013: Tottenham Hotspur v Basel – as it happenedhttp://www.guardian.co.uk/football/2013/apr/11/basel-tottenham-europa-league-live14 IMF warns over rock-bottom interest rates  April 11, 2013: IMF warns over rock-bottom interest rateshttp://www.guardian.co.uk/business/2013/apr/11/imf-warns-interest-rates15 Bitcoin exchange halts trades of digital currency after drop in value  April 11, 2013: Bitcoin exchange halts trades of digital currency after drop in valuehttp://www.guardian.co.uk/technology/2013/apr/11/bitcoin-exchange-halts-trade-value16 Eurozone crisis: Alarm as Cyprus rescue bill swells to €23bn - as it happened  April 11, 2013: Eurozone crisis: Alarm as Cyprus rescue bill swells to €23bn - as ithappenedhttp://www.guardian.co.uk/business/2013/apr/11/eurozone-crisis-cyprus-bailout-dsa-eurogroup17 Cyprus runs out of options after costly, bungled bailoutApril 11, 2013: Cyprus runs out of options after costly, bungled bailouthttp://www.guardian.co.uk/business/economics-blog/2013/apr/11/cyprus-bailout-bungled-costly18 Advertising budgets drop for fifth year in a rowApril 11, 2013: Advertising budgets drop for fifth year in a rowhttp://www.guardian.co.uk/media/2013/apr/11/advertising-budgets-drop-fifth-year19 Bitcoin hits new high before losing $160 in value in one dayApril 10, 2013: Bitcoin hits new high before losing $160 in value in one dayhttp://www.guardian.co.uk/technology/2013/apr/10/bitcoin-new-high-losing-16020 Andy Murray set for Great Britain's Davis Cup tie against CroatiaApril 10, 2013: Andy Murray set for Great Britain's Davis Cup tie against Croatiahttp://www.guardian.co.uk/sport/2013/apr/10/andy-murray-davis-cup-croatiaBusiness, Eurozone crisis,World news, Economics,Cyprus, Banking, EuroSport, Football, Kick ItOut, The FA, Uefa,Basel, TottenhamHotspurCHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION
204
classement des résultats, et le nombre de résultats). Le résultat de la recherche,
retourné par le web-service du Guardian, indique pour chaque document, en plus
des informations classiques (titre, résumé, auteur, date, url, etc.), une liste de mots-
clés apposée sur chaque document par les journalistes ou la rédaction. Ce sont ces
mots-clés qui sont utilisés pour construire le graphe biparti. L’application laisse
ensuite le choix entre diﬀérentes méthodes de clustering. L’étiquetage des clusters
peut être fait de diﬀérentes manières, dans tous les cas ce sont ces mots-clés qui
sont utilisés. Une méthode simple, et assez eﬃcace pour cette application, consiste
à utiliser tous les mots-clés regroupés avec les documents dans les clusters.

La ﬁgure 6.5 présente une copie d’écran réalisée pour la requête « euro ». Une
liste de 20 documents est présentée (coupée sur la ﬁgure), et deux clusters sont indi-
qués sur la gauche. On observe que ces deux clusters correspondent à deux emplois
d’« euro » : la coupe d’europe de football et la monnaie européenne. L’appartenance
des documents aux clusters est indiquée par des pastilles disposées après le titre de
chaque document.

Plusieurs améliorations sont envisageables. La première est de mettre en place
une évaluation aﬁn de pouvoir mesurer objectivement comment des modiﬁcations
de la chaîne de traitement améliore (ou dégrade) les résultats.

Il serait possible d’utiliser les titres et les extraits des articles pour ne pas mo-
déliser les documents uniquement avec les mots-clés. Une pondération donnant plus
d’importance aux mots-clés serait certainement utile.

Une autre amélioration importante pourrait résider dans une modiﬁcation de la
présentation des résultats. En l’état, par défaut, les 20 articles les plus récents répon-
dant à la requête sont retournés et clusterisés. Il serait possible de demander à l’API
du guardian plus de documents, de construire les clusters et ensuite de n’aﬃcher
que 20 documents en les prenant parmi chaque cluster. L’utilisateur n’aurait donc
toujours que 20 documents listés mais des documents plus diversiﬁés. Le choix d’un
cluster (par un clic) permettant d’aﬃcher plus d’articles appartenant à ce cluster.

6.5.2 Application sur l’API de recherche de DBLP

Une démonstration de Kodex a été mise en place en utilisant l’interface de dé-
veloppement (API) proposée par la base de publications scientiﬁques DBLP 13. Ce
web-service retourne, pour une requête, une liste d’articles scientiﬁques correspon-
dant. Pour chaque article diverses informations sont disponibles : titre, date, liste
des auteurs, . . .

Un composant Kodex de recherche interroge donc cette API, et transforme le
résultat en une liste de KodexDoc. Dans cette première version, le graphe est construit
en utilisant les listes d’auteurs. Chaque document est relié à ses auteurs, ainsi un
13. http://www.dblp.org/search/

6.5. DEUX APPLICATIONS DE KODEX

205

Figure 6.6 – Exemple d’application de Kodex par dessus DBLP

ce:author:paul_erdös:*searchSearchingDBLP_searchClusteringinfomapDBLPKodex over DBLP100 results for "ce:author:paul_erdös:*" : 20 clusters in 0.30 sec1 On some applications of graph theory, I. (2006)  Paul Erdös,  Vera T. Sós,  P. Turán,  A. Meirhttp://dx.doi.org/10.1016/j.disc.2006.03.006(bibtex on dblp.org) 2 Extremal problems among subsets of a set. (2006)  Paul Erdös,  Daniel J. Kleitmanhttp://dx.doi.org/10.1016/j.disc.2006.03.013(bibtex on dblp.org) 3 On the equality of the partial Grundy and upper ochromatic numbers ofgraphs. (2003)  Paul Erdös,  Geert C. E. Prins,  Stephen T. Hedetniemi,  Renu Laskarhttp://dx.doi.org/10.1016/S0012-365X(03)00184-5  (bibtex on dblp.org) 4 On large intersecting subfamilies of uniform setfamilies. (2003)  Paul Erdös,  Richard A. Duke,  Vojtech Rödlhttp://dx.doi.org/10.1002/rsa.10098  (bibtex on dblp.org) 5 Random induced graphs. (2002)  Béla Bollobás,  Paul Erdös,  Ralph J. Faudree,  Richard H. Schelp,  Cecil C. Rousseauhttp://dx.doi.org/10.1016/S0012-365X(01)00345-4  (bibtex on dblp.org) 6 A Ramsey-type problem and the Turán numbers. (2002)  M. Molloy,  Paul Erdös,  David S. Gunderson,  Noga Alonhttp://dx.doi.org/10.1002/jgt.10032  (bibtex on dblp.org) 7 Edge disjoint monochromatic triangles in 2-colored graphs. (2001)  Jenö Lehel,  Paul Erdös,  Ralph J. Faudree,  Ronald J. Gould,  Michael S. Jacobsonhttp://dx.doi.org/10.1016/S0012-365X(00)00312-5  (bibtex on dblp.org) 8 Extremal graphs for weights. (1999)  Béla Bollobás,  Paul Erdös,  Amites Sarkarhttp://dx.doi.org/10.1016/S0012-365X(98)00320-3  (bibtex on dblp.org) 9 Graphs of diameter two with no 4-circuits. (1999)  John Adrian Bondy,  Paul Erdös,  Siemion Fajtlowiczhttp://dx.doi.org/10.1016/S0012-365X(98)00321-5  (bibtex on dblp.org) 10 Sur les ensembles représentés par les partitions d'un entiern<sup>1</sup>. (1999)  Jean-Louis Nicolas,  Paul Erdös,  Marc Deléglisehttp://dx.doi.org/10.1016/S0012-365X(98)00330-6  (bibtex on dblp.org) 11 On a question about sum-free sequences. (1999)  Giuseppe Melfi,  Paul Erdös,  Jean-Marc Deshouillershttp://dx.doi.org/10.1016/S0012-365X(98)00322-7(bibtex on dblp.org) 12 The number of cycle lengths in graphs of given minimum degree and girth. (1999)  Richard H. Schelp,  Paul Erdös,  Ralph J. Faudree,  Cecil C. Rousseauhttp://dx.doi.org/10.1016/S0012-365X(98)00324-0(bibtex on dblp.org) 13 Induced subgraphs of given sizes. (1999)  Bruce Rothschild,  Paul Erdös,  Vera T. Sós,  Zoltán Füredihttp://dx.doi.org/10.1016/S0012-365X(98)00387-2(bibtex on dblp.org) 14 Split and balanced colorings of complete graphs. (1999)  Paul Erdös,  András Gyárfáshttp://dx.doi.org/10.1016/S0012-365X(98)00323-9(bibtex on dblp.org) 15 On the angular distribution of Gaussian integers with fixed norm. (1999)  Paul Erdös,  Richard Roxby Hallhttp://dx.doi.org/10.1016/S0012-365X(98)00329-X  (bibtex on dblp.org) 16 Popular distances in 3-space. (1999)  Paul Erdös,  Gergely Harcos,  János Pachhttp://dx.doi.org/10.1016/S0012-365X(98)00328-8  (bibtex on dblp.org) 17 Prime power divisors of binomial coefficients. (1999)  Paul Erdös,  Grigori Kolesnikhttp://dx.doi.org/10.1016/S0012-365X(98)00326-4  (bibtex on dblp.org) 18 Greedy algorithm, arithmetic progressions, subset sums and divisibility. (1999)  Paul Erdös,  András Sárközy,  Vsevolod F. Lev,  Csaba Sándor,  Gerard Rauzyhttp://dx.doi.org/10.1016/S0012-365X(98)00385-9  (bibtex on dblp.org) 19 Subsets of an interval whose product is a power. (1999)  Paul Erdös,  J. L. Selfridge,  Janice L. Malouf,  Esther Szekereshttp://dx.doi.org/10.1016/S0012-365X(98)00332-X  (bibtex on dblp.org) 20 On arithmetic properties of integers with missing digits II: Prime factors. (1999)  Christian Mauduit,  Paul Erdös,  András Sárközyhttp://dx.doi.org/10.1016/S0012-365X(98)00331-8  (bibtex on dblp.org) 21 On the orders of directly indecomposable groups. (1999)  Paul Erdös,  Péter P. Pálfyhttp://dx.doi.org/10.1016/S0012-365X(98)00325-2(bibtex on dblp.org) 22 Ensembles de multiples de suites finies. (1999)  Paul Erdös,  Gerald Tenenbaumhttp://dx.doi.org/10.1016/S0012-365X(98)00327-6  (bibtex on dblp.org) 23 Finding Large p-Colored Diameter Two Subgraphs. (1999)  Paul Erdös,  Tom Fowlerhttp://dx.doi.org/10.1007/s003730050029  (bibtex on dblp.org) 24 Graphs of Extremal Weights. (1998)  Béla Bollobás,  Paul Erdös  (bibtex on dblp.org) 25 How to decrease the diameter of triangle-free graphs. (1998)  Paul Erdös,  Miklós Ruszinkó,  András Gyárfáshttp://dx.doi.org/10.1007/s004930050035  (bibtex on dblp.org) 26 A Variant of the Classical Ramsey Problem. (1997)  Paul Erdös,  András Gyárfáshttp://dx.doi.org/10.1007/BF01195000  (bibtex on dblp.org) 27 Minimum Planar Sets with Maximum Equidistance Counts. (1997)  Paul Erdös,  Peter C. Fishburnhttp://dx.doi.org/10.1016/0925-7721(95)00050-X  (bibtex on dblp.org) 28 Some recent problems and results in graph theory. (1997)  Paul Erdöshttp://dx.doi.org/10.1016/S0012-365X(96)00044-1  (bibtex on dblp.org) 29 Distinct distances in finite planar sets. (1997)  Paul Erdös,  Peter C. Fishburnhttp://dx.doi.org/10.1016/S0012-365X(96)00145-8  (bibtex on dblp.org) 30 Some old and new problems in various branches of combinatorics. (1997)  Paul Erdöshttp://dx.doi.org/10.1016/S0012-365X(96)00173-2  (bibtex on dblp.org) 31 Covering a graph by complete bipartite graphs. (1997)  Paul Erdös,  László Pyberhttp://dx.doi.org/10.1016/S0012-365X(96)00124-0  (bibtex on dblp.org) 32 The size of the largest bipartite subgraphs. (1997)  Paul Erdös,  András Gyárfás,  Yoshiharu Kohayakawahttp://dx.doi.org/10.1016/S0012-365X(97)00004-6  (bibtex on dblp.org) 33 On cycles in the coprime graph of integers. (1997)  Paul Erdös,  Gábor N. Sárközyhttp://www.combinatorics.org/Volume_4/Abstracts/v4i2r08ab.html  (bibtex on dblp.org) 34 Intersection Statements for Systems of Sets. (1997)  A. G. Meyer,  Paul Erdös,  Alexandr V. Kostochka,  David S. Gunderson,  Walter A.Deuberhttp://dx.doi.org/10.1006/jcta.1997.2778(bibtex on dblp.org) 35 New Ramsey Bounds from Cyclic Graphs of Prime Order. (1997)  Paul Erdös,  Neil J. Calkin,  Craig A. Toveyhttp://dx.doi.org/10.1137/S0895480196298378  (bibtex on dblp.org) 36 Covering and independence in triangle structures. (1996)  Zsolt Tuza,  Paul Erdös,  Tibor Gallaihttp://dx.doi.org/10.1016/0012-365X(95)00178-Y  (bibtex on dblp.org) 37 Maximum planar sets that determine k distances. (1996)  Paul Erdös,  Peter C. Fishburnhttp://dx.doi.org/10.1016/0012-365X(95)00153-N  (bibtex on dblp.org) 38 Graphs in which each C<sub>4</sub> spans K<sub>4</sub>. (1996)  Tomasz Luczak,  Paul Erdös,  András Gyárfáshttp://dx.doi.org/10.1016/0012-365X(94)00353-K  (bibtex on dblp.org) 39 Sizes of graphs with induced subgraphs of large maximum degree. (1996)  Talmage James Reid,  Paul Erdös,  William Staton,  Richard H. Schelphttp://dx.doi.org/10.1016/0012-365X(95)00008-K  (bibtex on dblp.org) 40 Ramsey-remainder. (1996)  Zsolt Tuza,  Paul Erdös,  Pavel Valtrhttp://dx.doi.org/10.1006/eujc.1996.0045  (bibtex on dblp.org) 41 Proof of a Conjecture of Bollob's on Nested Cycles. (1996)  Guantao Chen,  Paul Erdös,  William Statonhttp://dx.doi.org/10.1006/jctb.1996.0005  (bibtex on dblp.org) 42 d-complete sequences of integers. (1996)  Paul Erdös,  Mordechai Lewinhttp://dx.doi.org/10.1090/S0025-5718-96-00707-7  (bibtex on dblp.org) 43 Two Combinatorial Problems in the Plane. (1995)  Paul Erdös,  George B. Purdyhttp://dx.doi.org/10.1007/BF02574054  (bibtex on dblp.org) 44 Multiplicities of Interpoint Distances in Finite Planar Sets. (1995)  Paul Erdös,  Peter C. Fishburnhttp://dx.doi.org/10.1016/0166-218X(94)00046-G  (bibtex on dblp.org) 45 Intervertex Distances in Convex Polygons. (1995)  Paul Erdös,  Peter C. Fishburnhttp://dx.doi.org/10.1016/0166-218X(94)00047-H  (bibtex on dblp.org) 46 Monochromatic and zero-sum sets of nondecreasing diameter. (1995)  Paul Erdös,  Arie Bialostocki,  Hanno Lefmannhttp://dx.doi.org/10.1016/0012-365X(93)E0148-W  (bibtex on dblp.org) 47 Degree sequence and independence in K(4)-free graphs. (1995)  Talmage James Reid,  Paul Erdös,  Ralph J. Faudree,  William Staton,  Richard H. Schelphttp://dx.doi.org/10.1016/0012-365X(93)E0226-T  (bibtex on dblp.org) 48 Extremal Graphs for Intersecting Triangles. (1995)  Ronald J. Gould,  Paul Erdös,  Zoltán Füredi,  David S. Gundersonhttp://dx.doi.org/10.1006/jctb.1995.1026  (bibtex on dblp.org) 49 On the Size of a Random Maximal Graph. (1995)  Stephen Suen,  Peter Winkler,  Paul Erdöshttp://dx.doi.org/10.1002/rsa.3240060217(bibtex on dblp.org) 50 Covering of r-Graphs by Complete r-Partite Subgraphs. (1995)  Paul Erdös,  Vojtech Rödlhttp://dx.doi.org/10.1002/rsa.3240060218(bibtex on dblp.org) 51 Crossing Families. (1994)  János Pach,  Boris Aronov,  Daniel J. Kleitman,  Wayne Goddard,  Paul Erdös,  Leonard J.Schulman,  Michael Klugermanhttp://dx.doi.org/10.1007/BF01215345  (bibtex on dblp.org) 52 Independet Transversals in Sparse Partite Hypergraphs. (1994)  Tomasz Luczak,  Paul Erdös,  András Gyárfáshttp://dx.doi.org/10.1017/S0963548300001206  (bibtex on dblp.org) 53 Turán-Ramsey Theorems and K<sup>p</sup>-Independence Numbers. (1994)  Paul Erdös,  Miklós Simonovits,  Vera T. Sós,  András Hajnal,  Endre Szemerédihttp://dx.doi.org/10.1017/S0963548300001218  (bibtex on dblp.org) 54 A Postscript on Distances in Convex n-Gons. (1994)  Paul Erdös,  Peter C. Fishburnhttp://dx.doi.org/10.1007/BF02573998  (bibtex on dblp.org) 55 Problems and results in discrete mathematics. (1994)  Paul Erdöshttp://dx.doi.org/10.1016/0012-365X(94)00107-T  (bibtex on dblp.org) 56 On additive properties of general sequences. (1994)  Paul Erdös,  Vera T. Sós,  András Sárközyhttp://dx.doi.org/10.1016/0012-365X(94)00108-U  (bibtex on dblp.org) 57 Extremal problems and generalized degrees. (1994)  Paul Erdös,  Ralph J. Faudree,  Cecil C. Rousseauhttp://dx.doi.org/10.1016/0012-365X(92)00473-5  (bibtex on dblp.org) 58 A local density condition for triangles. (1994)  Richard H. Schelp,  Paul Erdös,  Ralph J. Faudree,  Cecil C. Rousseauhttp://dx.doi.org/10.1016/0012-365X(92)00474-6  (bibtex on dblp.org) 59 Change of Leadership in a Random Graph Process. (1994)  Tomasz Luczak,  Paul Erdöshttp://dx.doi.org/10.1002/rsa.3240050122  (bibtex on dblp.org) 60 Clique coverings of the edges of a random graph. (1993)  Béla Bollobás,  Paul Erdös,  Joel Spencer,  Douglas B. Westhttp://dx.doi.org/10.1007/BF01202786  (bibtex on dblp.org) 61 Turán-Ramsey theorems and simple asymptotically extremal structures. (1993)  Paul Erdös,  Miklós Simonovits,  Vera T. Sós,  András Hajnal,  Endre Szemerédihttp://dx.doi.org/10.1007/BF01202788  (bibtex on dblp.org) 62 On graphical partitions. (1993)  Paul Erdös,  L. Bruce Richmondhttp://dx.doi.org/10.1007/BF01202789  (bibtex on dblp.org) 63 Ramsey Size Linear Graphs. (1993)  Richard H. Schelp,  Paul Erdös,  Ralph J. Faudree,  Cecil C. Rousseauhttp://dx.doi.org/10.1017/S096354830000078X  (bibtex on dblp.org) 64 Nearly Equal Distances in the Plane. (1993)  Paul Erdös,  Endre Makai,  János Pachhttp://dx.doi.org/10.1017/S0963548300000791  (bibtex on dblp.org) 65 Clique Partitions of Chordal Graphs. (1993)  Paul Erdös,  Edward T. Ordman,  Yechezkel Zalcsteinhttp://dx.doi.org/10.1017/S0963548300000808  (bibtex on dblp.org) 66 Monochromatic infinite paths. (1993)  Fred Galvin,  Paul Erdöshttp://dx.doi.org/10.1016/0012-365X(93)90508-Q  (bibtex on dblp.org) 67 The grid revisted. (1993)  Paul Erdös,  Imre Z. Ruzsa,  Zoltán Füredi,  János Pachhttp://dx.doi.org/10.1016/0012-365X(93)90155-M  (bibtex on dblp.org) 68 The size Ramsey number of a complete bipartite graph. (1993)  Paul Erdös,  Cecil C. Rousseauhttp://dx.doi.org/10.1016/0012-365X(93)90521-T  (bibtex on dblp.org) 69 Ramsey Problems Involving Degrees in Edge-colored Complete Graphs of VerticesBelonging to Monochromatic Subgraphs. (1993)  Guantao Chen,  Paul Erdös,  Richard H. Schelp,  Cecil C. Rousseauhttp://dx.doi.org/10.1006/eujc.1993.1023  (bibtex on dblp.org) 70 The Giant Component 1960-1993. (1993)  Paul Erdös  (bibtex on dblp.org) 71 Bounds for arrays of dots with distinct slopes or lengths. (1992)  Paul Erdös,  Herbert Taylor,  Ronald L. Graham,  Imre Z. Ruzsahttp://dx.doi.org/10.1007/BF01191203  (bibtex on dblp.org) 72 In memory of Tibor Gallai. (1992)  Paul Erdös  (bibtex on dblp.org) 73 Extremal problems involving vertices and edges on odd cycles. (1992)  Paul Erdös,  Ralph J. Faudree,  Cecil C. Rousseauhttp://dx.doi.org/10.1016/0012-365X(92)90586-5  (bibtex on dblp.org) 74 Arithmetic progressions in subset sums. (1992)  Paul Erdös,  András Sárközyhttp://dx.doi.org/10.1016/0012-365X(92)90119-Z  (bibtex on dblp.org) 75 Cycle-connected graphs. (1992)  Paul Erdös,  Richard A. Duke,  Vojtech Rödlhttp://dx.doi.org/10.1016/0012-365X(92)90680-E  (bibtex on dblp.org) 76 Covering the cliques of a graph with vertices. (1992)  Zsolt Tuza,  Paul Erdös,  Tibor Gallaihttp://dx.doi.org/10.1016/0012-365X(92)90681-5  (bibtex on dblp.org) 77 Diverse Homogeneous Sets. (1992)  Andreas Blass,  Paul Erdös,  Alan D. Taylor  (bibtex on dblp.org) 78 Distributed Loop Network with Minimum Transmission Delay. (1992)  Paul Erdös,  D. Frank Hsuhttp://dx.doi.org/10.1016/0304-3975(92)90370-U(bibtex on dblp.org) 79 Distinct Distances Determined By Subsets of a Point Set in Space. (1991)  Paul Erdös,  David Avis,  János Pachhttp://dx.doi.org/10.1016/0925-7721(91)90009-4(bibtex on dblp.org) 80 Lopsided Lovász Local Lemma and Latin transversals. (1991)  Paul Erdös,  Joel Spencerhttp://dx.doi.org/10.1016/0166-218X(91)90040-4(bibtex on dblp.org) 81 Degree sequences in triangle-free graphs. (1991)  Paul Erdös,  Siemion Fajtlowicz,  William Statonhttp://dx.doi.org/10.1016/0012-365X(91)90269-8(bibtex on dblp.org) 82 Saturated r-uniform hypergraphs. (1991)  Zsolt Tuza,  Paul Erdös,  Zoltán Füredihttp://dx.doi.org/10.1016/0012-365X(91)90035-Z(bibtex on dblp.org) 83 Matchings from a set below to a set above. (1991)  Paul Erdös,  Jean A. Larsonhttp://dx.doi.org/10.1016/0012-365X(91)90335-Y  (bibtex on dblp.org) 84 Some Ramsey-type theorems. (1991)  Fred Galvin,  Paul Erdöshttp://dx.doi.org/10.1016/0012-365X(91)90135-O  (bibtex on dblp.org) 85 Local constraints ensuring small representing sets. (1991)  Zsolt Tuza,  Paul Erdös,  András Hajnalhttp://dx.doi.org/10.1016/0097-3165(91)90074-Q  (bibtex on dblp.org) 86 Vertex coverings by monochromatic cycles and trees. (1991)  Paul Erdös,  András Gyárfás,  László Pyberhttp://dx.doi.org/10.1016/0095-8956(91)90007-7  (bibtex on dblp.org) 87 The Dimension of Random Ordered Sets. (1991)  Paul Erdös,  William T. Trotter,  Henry A. Kiersteadhttp://dx.doi.org/10.1002/rsa.3240020302  (bibtex on dblp.org) 88 Midpoints of Diagonals of Convex <i>n</i>-GONS. (1991)  Paul Erdös,  Peter C. Fishburn,  Zoltán Füredihttp://dx.doi.org/10.1137/0404030  (bibtex on dblp.org) 89 Crossing Families. (1991)  János Pach,  Boris Aronov,  Daniel J. Kleitman,  Wayne Goddard,  Paul Erdös,  Leonard J.Schulman,  Michael Klugermanhttp://doi.acm.org/10.1145/109648.109687  (bibtex on dblp.org) 90 Variation on the theme of repeated distances. (1990)  Paul Erdös,  János Pachhttp://dx.doi.org/10.1007/BF02122780  (bibtex on dblp.org) 91 Countable Decompositions of R<sup>2</sup> and R<sup>3</sup>. (1990)  Paul Erdös,  Péter Komjáthhttp://dx.doi.org/10.1007/BF02187793  (bibtex on dblp.org) 92 Subgraphs of minimal degree k. (1990)  Richard H. Schelp,  Paul Erdös,  Ralph J. Faudree,  Cecil C. Rousseauhttp://dx.doi.org/10.1016/0012-365X(90)90162-B  (bibtex on dblp.org) 93 Colouring prime distance graphs. (1990)  D. K. Skilton,  Paul Erdös,  Roger B. Eggletonhttp://dx.doi.org/10.1007/BF01787476(bibtex on dblp.org) 94 Quasi-progressions and descending waves. (1990)  Paul Erdös,  Tom C. Brown,  Allen R. Freedmanhttp://dx.doi.org/10.1016/0097-3165(90)90021-N  (bibtex on dblp.org) 95 Representations of Integers as the Sum of k Terms. (1990)  Paul Erdös,  Prasad Tetalihttp://dx.doi.org/10.1002/rsa.3240010302  (bibtex on dblp.org) 96 The size of chordal, interval and threshold subgraphs. (1989)  Edward T. Ordman,  Paul Erdös,  András Gyárfás,  Yechezkel Zalcsteinhttp://dx.doi.org/10.1007/BF02125893  (bibtex on dblp.org) 97 Disjoint Edges in Geometric Graphs. (1989)  Paul Erdös,  Noga Alonhttp://dx.doi.org/10.1007/BF02187731  (bibtex on dblp.org) 98 On the Graph of Large Distance. (1989)  László Lovász,  Paul Erdös,  Katalin Vesztergombihttp://dx.doi.org/10.1007/BF02187746  (bibtex on dblp.org) 99 Ramsey-type theorems. (1989)  Paul Erdös,  András Hajnalhttp://dx.doi.org/10.1016/0166-218X(89)90045-0  (bibtex on dblp.org) 100 On the number of distinct induced subgraphs of a graph. (1989)  Paul Erdös,  András Hajnalhttp://dx.doi.org/10.1016/0012-365X(89)90085-X  (bibtex on dblp.org) Paul ErdösCecil C. Rousseau,Richard H. Schelp, RalphJ. Faudree, Paul Erdös,William Staton, TalmageJames ReidAndrás Gyárfás, TomaszLuczak, Paul Erdös,Yoshiharu Kohayakawa,Miklós RuszinkóAndrás Hajnal, EndreSzemerédi, MiklósSimonovits, Vera T. Sós,Paul ErdösDaniel J. Kleitman,Leonard J. Schulman,Michael Klugerman,Wayne Goddard, BorisAronov, János PachJános Pach, Endre Makai,Gergely Harcos, DavidAvis, Paul ErdösJoel Spencer, BélaBollobás, Douglas B.West, Amites Sarkar,Paul ErdösZsolt Tuza, Tibor Gallai,Pavel Valtr, Paul Erdös,András HajnalPeter C. Fishburn, PaulErdösZoltán Füredi, BruceRothschild, Paul Erdös,Vera T. Sós, Zsolt Tuza,Peter C. FishburnAndrás Sárközy, ChristianMauduit, Paul Erdös,Vera T. SósRonald J. Gould, JenöLehel, Michael S.Jacobson, David S.Gunderson, ZoltánFüredi, Paul ErdösImre Z. Ruzsa, HerbertTaylor, Ronald L. Graham,Zoltán Füredi, Paul Erdös,János PachVojtech Rödl, Richard A.Duke, Paul ErdösYechezkel Zalcstein,Edward T. Ordman, PaulErdös, András GyárfásNoga Alon, M. Molloy,David S. Gunderson, PaulErdösSiemion Fajtlowicz, JohnAdrian Bondy, WilliamStaton, Paul ErdösLászló Pyber, Paul Erdös,András GyárfásFred Galvin, Paul ErdösPaul Erdös, L. BruceRichmond, KatalinVesztergombi, LászlóLovász, Prasad Tetali,Allen R. FreedmanCHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION
206
graphe biparti articles-auteurs est construit. Ce graphe peut ensuite être clusterisé
avec diﬀérentes méthodes. Enﬁn l’ensemble des auteurs des documents de chaque
cluster est utilisé pour étiqueter les clusters.

La ﬁgure 6.6 présente une copie d’écran obtenue pour une recherche des articles
dont Paul Erdös est auteur. La liste est limitée aux 100 articles les plus récents. On
observe que 20 clusters ont été construits.

Cette application permet donc de visualiser quels sont les collaborateurs d’un
auteur, et comment se regroupent ses collaborations. La principale limitation est
qu’aucune donnée thématique n’est utilisée. Il est probable que les clusters corres-
pondent à des thématiques diﬀérentes (diﬀérentes collaborations pour diﬀérentes
thématiques). Mais quand bien même ce serait le cas, il n’est pas évident de com-
prendre quelles sont les thématiques de chaque cluster. Extraire des informations
sur les thématiques abordées dans les articles n’est pas trivial. D’autant plus que la
seule information thématique disponible facilement est le titre des articles.

Aussi la donnée exploitée est certainement « trop » locale. Imaginons par exemple
un auteur A qui aurait collaboré une fois avec un auteur B et une fois (dans un
autre article) avec un auteur C et que ces deux auteurs B et C aient écrit un nombre
important d’articles ensemble. Si l’on ne connaît que les articles de A (comme c’est
le cas pour le moment) alors on ne sait pas qu’il est possible de rapprocher B et C.
Techniquement il faudrait disposer localement (i.e. sur le serveur de Kodex) de la
totalité de la base de données pour pouvoir enrichir le graphe d’articles non retournés
par la recherche mais pertinents vis à vis des auteurs présents. Cela demande une
mise en place technique plus conséquente, néanmoins envisageable car l’ensemble de
la base DBLP est disponible au téléchargement.

6.6 Conclusions du chapitre

Dans ce chapitre, nous avons présenté Kodex, un cadre logiciel modulable per-
mettant de construire des applications de catégorisation automatique des résultats
d’une recherche en utilisant des méthodes de clustering de graphe biparti.

Nous avons tout d’abord présenté un état de l’art des solutions proposées dans la
littérature pour répondre à ce problème de catégorisation automatique des résultats
d’une recherche. Ensuite l’architecture de Kodex a été exposée. Cette architecture
modulable permet de mettre en place facilement une chaîne de traitement pour
construire un clustering des résultats d’une recherche. Kodex permet aussi d’eﬀectuer
des pré-traitements hors-ligne.

Une application d’évaluation sur 2,6 millions de pages web a été présentée. Cette
évaluation a permis de montrer que cette approche améliore les résultats par rapport
à une simple liste ordonnée. Diﬀérentes méthodes de clustering ont pu être compa-
rées. Il a été montré que notre approche est préférable aux méthodes implémentées

6.6. CONCLUSIONS DU CHAPITRE

207

dans le logiciel Carrot. Aussi la méthode se basant sur le graphe objets-concepts
présentée au chapitre précédent donne les meilleurs résultats.

Plusieurs pistes de travail futur sont envisagées. La première concerne l’évalua-
tion. Nous avons vu les limites de l’évaluation présentée ici. Une nouvelle évaluation
a été mise en place dans le cadre du projet Quaero et on devrait bientôt pouvoir
en tirer des conclusions. Cette évaluation propose en particulier une tâche pour la-
quelle les documents retournés par un SRI ont été classés à la main en diﬀérentes
catégories.

Une autre idée pour faciliter l’évaluation est de construire un système pour aider
à annoter les résultats de Kodex. L’idée est de proposer un système pour faciliter
la construction d’un jeu d’évaluation en capitalisant le temps passé à tester et à
mettre au point une application Kodex. En eﬀet, lors d’une phase de mise au point,
on observe souvent de très bons résultats pour certains réglages, résultats qui ne
sont pas nécessairement aussi bons avec d’autres paramètres. Il faudrait donc pou-
voir sauvegarder aisément ces « bons » résultats, en y apportant éventuellement les
quelques corrections qui semblent nécessaires. Ces requêtes enregistrées pourraient
ainsi être rejouées et évaluées automatiquement, et constituer, a minima, des tests
de non-régression. En eﬀet un jeu de test ainsi construit n’aurait pas la même impar-
tialité qu’une évaluation tierce de type TREC. Cela pourrait toutefois permettre une
comparaison automatique utile et valable entre diﬀérents réglages ou algorithmes.
L’autre avantage est que ces jeux d’évaluation seront construits directement sur les
données des applications visées, et donc correspondent au plus près aux résultats
souhaités.

En parallèle à cette évolution nécessaire des systèmes d’évaluation, de nom-
breuses améliorations sont envisageables dans les chaînes de traitement de Kodex.
La modélisation des documents est certainement le point qui mérite le plus d’atten-
tion. En eﬀet nous avons vu que la modélisation faite dans les applications présentées
est relativement simple. En particulier sur l’application d’évaluation, la modélisation
est faite par une radicalisation des termes des documents puis un ﬁltrage de ceux-ci
à l’aide de diﬀérentes pondérations.

Plusieurs améliorations sont possibles. La première concerne la construction des
tokens utilisés. Une modélisation des documents autorisant des expressions multi-
mots serait nécessaire pour l’étiquetage des clusters, mais cela serait certainement
utile aussi pour la phase de clustering. Par exemple cela pourrait éviter de rap-
procher des documents parlant de « Jacques Brel » et des documents à propos de
« Jacques Cartier », ou encore des documents utilisant l’expression « tour d’ivoire »
et d’autres qui parleraient, par exemple, d’un réseau de traﬁc d’« ivoire » qui utilise
une « tour de guet » pour surveiller ces installations. À l’inverse cela permettrait
de rapprocher plus fortement des documents employant les mêmes expressions. Le
sens d’une expression multi-mots est moins ambigu que ceux de chacun de ces com-

Il serait possible aussi de désambiguïser, hors-ligne, les tokens associés à un
document. Par exemple diﬀérencier dans le modèle d’un document si « poulet » est
utilisé pour parler de manière argotique de la police, ou pour s’exprimer sur un
élevage de volailles.

Aussi un système plus complexe de sélection des tokens peut être envisagé. En
utilisant, par exemple, des méthodes d’apprentissage automatique. Cela peut être
pertinent sur des collections de documents assez homogènes.

CHAPITRE 6. APPLICATION À L’ORGANISATION
DES RÉSULTATS D’UNE RECHERCHE D’INFORMATION
208
posants pris indépendamment. Un système de tokenisation qui puisse extraire ces
expressions multi-mots pourrait donc améliorer les performances de Kodex.

Enﬁn un autre point d’amélioration important concerne le recouvrement entre
clusters. Pour le moment les méthodes de clustering utilisées assignent chaque do-
cument à un seul cluster. Il semble pourtant souhaitable que certains documents
apparaissent dans plusieurs clusters. Ce problème de clustering avec recouvrement
n’est pas simple à résoudre. En eﬀet les méthodes proposant un clustering avec re-
couvrement ont souvent comme défaut que le nombre de clusters retournés explose
(voir chapitre précédent). Or sur cette application, l’objectif est bien de garder un
nombre limité de clusters, quitte à perdre ou à contredire une partie de l’information
disponible.

209

Conclusion

Nous nous sommes intéressés dans cette thèse à l’exploitation de données de « ter-
rain » considérées sous forme de graphes : comment utiliser eﬃcacement la structure
de ces graphes, dits de terrain, pour en extraire une information « résumée » et per-
tinente en vue de diverses applications ? Dans une première partie l’information
extraite prend la forme d’une mesure de similarité entre sommets, puis dans une
seconde partie, d’un regroupement des sommets.

La première des contributions faites dans ce travail est l’introduction d’une nou-
velle mesure de similarité entre sommets d’un graphe : la conﬂuence. Cette mesure
est basée sur des marches aléatoires en temps courts, elle a donc l’avantage de pré-
senter une complexité temporelle raisonnable. Aussi la valeur de la conﬂuence est
normalisée entre zéro et un, et présente un seuil naturel à 0.5. Nous avons montré
que ce seuil peut se comprendre soit par rapport à la limite en temps inﬁni de la
marche aléatoire permettant le calcul, soit par rapport à la valeur de cette même
marche sur un graphe aléatoire équivalent. La conﬂuence est, à notre connaissance,
la seule similarité présentant ces caractéristiques.

Cette similarité est exploitée par la deuxième contribution. En eﬀet nous intro-
duisons GU D une méthode de comparaison de graphes se basant sur la conﬂuence.
GU D est une distance d’édition robuste entre graphes partageant les mêmes som-
mets. En comparant deux graphes, une arête présente seulement dans un des graphes
indique un conﬂit. Pourtant ce conﬂit n’est pas de la même intensité si les deux som-
mets correspondants sont proches (bien que non adjacents) ou s’ils sont distants.
Nous avons donc introduit une mesure qui évalue la « force » des conﬂits en utili-
sant la conﬂuence. À notre connaissance, ce problème de comparaison de graphes
dans sa forme particulière où les deux graphes partagent les mêmes sommets est
nouveau. Nous avons par ailleurs pu montrer son intérêt sur une application de
comparaison de ressources lexicales.

Une troisième contribution concerne le problème de clustering de graphe biparti.
Cette contribution est double : nous avons introduit une nouvelle procédure de clus-
tering ainsi qu’une méthode de prétraitement. La procédure de clustering permet, en
deux étapes, de construire un partitionnement des sommets d’un type sans imposer
de partitionnement correspondant sur les sommets de l’autre type. La méthode de
prétraitement utilise la conﬂuence pour simpliﬁer un graphe biparti en amont de

210

CONCLUSION

l’utilisation d’une méthode de clustering. Nous avons montré l’intérêt de ces deux
méthodes sur diﬀérents jeux d’évaluations artiﬁciels et issus de données réelles.

Enﬁn deux contributions applicatives ont été présentés : le système d’enrichis-
sement semi-automatique de ressources lexicales Wisigoth et le système de classiﬁ-
cation automatique des résultats d’une recherche Kodex. Wisigoth permet d’assister
la construction de ressources lexicales en proposant à l’utilisateur des candidats sy-
nonymes. Ces candidats sont calculés en utilisant une mesure de similarité entre
sommets d’un graphe, la ressource déjà existante étant employée comme donnée
source. Appliquée au dictionnaire collaboratif Wiktionary, nous avons montré que le
système permet de proposer des candidats pour la quasi totalité des entrées et que 30
à 60% des propositions permettent d’ajouter une relation de synonymie pertinente
d’après une ressource étalon externe.

Le système Kodex permet d’organiser automatiquement les résultats d’une re-
cherche d’information et ainsi d’exposer à l’utilisateur la polysémie éventuelle de la
requête. C’est une application directe des méthodes de clustering de graphe biparti.
Nous avons présenté les grandes lignes de l’architecture logicielle modulable mise
en place. Aussi une campagne d’évaluation sur une collection de plus de deux mil-
lions de pages web a été présentée. Cette évaluation a permis de montrer l’intérêt
de l’approche, tant par rapport à une simple liste ordonnée que par rapport à des
systèmes de clustering de documents de l’état de l’art. Enﬁn deux applications en
ligne basées sur Kodex ont été introduites.

En marge de ces diﬀérentes contributions, ce rapport présente un travail de
synthèse de l’état de l’art des similarités entre sommets et de celui concernant le
clustering de graphe biparti. En eﬀet, nous avons fourni un panorama aussi com-
plet que possible des mesures de similarité entre sommets d’un graphe. Nous avons
aussi proposé une comparaison expérimentale de ces mesures. Cette comparaison
permet de mieux cerner les diﬀérences et ressemblances qui peuvent exister entre
ces mesures. Et enﬁn nous proposons, dans la seconde partie, un tour des diﬀérentes
familles d’approches pouvant se comprendre en termes de clustering de graphe bi-
parti. Nous avons en particulier développé un parallèle entre l’analyse formelle de
concept et l’analyse de graphe biparti. Ce parallèle forme le socle nécessaire aux
contributions faites dans le chapitre correspondant.

De nombreuses perspectives ont été évoquées dans ce rapport, nous ne les répé-
tons donc pas ici. Nous souhaitons tout de même insister sur le fait que le travail
de synthèse entamé dans ce rapport peut être continué. Le nombre impressionnant
d’approches et de famille d’approches se rapportant, ou pouvant se rapporter, à la
métrologie des graphes de terrain rend cette tâche tant diﬃcile que nécessaire. Par
exemple, comme nous l’avons déjà remarqué, un parallèle plus approfondi entre les
méthodes de bi-clustering (utilisées en particulier en génétique) et les méthodes de
clustering de graphe (et de graphe biparti) serait proﬁtable. En plus de permettre

CONCLUSION

211

des transferts de connaissances entre les diﬀérentes communautés développant les
méthodes mises en parallèle, de tels parallèles permettent de mieux comprendre les
hypothèses parfois implicites faites par les méthodes, et donc amènent naturellement
à la construction de meilleures approches.

212

CONCLUSION

213

Bibliographie

L. Adamic et E. Adar. Friends and neighbors on the web. Social Networks, 25(3) :

211–230, 2003.

G. Adda, B. Sagot, K. Fort, et J. Mariani. Crowdsourcing for Language Resource
Development : Critical Analysis of Amazon Mechanical Turk Overpowering Use. In
LTC 2011 : Proceedings of the 5th Language and Technology Conference, Poznan,
Poland, 2011.

R. Agrawal, T. Imielinski, et A. Swami. Mining association rules between sets of items
in large databases. In Proc. of the 1993 ACM SIGMOD Inter. Conf. on management
of data, pages 207–216, New York, 1993.

O. Allali, C. Magnien, et M. Latapy. Internal link prediction : A new approach for

predicting links in bipartite graphs. Intell. Data Anal., 17(1) :5–25, 2013.

M. M. Babu. Introduction to microarray data analysis. In R. P. Grant, editor, Com-

putational Genomics : Theory and Application. Norwich, Horizon Press, 2004.

A. L. Barabási et R. Albert. Emergence of scaling in random networks. Science, 286

(5439) :509–512, 1999.

M.J. Barber. Modularity and community detection in bipartite networks. Physical
Review E (Statistical, Nonlinear, and Soft Matter Physics), 76(6), December 2007.
M. Barbut et B. Monjardet. Ordre et classiﬁcation : algèbre et combinatoire. Hachette,

1970.

R Belohlavek. Similarity relations in concept lattices. Journal of Logic and Computa-

tion, 10(6) :823–845, 2000.

R. Belohlavek. Fuzzy Relational Systems : Foundations and Principles. Kluwer Aca-

demic Publishers, 2002.

R. Belohlavek, J. Dvořák, et J. Outrata. Fast factorization by similarity in formal
concept analysis of data with fuzzy attributes. Journal of Computer and System
Sciences, 73(6) :1012 – 1022, 2007.

C. Berge. Graphes et hypergraphes. Monographies universitaires de mathématiques.

Dunod, Paris, 1970.

214

BIBLIOGRAPHIE

C. E. Bichot et P. Siarry. Partitionnement de graphe. Lavoisier, 2010.
V. D. Blondel, A. Gajardo, M. Heymans, P. Senellart, et P. V. Dooren. A measure
of similarity between graph vertices : Applications to synonym extraction and web
searching. SIAM Rev., 46 :647–666, 2004. ISSN 0036-1445.

S. Boccaletti, V. Latora, Y. Moreno, M. Chavez, et D.-U. Hwang. Complex networks :

Structure and dynamics. Physics Reports, 424(4–5) :175–308, 2006.

D. Boley, M. Gini, R. Gross, E.H. (Sam) Han, K. Hastings, G. Karypis, V. Kumar,
B. Mobasher, et J. Moore. Partitioning-based clustering for web document catego-
rization. Decision Support Systems, 27(3) :329–341, December 1999.

B. Bollobás. Modern Graph Theory. Graduate texts in mathematics. Springer, 1998.
Y. Boniface. Sluci : outil open source de text-mining en français, 2011.

http://sulci.fr/.

O. Bouidghaghen, M. Boughanem, H. Prade, et I. Mallak. A fuzzy logic approach
to topic extraction in texts. International Journal of Uncertainty, Fuzziness and
Knowledge-Based Systems, 17(Supplement-1) :81–112, 2009.

Didier Bourigault. Un Analyseur Syntaxique Opérationnel : SYNTEX. Mémoire d’ha-

bilitation à diriger des recherches, Université de Toulouse, 2007.

S. Brin et L. Page. The anatomy of a large-scale hypertextual web search engine.

Computer Networks, 30(1-7) :107–117, 1998.

M. Brunello. The Creation of Free Linguistic Corpora from the Web. In Proceedings
of WAC5 : 5th Workshop on Web As Corpus, pages 37–44, San Sebastian, 09 2009.
S. Busygin, O. Prokopyev, et P. M. Pardalos. Biclustering in data mining. Computers

and Operations Research, 35(9) :2964–2987, September 2008. ISSN 0305-0548.

C. Carpineto, S. Osiński, G. Romano, et D. Weiss. A survey of web clustering engines.

ACM Comput. Surv., 41(3) :1–38, 2009.

P. J. Carrington, J. Scott, et S. Wasserman, editors. Models and methods in social

network analysis. Cambridge University Press, 2005.

L. Cerf, P.N. Mougel, et J.F. Boulicaut. Agglomerating local patterns hierarchically
with alpha. In Proc. of the 18th ACM Conf. on Information and Knowledge Mana-
gement, CIKM’09, pages 1753–1756, New York, 2009.

S.H. Cha. Comprehensive survey on distance/similarity measures between probability
Int. Journal of Mathematical Models and Methods in Applied

